#!/usr/bin/env python3
"""
AI Prompt Quality Evaluator
è¯„ä¼° AI æç¤ºè¯çš„è´¨é‡
"""

import os
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional
import requests
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/root/clawd/data/evaluator.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class PromptEvaluator:
    """æç¤ºè¯è´¨é‡è¯„ä¼°å™¨"""

    def __init__(self, api_key: Optional[str] = None, provider: str = 'anthropic'):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            api_key: LLM API key
            provider: LLM æä¾›å•† (anthropic, openai, etc.)
        """
        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY') or os.getenv('OPENAI_API_KEY')
        self.provider = provider
        self.evaluation_dimensions = {
            'clarity': 'æ¸…æ™°åº¦ - æç¤ºè¯æ˜¯å¦æ˜ç¡®ã€æ— æ­§ä¹‰',
            'specificity': 'å…·ä½“æ€§ - æ˜¯å¦æä¾›å…·ä½“å‚æ•°å’Œçº¦æŸ',
            'structured': 'ç»“æ„åŒ– - æ˜¯å¦æœ‰æ¸…æ™°çš„æ ¼å¼å’Œç»„ç»‡',
            'practical': 'å®ç”¨æ€§ - æ˜¯å¦å¯ä»¥å®é™…ä½¿ç”¨',
            'innovative': 'åˆ›æ–°æ€§ - æ˜¯å¦æœ‰ç‹¬ç‰¹ä»·å€¼'
        }
        self.thresholds = {
            'excellent': 20,  # ä¼˜ç§€
            'good': 15,       # è‰¯å¥½
            'average': 10     # ä¸€èˆ¬
        }

    def evaluate_prompt(self, prompt: str, context: Optional[Dict] = None) -> Dict:
        """
        è¯„ä¼°å•ä¸ªæç¤ºè¯

        Args:
            prompt: æç¤ºè¯å†…å®¹
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info(f"å¼€å§‹è¯„ä¼°æç¤ºè¯: {prompt[:50]}...")

        try:
            # æ„å»ºè¯„ä¼°è¯·æ±‚
            evaluation_result = self._call_llm_evaluation(prompt, context)

            # è®¡ç®—æ€»åˆ†å’Œè¯„çº§
            scores = evaluation_result.get('scores', {})
            total_score = sum(scores.values())
            rating = self._calculate_rating(total_score)

            result = {
                'prompt': prompt,
                'context': context,
                'scores': scores,
                'total_score': total_score,
                'rating': rating,
                'evaluation': evaluation_result.get('evaluation', ''),
                'suggestions': evaluation_result.get('suggestions', []),
                'evaluated_at': datetime.now().isoformat()
            }

            logger.info(f"è¯„ä¼°å®Œæˆ - æ€»åˆ†: {total_score}/25, è¯„çº§: {rating}")
            return result

        except Exception as e:
            logger.error(f"è¯„ä¼°æç¤ºè¯æ—¶å‡ºé”™: {e}", exc_info=True)
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                'prompt': prompt,
                'context': context,
                'scores': {dim: 1 for dim in self.evaluation_dimensions.keys()},
                'total_score': 5,
                'rating': 'poor',
                'evaluation': f'è¯„ä¼°å¤±è´¥: {str(e)}',
                'suggestions': [],
                'evaluated_at': datetime.now().isoformat(),
                'error': str(e)
            }

    def _call_llm_evaluation(self, prompt: str, context: Optional[Dict]) -> Dict:
        """
        è°ƒç”¨ LLM è¿›è¡Œè¯„ä¼°

        Args:
            prompt: æç¤ºè¯å†…å®¹
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            è¯„ä¼°ç»“æœ
        """
        # æ„å»ºè¯„ä¼°æç¤º
        evaluation_prompt = self._build_evaluation_prompt(prompt, context)

        # æ ¹æ®æä¾›å•†é€‰æ‹© API
        if self.provider == 'anthropic':
            return self._call_anthropic(evaluation_prompt)
        elif self.provider == 'openai':
            return self._call_openai(evaluation_prompt)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {self.provider}")

    def _build_evaluation_prompt(self, prompt: str, context: Optional[Dict]) -> str:
        """
        æ„å»ºè¯„ä¼°æç¤º

        Args:
            prompt: è¦è¯„ä¼°çš„æç¤ºè¯
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            è¯„ä¼°æç¤ºå­—ç¬¦ä¸²
        """
        context_info = ""
        if context:
            context_info = f"\nä¸Šä¸‹æ–‡ä¿¡æ¯:\n{json.dumps(context, ensure_ascii=False, indent=2)}\n"

        evaluation_template = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI æç¤ºè¯è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æç¤ºè¯çš„è´¨é‡ã€‚

æç¤ºè¯å†…å®¹:
```
{prompt}
```
{context_info}
è¯·ä»ä»¥ä¸‹ 5 ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆæ¯ä¸ªç»´åº¦ 1-5 åˆ†ï¼Œ5 åˆ†ä¸ºæœ€å¥½ï¼‰ï¼š

1. æ¸…æ™°åº¦ (Clarity) - æç¤ºè¯æ˜¯å¦æ˜ç¡®ã€æ— æ­§ä¹‰
2. å…·ä½“æ€§ (Specificity) - æ˜¯å¦æä¾›å…·ä½“å‚æ•°å’Œçº¦æŸ
3. ç»“æ„åŒ– (Structured) - æ˜¯å¦æœ‰æ¸…æ™°çš„æ ¼å¼å’Œç»„ç»‡
4. å®ç”¨æ€§ (Practical) - æ˜¯å¦å¯ä»¥å®é™…ä½¿ç”¨
5. åˆ›æ–°æ€§ (Innovative) - æ˜¯å¦æœ‰ç‹¬ç‰¹ä»·å€¼

è¯·ä»¥ JSON æ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "scores": {{
    "clarity": <1-5çš„æ•´æ•°>,
    "specificity": <1-5çš„æ•´æ•°>,
    "structured": <1-5çš„æ•´æ•°>,
    "practical": <1-5çš„æ•´æ•°>,
    "innovative": <1-5çš„æ•´æ•°>
  }},
  "evaluation": "<ç®€è¦è¯„ä¼°è¯´æ˜ï¼Œ50-100å­—>",
  "suggestions": [
    "<æ”¹è¿›å»ºè®®1>",
    "<æ”¹è¿›å»ºè®®2>",
    "<æ”¹è¿›å»ºè®®3>"
  ]
}}

è¯·åªè¿”å› JSONï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚"""

        return evaluation_template

    def _call_anthropic(self, prompt: str) -> Dict:
        """
        è°ƒç”¨ Anthropic API

        Args:
            prompt: è¯„ä¼°æç¤º

        Returns:
            è¯„ä¼°ç»“æœ
        """
        try:
            url = "https://api.anthropic.com/v1/messages"
            headers = {
                "x-api-key": self.api_key,
                "content-type": "application/json",
                "anthropic-version": "2023-06-01"
            }
            payload = {
                "model": "claude-3-haiku-20240307",
                "max_tokens": 2000,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            }

            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()

            # æå–å“åº”å†…å®¹
            content = data['content'][0]['text']

            # è§£æ JSON
            result = json.loads(self._extract_json(content))
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"Anthropic API è¯·æ±‚å¤±è´¥: {e}")
            raise
        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"è§£æ Anthropic å“åº”å¤±è´¥: {e}")
            raise

    def _call_openai(self, prompt: str) -> Dict:
        """
        è°ƒç”¨ OpenAI API

        Args:
            prompt: è¯„ä¼°æç¤º

        Returns:
            è¯„ä¼°ç»“æœ
        """
        try:
            url = "https://api.openai.com/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": "gpt-3.5-turbo",
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": 2000,
                "temperature": 0.3
            }

            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()

            # æå–å“åº”å†…å®¹
            content = data['choices'][0]['message']['content']

            # è§£æ JSON
            result = json.loads(self._extract_json(content))
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"OpenAI API è¯·æ±‚å¤±è´¥: {e}")
            raise
        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"è§£æ OpenAI å“åº”å¤±è´¥: {e}")
            raise

    def _extract_json(self, text: str) -> str:
        """
        ä»æ–‡æœ¬ä¸­æå– JSON

        Args:
            text: åŒ…å« JSON çš„æ–‡æœ¬

        Returns:
            JSON å­—ç¬¦ä¸²
        """
        # å°è¯•ç›´æ¥è§£æ
        try:
            json.loads(text)
            return text
        except json.JSONDecodeError:
            pass

        # å°è¯•æŸ¥æ‰¾ JSON ä»£ç å—
        if '```json' in text:
            start = text.find('```json') + 7
            end = text.find('```', start)
            if end != -1:
                return text[start:end].strip()
        elif '```' in text:
            start = text.find('```') + 3
            end = text.find('```', start)
            if end != -1:
                return text[start:end].strip()

        # å°è¯•æŸ¥æ‰¾ { } åŒ…å›´çš„ JSON
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1:
            return text[start:end+1]

        return text

    def _calculate_rating(self, total_score: int) -> str:
        """
        è®¡ç®—è¯„çº§

        Args:
            total_score: æ€»åˆ†

        Returns:
            è¯„çº§å­—ç¬¦ä¸²
        """
        if total_score >= self.thresholds['excellent']:
            return 'excellent'  # ä¼˜ç§€
        elif total_score >= self.thresholds['good']:
            return 'good'       # è‰¯å¥½
        elif total_score >= self.thresholds['average']:
            return 'average'    # ä¸€èˆ¬
        else:
            return 'poor'       # è¾ƒå·®

    def evaluate_batch(self, prompts: List[str], contexts: Optional[List[Dict]] = None) -> List[Dict]:
        """
        æ‰¹é‡è¯„ä¼°æç¤ºè¯

        Args:
            prompts: æç¤ºè¯åˆ—è¡¨
            contexts: ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        results = []

        for i, prompt in enumerate(prompts):
            context = contexts[i] if contexts and i < len(contexts) else None

            try:
                result = self.evaluate_prompt(prompt, context)
                results.append(result)

                # é¿å…é™æµ
                time.sleep(1)

            except Exception as e:
                logger.error(f"è¯„ä¼°ç¬¬ {i+1} ä¸ªæç¤ºè¯æ—¶å‡ºé”™: {e}")
                continue

        return results

    def generate_report(self, evaluations: List[Dict]) -> str:
        """
        ç”Ÿæˆ Markdown è¯„ä¼°æŠ¥å‘Š

        Args:
            evaluations: è¯„ä¼°ç»“æœåˆ—è¡¨

        Returns:
            Markdown æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        lines = [
            "# AI æç¤ºè¯è´¨é‡è¯„ä¼°æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**è¯„ä¼°æ•°é‡**: {len(evaluations)}",
            "",
            "---",
            ""
        ]

        # ç»Ÿè®¡ä¿¡æ¯
        rating_counts = {'excellent': 0, 'good': 0, 'average': 0, 'poor': 0}
        total_score = 0

        for eval_result in evaluations:
            rating = eval_result.get('rating', 'poor')
            rating_counts[rating] += 1
            total_score += eval_result.get('total_score', 0)

        avg_score = total_score / len(evaluations) if evaluations else 0

        lines.extend([
            "## è¯„åˆ†ç»Ÿè®¡",
            "",
            f"- **ä¼˜ç§€ (â‰¥20åˆ†)**: {rating_counts['excellent']}",
            f"- **è‰¯å¥½ (15-19åˆ†)**: {rating_counts['good']}",
            f"- **ä¸€èˆ¬ (10-14åˆ†)**: {rating_counts['average']}",
            f"- **è¾ƒå·® (<10åˆ†)**: {rating_counts['poor']}",
            "",
            f"**å¹³å‡åˆ†**: {avg_score:.1f}/25",
            "",
            "---",
            "",
            "## è¯¦ç»†è¯„ä¼°ç»“æœ",
            ""
        ])

        # è¯¦ç»†è¯„ä¼°
        for i, eval_result in enumerate(evaluations, 1):
            prompt = eval_result.get('prompt', '')
            scores = eval_result.get('scores', {})
            total = eval_result.get('total_score', 0)
            rating = eval_result.get('rating', 'poor')
            evaluation = eval_result.get('evaluation', '')
            suggestions = eval_result.get('suggestions', [])

            rating_map = {
                'excellent': 'ğŸŒŸ ä¼˜ç§€',
                'good': 'ğŸ‘ è‰¯å¥½',
                'average': 'ğŸ˜ ä¸€èˆ¬',
                'poor': 'ğŸ‘ è¾ƒå·®'
            }

            lines.extend([
                f"### {i}. {prompt[:50]}...",
                "",
                f"**æ€»åˆ†**: {total}/25 | **è¯„çº§**: {rating_map.get(rating, rating)}",
                "",
                "**å„é¡¹å¾—åˆ†**:",
                f"- æ¸…æ™°åº¦: {scores.get('clarity', 0)}/5",
                f"- å…·ä½“æ€§: {scores.get('specificity', 0)}/5",
                f"- ç»“æ„åŒ–: {scores.get('structured', 0)}/5",
                f"- å®ç”¨æ€§: {scores.get('practical', 0)}/5",
                f"- åˆ›æ–°æ€§: {scores.get('innovative', 0)}/5",
                "",
                f"**è¯„ä¼°è¯´æ˜**: {evaluation}",
                ""
            ])

            if suggestions:
                lines.append("**æ”¹è¿›å»ºè®®**:")
                for suggestion in suggestions:
                    lines.append(f"- {suggestion}")
                lines.append("")

            lines.extend(["---", "", ""])

        # æ¨èåˆ—è¡¨
        excellent_prompts = [e for e in evaluations if e.get('rating') == 'excellent']
        if excellent_prompts:
            lines.extend([
                "## ğŸŒŸ ä¼˜ç§€æç¤ºè¯æ¨è",
                ""
            ])
            for i, eval_result in enumerate(excellent_prompts, 1):
                lines.append(f"{i}. {eval_result.get('prompt', '')}")
                lines.append(f"   - æ€»åˆ†: {eval_result.get('total_score', 0)}/25")
                lines.append("")
            lines.append("---")
            lines.append("")

        return '\n'.join(lines)

    def save_results(self, evaluations: List[Dict], json_path: str, markdown_path: str):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ

        Args:
            evaluations: è¯„ä¼°ç»“æœåˆ—è¡¨
            json_path: JSON è¾“å‡ºè·¯å¾„
            markdown_path: Markdown è¾“å‡ºè·¯å¾„
        """
        try:
            # ä¿å­˜ JSON
            os.makedirs(os.path.dirname(json_path), exist_ok=True)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(evaluations, f, ensure_ascii=False, indent=2)
            logger.info(f"JSON ç»“æœå·²ä¿å­˜åˆ°: {json_path}")

            # ä¿å­˜ Markdown
            os.makedirs(os.path.dirname(markdown_path), exist_ok=True)
            report = self.generate_report(evaluations)
            with open(markdown_path, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"Markdown æŠ¥å‘Šå·²ä¿å­˜åˆ°: {markdown_path}")

        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}", exc_info=True)
            raise


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    logger.info("=" * 60)
    logger.info("AI æç¤ºè¯è´¨é‡è¯„ä¼°å™¨")
    logger.info("=" * 60)

    # æµ‹è¯•æç¤ºè¯
    test_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç ï¼Œé‡ç‚¹å…³æ³¨ï¼š
1. ä»£ç é£æ ¼å’Œæ ¼å¼
2. æ½œåœ¨çš„ bug
3. æ€§èƒ½ä¼˜åŒ–å»ºè®®
4. æœ€ä½³å®è·µéµå¾ªæƒ…å†µ

è¯·æä¾›è¯¦ç»†çš„åé¦ˆå’Œæ”¹è¿›å»ºè®®ã€‚"""

    try:
        evaluator = PromptEvaluator()
        result = evaluator.evaluate_prompt(test_prompt)

        print("\nè¯„ä¼°ç»“æœ:")
        print(json.dumps(result, ensure_ascii=False, indent=2))

    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}", exc_info=True)


if __name__ == '__main__':
    main()
