#!/usr/bin/env python3
"""
ä» X (Twitter) æœç´¢ AI æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

åŠŸèƒ½ç‰¹æ€§ï¼š
- æ ‡ç­¾è¿‡æ»¤ï¼ˆ#AIPrompts, #promptengineering ç­‰ï¼‰
- è´¦å·ç™½åå•åŠŸèƒ½
- ç¼“å­˜æœºåˆ¶å‡å°‘ API è°ƒç”¨
- ä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢æ ¼å¼
"""

import json
import os
import re
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set
import logging
import pickle

try:
    import yaml
except ImportError:
    print("âš ï¸  è¯·å®‰è£… PyYAML: pip install pyyaml")
    exit(1)

try:
    import requests
except ImportError:
    print("âš ï¸  è¯·å®‰è£… requests: pip install requests")
    exit(1)

# æ—¥å¿—é…ç½®
def setup_logging(log_dir: str = "/root/clawd/logs") -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, "search-x-prompts.log")

    logger = logging.getLogger("search_x_prompts")
    logger.setLevel(logging.INFO)

    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger

logger = setup_logging()


class Config:
    """é…ç½®ç®¡ç†"""

    def __init__(self, config_path: str = "/root/clawd/config/prompts-config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()

    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(self.config_path):
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._default_config()

        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'twitter': {
                'api_endpoint': 'https://api.twitterapi.io/v2/tweets/search/recent',
                'api_key_env': 'TWITTER_API_KEY',
                'search_queries': [
                    "AI prompts (#AIPrompts OR #promptengineering) -is:retweet lang:en",
                    "ChatGPT prompts (#ChatGPT OR #GPT4) -is:retweet lang:en"
                ],
                'hashtag_filters': ['#AIPrompts', '#promptengineering', '#ChatGPT'],
                'account_whitelist': ['openai', 'AnthropicAI'],
                'cache': {
                    'enabled': True,
                    'cache_dir': '/root/clawd/cache/twitter',
                    'cache_ttl_hours': 24
                },
                'max_results_per_query': 100,
                'max_total_results': 500
            },
            'output': {
                'data_dir': '/root/clawd/data/prompts',
                'format': 'jsonl'
            }
        }

    @property
    def twitter_api_key(self) -> Optional[str]:
        """è·å– Twitter API Key"""
        key = os.getenv(self.config['twitter']['api_key_env'])
        if not key:
            logger.error(f"ç¯å¢ƒå˜é‡ {self.config['twitter']['api_key_env']} æœªè®¾ç½®")
        return key

    @property
    def twitter_endpoint(self) -> str:
        return self.config['twitter']['api_endpoint']

    @property
    def search_queries(self) -> List[str]:
        return self.config['twitter'].get('search_queries', [])

    @property
    def hashtag_filters(self) -> List[str]:
        return self.config['twitter'].get('hashtag_filters', [])

    @property
    def account_whitelist(self) -> List[str]:
        return self.config['twitter'].get('account_whitelist', [])

    @property
    def cache_enabled(self) -> bool:
        return self.config['twitter']['cache'].get('enabled', True)

    @property
    def cache_dir(self) -> str:
        return self.config['twitter']['cache']['cache_dir']

    @property
    def cache_ttl_hours(self) -> int:
        return self.config['twitter']['cache']['cache_ttl_hours']

    @property
    def max_results_per_query(self) -> int:
        return self.config['twitter']['max_results_per_query']

    @property
    def max_total_results(self) -> int:
        return self.config['twitter']['max_total_results']

    @property
    def output_dir(self) -> str:
        return self.config['output']['data_dir']


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str, ttl_hours: int):
        self.cache_dir = Path(cache_dir)
        self.ttl_hours = ttl_hours
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_cache_key(self, query: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(query.encode()).hexdigest()

    def _get_cache_path(self, query: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{self._get_cache_key(query)}.pkl"

    def get(self, query: str) -> Optional[Dict]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        cache_path = self._get_cache_path(query)

        if not cache_path.exists():
            return None

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        cache_age = datetime.now() - datetime.fromtimestamp(cache_path.stat().st_mtime)
        if cache_age > timedelta(hours=self.ttl_hours):
            logger.info(f"ç¼“å­˜è¿‡æœŸ: {query}")
            cache_path.unlink()
            return None

        try:
            with open(cache_path, 'rb') as f:
                cached_data = pickle.load(f)
            logger.info(f"ä»ç¼“å­˜åŠ è½½: {query} ({len(cached_data.get('data', []))} æ¡)")
            return cached_data
        except Exception as e:
            logger.error(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return None

    def set(self, query: str, data: Dict):
        """å†™å…¥ç¼“å­˜"""
        cache_path = self._get_cache_path(query)

        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
            logger.info(f"å†™å…¥ç¼“å­˜: {query} ({len(data.get('data', []))} æ¡)")
        except Exception as e:
            logger.error(f"å†™å…¥ç¼“å­˜å¤±è´¥: {e}")


class TwitterSearcher:
    """Twitter æœç´¢å™¨"""

    def __init__(self, config: Config):
        self.config = config
        self.cache = CacheManager(config.cache_dir, config.cache_ttl_hours) if config.cache_enabled else None

    def search(self, query: str) -> List[Dict]:
        """æœç´¢ Twitter"""
        # æ£€æŸ¥ç¼“å­˜
        if self.cache:
            cached = self.cache.get(query)
            if cached:
                return cached.get('data', [])

        headers = {
            "X-API-Key": self.config.twitter_api_key,
            "Accept": "application/json"
        }

        params = {
            "query": query,
            "max_results": self.config.max_results_per_query,
            "tweet.fields": "created_at,author_id,public_metrics,entities"
        }

        try:
            response = requests.get(
                self.config.twitter_endpoint,
                headers=headers,
                params=params,
                timeout=30
            )
            response.raise_for_status()

            data = response.json()
            tweets = data.get("data", [])

            # è¿‡æ»¤æ¨æ–‡
            filtered_tweets = self._filter_tweets(tweets, query)

            # å†™å…¥ç¼“å­˜
            if self.cache:
                self.cache.set(query, {
                    'data': filtered_tweets,
                    'timestamp': datetime.now().isoformat()
                })

            logger.info(f"æœç´¢ '{query}': æ‰¾åˆ° {len(filtered_tweets)} æ¡æ¨æ–‡")
            return filtered_tweets

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥ '{query}': {e}")
            return []

    def _filter_tweets(self, tweets: List[Dict], query: str) -> List[Dict]:
        """è¿‡æ»¤æ¨æ–‡ï¼šæ ‡ç­¾è¿‡æ»¤ + è´¦å·ç™½åå•"""
        filtered = []

        hashtag_filters = self.config.hashtag_filters
        account_whitelist = [acc.lower() for acc in self.config.account_whitelist]

        for tweet in tweets:
            author_id = tweet.get('author_id', '')
            entities = tweet.get('entities', {})
            hashtags = [tag.get('tag', '').lower() for tag in entities.get('hashtags', [])]

            # æ£€æŸ¥æ ‡ç­¾è¿‡æ»¤
            has_target_hashtag = any(
                tag.lower() in [ht.lower() for ht in hashtag_filters]
                for tag in hashtags
            )

            # æ£€æŸ¥è´¦å·ç™½åå•
            in_whitelist = any(acc in str(author_id).lower() for acc in account_whitelist)

            # å¦‚æœæœ‰æ ‡ç­¾æˆ–åœ¨ç™½åå•ä¸­ï¼Œåˆ™ä¿ç•™
            if has_target_hashtag or in_whitelist:
                tweet_data = {
                    "tweet_id": tweet.get("id"),
                    "text": tweet.get("text", ""),
                    "author_id": author_id,
                    "created_at": tweet.get("created_at"),
                    "public_metrics": tweet.get("public_metrics", {}),
                    "hashtags": hashtags,
                    "source": "X",
                    "search_query": query,
                    "scraped_at": datetime.now().isoformat(),
                    "matched_hashtag": has_target_hashtag,
                    "in_whitelist": in_whitelist
                }
                filtered.append(tweet_data)

        return filtered


class PromptExtractor:
    """æç¤ºè¯æå–å™¨"""

    def extract_from_tweets(self, tweets: List[Dict]) -> List[Dict]:
        """ä»æ¨æ–‡ä¸­æå–æç¤ºè¯"""
        prompts = []

        for tweet in tweets:
            text = tweet.get("text", "")
            extracted = self._extract_prompts(text)

            for prompt in extracted:
                if len(prompt.strip()) > 20:
                    prompts.append({
                        "prompt": prompt.strip(),
                        "source": "X",
                        "source_tweet_id": tweet.get("tweet_id"),
                        "author_id": tweet.get("author_id"),
                        "search_query": tweet.get("search_query"),
                        "hashtags": tweet.get("hashtags", []),
                        "extracted_at": datetime.now().isoformat(),
                        "quality_score": 50  # é»˜è®¤åˆ†æ•°ï¼Œå¾… LLM è¯„ä¼°
                    })

        logger.info(f"ä» {len(tweets)} æ¡æ¨æ–‡ä¸­æå– {len(prompts)} ä¸ªæç¤ºè¯")
        return prompts

    def _extract_prompts(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æç¤ºè¯"""
        prompts = []

        # ä»£ç å—
        code_blocks = re.findall(r'```([\s\S]*?)```', text)
        prompts.extend(code_blocks)

        # å¼•å·
        quotes = re.findall(r'"([^"]{20,})"', text)
        prompts.extend(quotes)

        # Prompt: æ ‡è®°
        specific = re.findall(r'(?:Prompt|prompt)[:ï¼š]\s*([^\n]{20,})', text)
        prompts.extend(specific)

        return list(set(prompts))  # å»é‡


def save_jsonl(data: List[Dict], filepath: str):
    """ä¿å­˜ JSONL æ ¼å¼æ•°æ®"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)

    with open(filepath, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    logger.info(f"ä¿å­˜ {len(data)} æ¡æ•°æ®åˆ°: {filepath}")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("ğŸ” å¼€å§‹æœç´¢ X (Twitter) è·å– AI æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    logger.info("=" * 80)

    # åŠ è½½é…ç½®
    config = Config()

    # æ£€æŸ¥ API Key
    if not config.twitter_api_key:
        logger.error("âŒ TWITTER_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = config.output_dir
    os.makedirs(output_dir, exist_ok=True)

    # æœç´¢å™¨
    searcher = TwitterSearcher(config)

    # æ‰§è¡Œæœç´¢
    all_tweets = []
    total_searched = 0

    logger.info(f"æœç´¢æŸ¥è¯¢æ•°: {len(config.search_queries)}")

    for i, query in enumerate(config.search_queries, 1):
        logger.info(f"[{i}/{len(config.search_queries)}] æœç´¢: {query}")

        tweets = searcher.search(query)

        if tweets:
            all_tweets.extend(tweets)
            total_searched += len(tweets)

        # é™åˆ¶æ€»æ•°
        if len(all_tweets) >= config.max_total_results:
            logger.info(f"è¾¾åˆ°æœ€å¤§ç»“æœæ•°é™åˆ¶: {config.max_total_results}")
            all_tweets = all_tweets[:config.max_total_results]
            break

    # ä¿å­˜æ¨æ–‡
    tweets_file = os.path.join(output_dir, "x-search-results.jsonl")
    save_jsonl(all_tweets, tweets_file)

    # æå–æç¤ºè¯
    extractor = PromptExtractor()
    prompts = extractor.extract_from_tweets(all_tweets)

    prompts_file = os.path.join(output_dir, "extracted-prompts.jsonl")
    save_jsonl(prompts, prompts_file)

    # ç”ŸæˆæŠ¥å‘Š
    report = {
        "timestamp": datetime.now().isoformat(),
        "searched_queries": config.search_queries,
        "hashtag_filters": config.hashtag_filters,
        "account_whitelist": config.account_whitelist,
        "total_tweets_found": len(all_tweets),
        "total_prompts_extracted": len(prompts),
        "cache_enabled": config.cache_enabled,
        "output_files": {
            "tweets": tweets_file,
            "prompts": prompts_file
        }
    }

    report_file = os.path.join(output_dir, f"x-search-report-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    logger.info("=" * 80)
    logger.info("âœ… æœç´¢å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info(f"ğŸ“Š ç»Ÿè®¡:")
    logger.info(f"  æœç´¢æŸ¥è¯¢: {len(config.search_queries)} ä¸ª")
    logger.info(f"  æ‰¾åˆ°æ¨æ–‡: {len(all_tweets)} æ¡")
    logger.info(f"  æå–æç¤ºè¯: {len(prompts)} ä¸ª")
    logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    logger.info(f"  æ¨æ–‡: {tweets_file}")
    logger.info(f"  æç¤ºè¯: {prompts_file}")
    logger.info(f"  æŠ¥å‘Š: {report_file}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
