#!/usr/bin/env python3
"""
ä» GitHub å’Œ Reddit æ”¶é›†è°·æ­Œç”Ÿå›¾å’Œ Sora 2 çš„ Prompts
"""

import json
import os
from datetime import datetime

# æ•°æ®æº
DATA_SOURCES = [
    "/root/clawd/data/prompts/reddit-prompts.jsonl",
    "/root/clawd/data/prompts/hacker-news-ai.jsonl",
    "/root/clawd/data/prompts/collected.jsonl"
]

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "/root/clawd/data/prompts"
OUTPUT_FILE = f"{OUTPUT_DIR}/google-sora2-prompts-manual.jsonl"

# è°·æ­Œç”Ÿå›¾ç›¸å…³å…³é”®è¯
GOOGLE_IMAGE_KEYWORDS = [
    "google imagen 3",
    "google imagen prompt",
    "google image generation",
    "veo prompt",
    "google ai image",
    "imagen 3",
    "google veo"
    "google image model"
    "è°·æ­Œç”Ÿå›¾",
    "Google Imagen",
    "Google Veo"
]

# Sora 2 ç›¸å…³å…³é”®è¯
SORA2_KEYWORDS = [
    "sora 2",
    "sora2",
    "openai sora 2",
    "sora 2 prompt",
    "sora 2 video",
    "sora 2 text to video",
    "sora 2 vs",
    "openai sora2",
    "Sora 2",
    "Sora 2 æç¤ºè¯",
    "OpenAI Sora 2"
]

def search_in_content(content, keywords):
    """åœ¨å†…å®¹ä¸­æœç´¢å…³é”®è¯"""
    if not content:
        return []
    
    text = content.lower()
    found_keywords = []
    
    for keyword in keywords:
        if keyword.lower() in text:
            found_keywords.append(keyword)
    
    return found_keywords

def categorize_prompt(content):
    """åˆ†ç±» prompt ç±»å‹"""
    if not content:
        return "é€šç”¨"
    
    text = content.lower()
    
    # æ£€æŸ¥è°·æ­Œç”Ÿå›¾ç›¸å…³
    google_keywords = ['imagen', 'veo', 'google image', 'è°·æ­Œç”Ÿå›¾', 
                     'Google Imagen', 'Google Veo', 'image generation']
    if any(kw in text for kw in google_keywords):
        return "è°·æ­Œç”Ÿå›¾"
    
    # æ£€æŸ¥ Sora 2 ç›¸å…³
    sora_keywords = ['sora 2', 'sora2', 'openai sora 2', 'sora 2 video',
                    'Sora 2', 'OpenAI Sora 2']
    if any(kw in text for kw in sora_keywords):
        return "Sora 2"
    
    return "é€šç”¨"

def extract_prompt_from_text(text):
    """ä»æ–‡æœ¬ä¸­æå– prompt"""
    if not text:
        return ""
    
    prompt = text.strip()
    
    # é™åˆ¶é•¿åº¦
    if len(prompt) > 2000:
        prompt = prompt[:2000] + "..."
    
    return prompt

def calculate_quality_score(data):
    """è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆ0-100ï¼‰"""
    score = 0
    
    content = extract_prompt_from_text(data.get('content', data.get('text', '')))
    title = data.get('title', '')
    likes = data.get('likes', 0) or data.get('points', 0) or data.get('score', 0)
    
    # å†…å®¹é•¿åº¦è¯„åˆ†
    if len(content) > 100:
        score += 10
    if len(content) > 300:
        score += 10
    if len(content) > 500:
        score += 10
    
    # æåˆ°å…³é”®è¯çš„è¯„åˆ†
    text = content.lower() + ' ' + title.lower()
    
    google_keywords = ['imagen', 'veo', 'google image', 'è°·æ­Œç”Ÿå›¾', 'Google Imagen']
    sora_keywords = ['sora 2', 'sora2', 'openai sora 2']
    video_keywords = ['video', 'video generation', 'text to video', 'ç”Ÿè§†é¢‘']
    
    for keyword in google_keywords + sora_keywords + video_keywords:
        if keyword in text:
            score += 20
            break
    
    # äº’åŠ¨è¯„åˆ†
    if likes > 0:
        import math
        score += min(30, math.log2(likes + 1) * 3)
    
    return min(100, score)

def main():
    print("=" * 80)
    print("ğŸ” ä»å·²æ”¶é›†çš„æ•°æ®ä¸­æå–è°·æ­Œç”Ÿå›¾å’Œ Sora 2 Prompts")
    print("=" * 80)
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # è¯»å–æ‰€æœ‰æ•°æ®
    all_data = []
    for source_file in DATA_SOURCES:
        if os.path.exists(source_file):
            print(f"ğŸ“– è¯»å–: {source_file}")
            try:
                with open(source_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if not line.strip():
                            continue
                        try:
                            data = json.loads(line)
                            all_data.append(data)
                        except:
                            continue
                print(f"  âœ“ è¯»å–äº† {len(all_data)} æ¡æ•°æ®")
            except Exception as e:
                print(f"  âš ï¸  è¯»å–å¤±è´¥: {e}")
    
    print()
    print(f"ğŸ“Š æ€»å…±è¯»å– {len(all_data)} æ¡æ•°æ®")
    print()
    
    # æœç´¢è°·æ­Œç”Ÿå›¾ Prompts
    print("[1/2] æœç´¢è°·æ­Œç”Ÿå›¾ Prompts...")
    google_prompts = []
    
    for data in all_data:
        content = data.get('content', data.get('text', ''))
        title = data.get('title', '')
        
        found_keywords = search_in_content(content + ' ' + title, GOOGLE_IMAGE_KEYWORDS)
        
        if found_keywords:
            prompt_type = "è°·æ­Œç”Ÿå›¾"
            quality_score = calculate_quality_score(data)
            
            google_prompts.append({
                "content": content,
                "title": title,
                "prompt_type": prompt_type,
                "quality_score": quality_score,
                "found_keywords": found_keywords,
                "source": data.get('source', 'unknown'),
                "url": data.get('url', ''),
                "likes": data.get('likes', 0) or data.get('points', 0) or data.get('score', 0)
            })
    
    print(f"  âœ“ æ‰¾åˆ° {len(google_prompts)} ä¸ªè°·æ­Œç”Ÿå›¾ Prompts")
    print()
    
    # æœç´¢ Sora 2 Prompts
    print("[2/2] æœç´¢ Sora 2 Prompts...")
    sora2_prompts = []
    
    for data in all_data:
        content = data.get('content', data.get('text', ''))
        title = data.get('title', '')
        
        found_keywords = search_in_content(content + ' ' + title, SORA2_KEYWORDS)
        
        if found_keywords:
            prompt_type = "Sora 2"
            quality_score = calculate_quality_score(data)
            
            sora2_prompts.append({
                "content": content,
                "title": title,
                "prompt_type": prompt_type,
                "quality_score": quality_score,
                "found_keywords": found_keywords,
                "source": data.get('source', 'unknown'),
                "url": data.get('url', ''),
                "likes": data.get('likes', 0) or data.get('points', 0) or data.get('score', 0)
            })
    
    print(f"  âœ“ æ‰¾åˆ° {len(sora2_prompts)} ä¸ª Sora 2 Prompts")
    print()
    
    # å»é‡
    seen = set()
    unique_prompts = []
    
    for prompt in google_prompts + sora2_prompts:
        content_hash = hash(prompt.get('content', ''))
        if content_hash not in seen:
            seen.add(content_hash)
            unique_prompts.append(prompt)
    
    print(f"ğŸ“Š å»é‡å: {len(unique_prompts)} ä¸ª Prompts")
    print()
    
    # é«˜è´¨é‡è¿‡æ»¤
    high_quality = [p for p in unique_prompts if p.get('quality_score', 0) >= 50]
    
    print(f"ğŸ“Š é«˜è´¨é‡ Prompts (>=50 åˆ†): {len(high_quality)} ä¸ª")
    print()
    
    # åˆ†ç±»ç»Ÿè®¡
    google_count = sum(1 for p in high_quality if p.get('prompt_type') == 'è°·æ­Œç”Ÿå›¾')
    sora2_count = sum(1 for p in high_quality if p.get('prompt_type') == 'Sora 2')
    
    print(f"ğŸ“‚ è°·æ­Œç”Ÿå›¾ Prompts: {google_count} ä¸ª")
    print(f"ğŸ“‚ Sora 2 Prompts: {sora2_count} ä¸ª")
    print()
    
    # ä¿å­˜æ•°æ®
    print("ğŸ’¾ ä¿å­˜æ•°æ®...")
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for prompt in high_quality:
            f.write(json.dumps(prompt, ensure_ascii=False) + '\n')
    
    print(f"  âœ“ å·²ä¿å­˜: {OUTPUT_FILE}")
    print()
    
    # æ˜¾ç¤º Top 20
    print("ğŸ† Top 20 è°·æ­Œç”Ÿå›¾å’Œ Sora 2 Prompts")
    print("=" * 80)
    print()
    
    print(f"{'æ’å':<6} {'ç±»å‹':<15} {'åˆ†æ•°':<10} {'å…³é”®è¯'}")
    print("-" * 80)
    
    sorted_prompts = sorted(high_quality, key=lambda x: x.get('quality_score', 0), reverse=True)
    
    for i, prompt in enumerate(sorted_prompts[:20], 1):
        prompt_type = prompt.get('prompt_type', '')
        score = prompt.get('quality_score', 0)
        keywords = ', '.join(prompt.get('found_keywords', [])[:2])
        
        print(f"{i:<6} {prompt_type:<15} {score:<10} {keywords}")
    
    print()
    print("=" * 80)
    print("âœ… æ”¶é›†å®Œæˆï¼")
    print("=" * 80)
    print()
    
    # ç”ŸæˆæŠ¥å‘Š
    timestamp = datetime.now().strftime('%Y-%m-%d')
    report = {
        "timestamp": datetime.now().isoformat(),
        "total_data": len(all_data),
        "unique_prompts": len(unique_prompts),
        "high_quality": len(high_quality),
        "google_prompts": google_count,
        "sora2_prompts": sora2_count,
        "google_search_keywords": GOOGLE_IMAGE_KEYWORDS,
        "sora2_search_keywords": SORA2_KEYWORDS,
        "output_file": OUTPUT_FILE
    }
    
    report_file = f"{OUTPUT_DIR}/google-sora2-manual-collection-report-{timestamp}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(json.dumps(report, indent=2, ensure_ascii=False))
    
    print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print()
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {OUTPUT_FILE}")
    print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_file}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
