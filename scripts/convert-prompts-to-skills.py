#!/usr/bin/env python3
"""
å°† Prompts è½¬æ¢æˆ Skills (å¢å¼ºç‰ˆ)
ä¿®å¤å†…å®¹ï¼š
1. å†…å®¹æå–éªŒè¯
2. ç±»å‹æ¨æ–­ï¼ˆè€Œéç¡¬ç¼–ç ï¼‰
3. å»é‡æœºåˆ¶
4. è´¨é‡å¢å¼ºéªŒè¯
5. è¯¦ç»†æ—¥å¿—è®°å½•
"""

import json
import os
import re
from datetime import datetime
import hashlib

# é…ç½®
PROMPTS_DIR = "/root/clawd/data/prompts"
SKILLS_DIR = "/root/clawd/generated-skills"
SKILLS_OUTPUT_DIR = "/root/clawd/dist/skills"
LOGS_DIR = "/root/clawd/data/conversion-logs"

# é«˜è´¨é‡é˜ˆå€¼
MIN_QUALITY_SCORE = 60

# è´¨é‡éªŒè¯è§„åˆ™
MIN_CONTENT_LENGTH = 20
MAX_CONTENT_LENGTH = 2000
TRUNCATION_MARKERS = ['...', '# 1', '# 2', 'Read more', 'continue reading', 'click to continue']
ACTION_VERBS = ['generate', 'write', 'create', 'design', 'build', 'make', 'produce', 'develop', 'craft', 'render', 'draw', 'paint', 'compose']

# ç±»å‹æ¨æ–­å…³é”®è¯
TYPE_KEYWORDS = {
    "Image Generation": ['image', 'photo', 'picture', 'render image', 'generate image', 'portrait', 'landscape', 'scene', 'visual', 'illustration'],
    "Video Generation": ['video', 'animation', 'motion', 'render video', 'generate video', 'clip', 'sequence', 'animate', 'movement'],
}

def validate_content(content):
    """
    éªŒè¯å†…å®¹è´¨é‡
    è¿”å› (is_valid, reason)
    """
    # æ£€æŸ¥é•¿åº¦
    if not content or len(content) < MIN_CONTENT_LENGTH:
        return False, f"å†…å®¹è¿‡çŸ­: {len(content) if content else 0} < {MIN_CONTENT_LENGTH}"
    
    if len(content) > MAX_CONTENT_LENGTH:
        return False, f"å†…å®¹è¿‡é•¿: {len(content)} > {MAX_CONTENT_LENGTH}"
    
    # æ£€æŸ¥æˆªæ–­æ ‡è®°
    for marker in TRUNCATION_MARKERS:
        if marker.lower() in content.lower():
            return False, f"åŒ…å«æˆªæ–­æ ‡è®°: '{marker}'"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«åŠ¨ä½œåŠ¨è¯
    content_lower = content.lower()
    has_action_verb = any(verb in content_lower for verb in ACTION_VERBS)
    
    if not has_action_verb:
        return False, "ç¼ºå°‘åŠ¨ä½œåŠ¨è¯ï¼ˆéœ€è¦ generate, write, create ç­‰ï¼‰"
    
    return True, "é€šè¿‡"

def infer_type(content):
    """
    æ ¹æ®å†…å®¹æ¨æ–­ç±»å‹
    è¿”å› inferred_type
    """
    content_lower = content.lower()
    
    # æ£€æŸ¥æ¯ä¸ªç±»å‹çš„å…³é”®è¯
    for prompt_type, keywords in TYPE_KEYWORDS.items():
        for keyword in keywords:
            if keyword in content_lower:
                return prompt_type
    
    # é»˜è®¤ä¸ºæ–‡æœ¬ prompt
    return "Text Prompt"

def create_skill_from_prompt(prompt_data, inferred_type, processed_hashes, processed_skill_names, log_file):
    """ä» prompt åˆ›å»º skillï¼ˆå¢å¼ºç‰ˆï¼‰"""
    content = prompt_data.get('content', '').strip()
    title = prompt_data.get('title', 'AI Skill')
    source = prompt_data.get('source', '')
    url = prompt_data.get('url', '')
    quality_score = prompt_data.get('quality_score', 0)
    
    # 1. å†…å®¹éªŒè¯
    is_valid, validation_reason = validate_content(content)
    
    if not is_valid:
        # è®°å½•è·³è¿‡åŸå› 
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "title": title[:100],
            "status": "skipped",
            "reason": validation_reason,
            "content_length": len(content) if content else 0,
            "quality_score": quality_score
        }
        log_file.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        log_file.flush()  # ç«‹å³ flushï¼Œç¡®ä¿æ•°æ®å†™å…¥
        return None
    
    # 2. ç”Ÿæˆ skill nameï¼ˆå…ˆç”¨äºå»é‡æ£€æŸ¥ï¼‰
    skill_name_clean = title.lower()
    skill_name_clean = re.sub(r'[^a-z0-9\s-]', '-', skill_name_clean)
    skill_name_clean = re.sub(r'\s+', '-', skill_name_clean)
    skill_name_clean = re.sub(r'-+', '-', skill_name_clean)
    skill_name_clean = skill_name_clean.strip('-')
    skill_name_clean = skill_name_clean[:50]
    
    # 3. åŸºäº skill name å»é‡ï¼ˆæ›´ç¨³å¥ï¼‰
    if skill_name_clean in processed_skill_names:
        # è®°å½•é‡å¤
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "title": title[:100],
            "status": "duplicate",
            "skill_name": skill_name_clean,
            "content_length": len(content)
        }
        log_file.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        log_file.flush()  # ç«‹å³ flushï¼Œç¡®ä¿æ•°æ®å†™å…¥
        return None
    
    processed_skill_names.add(skill_name_clean)
    
    # 4. è®¡ç®— content hashï¼ˆä¿ç•™ç”¨äºæ—¥å¿—ï¼‰
    content_hash = hashlib.md5(content.encode()).hexdigest()
    
    if content_hash in processed_hashes:
        # è®°å½•é‡å¤
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "title": title[:100],
            "status": "duplicate",
            "content_hash": content_hash,
            "content_length": len(content)
        }
        log_file.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        return None
    
    processed_hashes.add(content_hash)
    
    # 5. ç±»å‹æ¨æ–­ï¼ˆå·²ç»åœ¨ä¸Šå±‚å®Œæˆï¼Œä½†è¿™é‡Œç”¨äºè®°å½•ï¼‰
    actual_type = inferred_type
    
    # ç”Ÿæˆ descriptionï¼ˆä¸è¢«æˆªæ–­ï¼‰
    description = content[:500] + "..." if len(content) > 500 else content
    
    # 6. ç”Ÿæˆ skill
    # ç”Ÿæˆå”¯ä¸€çš„ skill nameï¼ˆæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
    skill_name_final = f"{skill_name_clean}-{content_hash[:8]}"
    
    # ç”Ÿæˆ SKILL.md with proper YAML frontmatter
    # å…ˆæ„å»º metadata JSON å¯¹è±¡
    metadata_obj = {
        "clawdbot": {
            "type": actual_type.lower(),
            "inferred_type": actual_type,
            "source": source,
            "original_url": url,
            "quality_score": quality_score
        }
    }
    import json as json_lib
    metadata_json = json_lib.dumps(metadata_obj, ensure_ascii=False)

    skill_md = f"""---
name: {skill_name_final}
description: {description}
metadata: {metadata_json}
---

# {title}

## æè¿°
{description}

## æ¥æº
- å¹³å°: {source}
- åŸå§‹é“¾æ¥: {url}
- ç±»å‹: {actual_type}
- è´¨é‡åˆ†æ•°: {quality_score}

## Prompt
```
{prompt_display}
```

---

## æ ‡ç­¾
- AI
- {actual_type}
- prompt
- ç”Ÿæˆ
- clawdbot

---

*Skill generated by Clawdbot*
"""

    # åˆ›å»º skill ç›®å½•
    skill_dir = os.path.join(SKILLS_OUTPUT_DIR, skill_name_final)
    os.makedirs(skill_dir, exist_ok=True)
    
    # ä¿å­˜ SKILL.md
    with open(os.path.join(skill_dir, "SKILL.md"), 'w', encoding='utf-8') as f:
        f.write(skill_md)
    
    # åˆ›å»º metadata.json
    metadata = {
        "name": title,
        "version": "1.0.0",
        "description": description,
        "author": "Clawdbot",
        "type": actual_type,
        "source": source,
        "url": url,
        "quality_score": quality_score,
        "content_hash": content_hash,
        "created_at": datetime.now().isoformat()
    }
    
    with open(os.path.join(skill_dir, "metadata.json"), 'w', encoding='utf-8') as f:
        f.write(json.dumps(metadata, indent=2, ensure_ascii=False))
    
    # è®°å½•æˆåŠŸ
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "title": title[:100],
        "status": "success",
        "skill_name": skill_name_final,
        "inferred_type": actual_type,
        "content_length": len(content),
        "content_hash": content_hash,
        "quality_score": quality_score
    }
    log_file.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
    
    return {
        "name": skill_name_final,
        "path": skill_dir,
        "md_file": os.path.join(skill_dir, "SKILL.md"),
        "metadata": metadata
    }

def main():
    print("=" * 80)
    print("ğŸ”„ è½¬æ¢ Prompts ä¸º Skills (å¢å¼ºç‰ˆ)")
    print("=" * 80)
    print()
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(SKILLS_OUTPUT_DIR, exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file_path = os.path.join(LOGS_DIR, f"conversion-{timestamp}.jsonl")
    
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    print()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶ - æ‰©å±•æ”¯æŒæ‰€æœ‰æ•°æ®æº
    input_file_configs = [
        ("reddit", "reddit-prompts.jsonl"),
        ("github", "github-prompts.jsonl"),
        ("github-awesome", "github-awesome-prompts.jsonl"),
        ("hackernews", "hacker-news-ai.jsonl"),
        ("collected", "collected.jsonl"),
        ("firecrawl", "firecrawl-prompts.jsonl"),
        ("image", "image-prompts.jsonl"),
        ("general", "general-prompts-v2.jsonl"),
        ("image-v2", "image-prompts-v2.jsonl"),
        ("video-v2", "video-prompts-v2.jsonl")
    ]
    
    all_skills = []
    processed_hashes = set()
    processed_skill_names = set()  # æ–°å¢ï¼šåŸºäº skill name çš„å»é‡
    
    # ç”¨äºç»Ÿè®¡
    stats = {
        "total_processed": 0,
        "converted": 0,
        "skipped_invalid": 0,
        "skipped_duplicate": 0,
        "skipped_low_quality": 0,
        "type_text": 0,
        "type_image": 0,
        "type_video": 0
    }
    
    # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
    with open(log_file_path, 'w', encoding='utf-8') as log_file:
        # å¤„ç†æ‰€æœ‰è¾“å…¥æ–‡ä»¶ - æ‰©å±•æ”¯æŒæ‰€æœ‰æ•°æ®æº
        input_files = []
        for file_type, filename in input_file_configs:
            file_path = os.path.join(PROMPTS_DIR, filename)
            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                input_files.append((file_type, file_path))
                print(f"[âœ“ å·²åŠ è½½] {file_type}: {filename} ({os.path.getsize(file_path)} bytes)")
            else:
                print(f"[âœ— è·³è¿‡] {file_type}: {filename} (ä¸å­˜åœ¨æˆ–ä¸ºç©º)")
        
        print()
        print(f"æ€»å…±åŠ è½½ {len(input_files)} ä¸ªæ•°æ®æº")
        print()
        
        # éå†æ‰€æœ‰åŠ è½½çš„æ•°æ®æº
        for file_type, file_path in input_files:
            print(f"[å¤„ç†ä¸­] {file_type} prompts: {os.path.basename(file_path)}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if not line.strip():
                        continue
                    
                    try:
                        prompt_data = json.loads(line)
                        stats["total_processed"] += 1
                        
                        # æ£€æŸ¥è´¨é‡åˆ†æ•° - æ”¯æŒå¤šç§åˆ†æ•°å­—æ®µå’ŒèŒƒå›´
                        quality_score = prompt_data.get('quality_score', 0)
                        
                        # å¦‚æœæ˜¯ collected.jsonl (SearXNG æ•°æ®)ï¼Œä½¿ç”¨ score å­—æ®µå¹¶æ˜ å°„åˆ° 0-100
                        if file_type == "collected":
                            raw_score = prompt_data.get('score', 0)
                            # SearXNG çš„ score èŒƒå›´æ˜¯ 0-5ï¼Œéœ€è¦æ˜ å°„åˆ° 0-100
                            quality_score = raw_score * 20
                        
                        # å¦‚æœæ˜¯ firecrawl æ•°æ®ï¼Œè®¡ç®—è´¨é‡åˆ†æ•°
                        elif file_type == "firecrawl":
                            content = prompt_data.get('content', '')
                            word_count = len(content.split())
                            # æ ¹æ®å†…å®¹é•¿åº¦å’Œæç¤ºè¯æ•°é‡è®¡ç®—åˆ†æ•°
                            prompts_found = prompt_data.get('prompts_found', 0)
                            quality_score = min(90, word_count / 10 + prompts_found * 10)
                        
                        # GitHub awesome prompts çš„åˆ†æ•°èŒƒå›´è¾ƒå°ï¼Œç»™äºˆé¢å¤–åŠ åˆ†
                        elif file_type == "github-awesome" and quality_score > 0:
                            quality_score = min(90, quality_score * 4)
                        
                        if quality_score < MIN_QUALITY_SCORE:
                            # è®°å½•ä½è´¨é‡è·³è¿‡
                            log_entry = {
                                "timestamp": datetime.now().isoformat(),
                                "title": prompt_data.get('title', 'Unknown')[:100],
                                "status": "skipped_low_quality",
                                "quality_score": quality_score,
                                "min_required": MIN_QUALITY_SCORE
                            }
                            log_file.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                            stats["skipped_low_quality"] += 1
                            continue
                        
                        # æ¨æ–­ç±»å‹
                        content = prompt_data.get('content', '')
                        inferred_type = infer_type(content)
                        
                        # æ›´æ–°ç±»å‹ç»Ÿè®¡
                        if inferred_type == "Text Prompt":
                            stats["type_text"] += 1
                        elif inferred_type == "Image Generation":
                            stats["type_image"] += 1
                        elif inferred_type == "Video Generation":
                            stats["type_video"] += 1
                        
                        # åˆ›å»º skill
                        skill = create_skill_from_prompt(prompt_data, inferred_type, processed_hashes, processed_skill_names, log_file)
                        
                        if skill:
                            all_skills.append(skill)
                            stats["converted"] += 1
                            if line_num % 10 == 0:
                                print(f"  å·²å¤„ç† {line_num} æ¡ï¼ŒæˆåŠŸè½¬æ¢ {stats['converted']} ä¸ª")
                        else:
                            # è¯»å–æœ€åä¸€æ¡æ—¥å¿—æ¥åˆ¤æ–­æ˜¯æ— æ•ˆè¿˜æ˜¯é‡å¤
                            try:
                                with open(log_file_path, 'r', encoding='utf-8') as log_f:
                                    lines = log_f.readlines()
                                    if lines:
                                        last_log = json.loads(lines[-1].strip())
                                        if last_log.get('status') == 'duplicate':
                                            stats["skipped_duplicate"] += 1
                                        else:
                                            stats["skipped_invalid"] += 1
                            except:
                                stats["skipped_invalid"] += 1
                            
                    except Exception as e:
                        # è®°å½•é”™è¯¯
                        log_entry = {
                            "timestamp": datetime.now().isoformat(),
                            "line_number": line_num,
                            "error": str(e),
                            "status": "error"
                        }
                        log_file.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                        print(f"  âš ï¸  è·³è¿‡ç¬¬ {line_num} è¡Œ: {e}")
            
            print(f"  âœ“ å®Œæˆ")
            print()
    
    # æ‰“åŒ…æˆ .skill æ–‡ä»¶
    print("[æ‰“åŒ…] ç”Ÿæˆ .skill æ–‡ä»¶...")
    
    packaged_count = 0
    for skill in all_skills:
        skill_name = skill["name"]
        skill_path = skill["path"]
        
        # æ‰“åŒ…æˆ zip
        import zipfile
        skill_file = os.path.join(SKILLS_OUTPUT_DIR, f"{skill_name}.skill")
        
        try:
            with zipfile.ZipFile(skill_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(skill_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, skill_path)
                        zipf.write(file_path, arcname)
            
            packaged_count += 1
        except Exception as e:
            print(f"  âš ï¸  æ‰“åŒ…å¤±è´¥ {skill_name}: {e}")
    
    print(f"  âœ“ æ‰“åŒ…å®Œæˆ: {packaged_count} ä¸ª .skill æ–‡ä»¶")
    print()
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    print("ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
    
    report_file = os.path.join(LOGS_DIR, f"conversion-report-{timestamp}.json")
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "stats": stats,
        "total_processed": stats['total_processed'],
        "total_converted": stats['converted'],
        "total_skipped": stats['skipped_invalid'] + stats['skipped_duplicate'] + stats['skipped_low_quality'],
        "total_packaged": packaged_count,
        "type_distribution": {
            "text_prompt": stats['type_text'],
            "image_generation": stats['type_image'],
            "video_generation": stats['type_video']
        },
        "output_dir": SKILLS_OUTPUT_DIR,
        "log_file": log_file_path
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(json.dumps(report, indent=2, ensure_ascii=False))
    
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print()
    print("=" * 80)
    print("âœ… è½¬æ¢å®Œæˆï¼")
    print("=" * 80)
    print()
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»è®¡å¤„ç†: {stats['total_processed']} æ¡")
    print(f"  æˆåŠŸè½¬æ¢: {stats['converted']} ä¸ª")
    print(f"  è·³è¿‡ (å†…å®¹æ— æ•ˆ): {stats['skipped_invalid']} æ¡")
    print(f"  è·³è¿‡ (é‡å¤): {stats['skipped_duplicate']} æ¡")
    print(f"  è·³è¿‡ (ä½è´¨é‡): {stats['skipped_low_quality']} æ¡")
    print()
    print(f"  ç±»å‹åˆ†å¸ƒ:")
    print(f"    Text Prompt: {stats['type_text']}")
    print(f"    Image Generation: {stats['type_image']}")
    print(f"    Video Generation: {stats['type_video']}")
    print()
    print(f"  æ‰“åŒ…: {packaged_count} ä¸ª .skill æ–‡ä»¶")
    print()
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  Skills: {SKILLS_OUTPUT_DIR}")
    print(f"  æ—¥å¿—: {log_file_path}")
    print(f"  æŠ¥å‘Š: {report_file}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
