#!/usr/bin/env python3
"""
åŸºäºç³»ç»Ÿçº§ Agent çš„æç¤ºè¯è´¨é‡è¯„åˆ†ç³»ç»Ÿ

åŠŸèƒ½ç‰¹æ€§ï¼š
- ä½¿ç”¨ Clawdbot sessions_spawn è°ƒç”¨ç³»ç»Ÿå†…ç½® LLM è¿›è¡Œè¯„ä¼°
- è¯„ä¼°æç¤ºè¯è´¨é‡ã€å®ç”¨æ€§ã€å®Œæ•´æ€§ã€åˆ›æ–°æ€§
- è¾“å‡º 0-100 åˆ†å’Œè¯¦ç»†è¯„ä¼°ç†ç”±
- æ‰¹é‡å¤„ç†å·²æœ‰æ•°æ®
- ä¸éœ€è¦å¤–éƒ¨ API key
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import logging
import subprocess

# æ—¥å¿—é…ç½®
def setup_logging(log_dir: str = "/root/clawd/logs") -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, "evaluate-prompts-agent.log")

    logger = logging.getLogger("evaluate_prompts_agent")
    logger.setLevel(logging.INFO)

    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger

logger = setup_logging()


class Config:
    """é…ç½®ç®¡ç†"""

    def __init__(self):
        self.config = self._default_config()

    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'llm_evaluation': {
                'dimensions': {
                    'quality': {'weight': 0.35, 'description': 'æç¤ºè¯çš„è´¨é‡å’Œæ¸…æ™°åº¦'},
                    'usefulness': {'weight': 0.30, 'description': 'æç¤ºè¯çš„å®ç”¨æ€§'},
                    'completeness': {'weight': 0.20, 'description': 'æç¤ºè¯çš„å®Œæ•´æ€§'},
                    'innovation': {'weight': 0.15, 'description': 'æç¤ºè¯çš„åˆ›æ–°æ€§'}
                },
                'thresholds': {
                    'excellent': 85,
                    'good': 70,
                    'average': 50
                },
                'batch_size': 10,
                'agent_model': 'default',  # ä½¿ç”¨é»˜è®¤æ¨¡å‹
                'timeout_seconds': 60
            },
            'output': {
                'data_dir': '/root/clawd/data/prompts',
                'reports_dir': '/root/clawd/reports'
            }
        }

    @property
    def dimensions(self) -> Dict:
        return self.config['llm_evaluation']['dimensions']

    @property
    def thresholds(self) -> Dict:
        return self.config['llm_evaluation']['thresholds']

    @property
    def batch_size(self) -> int:
        return self.config['llm_evaluation']['batch_size']

    @property
    def agent_model(self) -> str:
        return self.config['llm_evaluation']['agent_model']

    @property
    def timeout_seconds(self) -> int:
        return self.config['llm_evaluation']['timeout_seconds']

    @property
    def input_dir(self) -> str:
        return self.config['output']['data_dir']

    @property
    def reports_dir(self) -> str:
        return self.config['output']['reports_dir']


class AgentEvaluator:
    """ç³»ç»Ÿçº§ Agent è¯„ä¼°å™¨ - ä½¿ç”¨ sessions_spawn"""

    def __init__(self, config: Config):
        self.config = config

    def evaluate_prompt(self, prompt_text: str) -> Dict:
        """è¯„ä¼°å•ä¸ªæç¤ºè¯"""
        evaluation_prompt = self._build_evaluation_prompt(prompt_text)

        try:
            # ä½¿ç”¨ sessions_spawn è°ƒç”¨ç³»ç»Ÿå†…ç½® LLM
            result = self._call_agent(evaluation_prompt)
            parsed_result = self._parse_response(result, prompt_text)
            return parsed_result

        except Exception as e:
            logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._fallback_evaluation(prompt_text)

    def _build_evaluation_prompt(self, prompt_text: str) -> str:
        """æ„å»ºè¯„ä¼°æç¤ºè¯"""
        dimensions_desc = "\n".join([
            f"- {name}: {info['description']} (æƒé‡ {info['weight']})"
            for name, info in self.config.dimensions.items()
        ])

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æç¤ºè¯è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æç¤ºè¯è¿›è¡Œè´¨é‡è¯„ä¼°ã€‚

æç¤ºè¯:
```
{prompt_text}
```

è¯„ä¼°ç»´åº¦:
{dimensions_desc}

è¯·æä¾›ä»¥ä¸‹æ ¼å¼çš„ JSON è¾“å‡ºï¼ˆä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼Œåªè¾“å‡ºçº¯ JSONï¼‰:
{{
  "dimensions": {{
    "quality": <0-100 åˆ†>,
    "usefulness": <0-100 åˆ†>,
    "completeness": <0-100 åˆ†>,
    "innovation": <0-100 åˆ†>
  }},
  "overall_score": <0-100 åŠ æƒæ€»åˆ†>,
  "reasoning": "<è¯¦ç»†è¯„ä¼°ç†ç”±>",
  "strengths": ["<ä¼˜ç‚¹1>", "<ä¼˜ç‚¹2>"],
  "weaknesses": ["<ä¸è¶³1>", "<<ä¸è¶³2>"],
  "suggestions": ["<æ”¹è¿›å»ºè®®1>", "<æ”¹è¿›å»ºè®®2>"]
}}

è¯„ä¼°æ ‡å‡†:
- è´¨é‡ (Quality): æç¤ºè¯æ˜¯å¦æ¸…æ™°ã€å…·ä½“ã€æ— æ­§ä¹‰
- å®ç”¨æ€§ (Usefulness): æç¤ºè¯æ˜¯å¦å…·æœ‰å®é™…åº”ç”¨ä»·å€¼
- å®Œæ•´æ€§ (Completeness): æç¤ºè¯æ˜¯å¦åŒ…å«å¿…è¦çš„ä¿¡æ¯å’Œä¸Šä¸‹æ–‡
- åˆ›æ–°æ€§ (Innovation): æç¤ºè¯æ˜¯å¦æœ‰ç‹¬ç‰¹çš„åˆ›æ„æˆ–è§’åº¦

é‡è¦ï¼šåªè¾“å‡º JSONï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ ¼å¼åŒ–æ–‡æœ¬ã€‚"""

    def _call_agent(self, prompt: str) -> str:
        """è°ƒç”¨ç³»ç»Ÿçº§ Agent - ä½¿ç”¨ sessions_spawn"""
        # æ„å»º clawdbot sessions_spawn å‘½ä»¤
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é€šè¿‡ API æˆ–å­è¿›ç¨‹è°ƒç”¨
        # ç”±äº Clawdbot ä¸ç›´æ¥æä¾› Python APIï¼Œæˆ‘ä»¬ä½¿ç”¨å­è¿›ç¨‹è°ƒç”¨

        cmd = [
            'clawdbot',
            'sessions',
            'spawn',
            '--task', prompt,
            '--timeout', str(self.config.timeout_seconds),
            '--cleanup', 'delete'  # è¯„ä¼°å®Œæˆååˆ é™¤ä¼šè¯
        ]

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self.config.timeout_seconds + 10,
                cwd='/root/clawd'
            )

            # è§£æè¾“å‡º
            # sessions_spawn ä¼šå°†ç»“æœè¿”å›åˆ°ä¸»ä¼šè¯ï¼Œæˆ‘ä»¬éœ€è¦ä»è¾“å‡ºä¸­æå–
            output = result.stdout

            # å°è¯•ä»è¾“å‡ºä¸­æå– JSON
            import re
            json_match = re.search(r'\{[\s\S]*\}', output)
            if json_match:
                return json_match.group(0)

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° JSONï¼Œè¿”å›åŸå§‹è¾“å‡º
            return output

        except subprocess.TimeoutExpired:
            logger.error(f"Agent è°ƒç”¨è¶…æ—¶")
            raise
        except Exception as e:
            logger.error(f"è°ƒç”¨ Agent å¤±è´¥: {e}")
            raise

    def _parse_response(self, response: str, prompt_text: str) -> Dict:
        """è§£æ Agent å“åº”"""
        try:
            # æå– JSON
            json_match = response.strip()
            if json_match.startswith('```json'):
                json_match = json_match[7:-3].strip()
            elif json_match.startswith('```'):
                json_match = json_match[3:-3].strip()

            result = json.loads(json_match)

            # è®¡ç®—åŠ æƒæ€»åˆ†
            dimensions = result.get('dimensions', {})
            overall = sum(
                dimensions.get(dim, 0) * weight
                for dim, weight in [
                    (name, info['weight'])
                    for name, info in self.config.dimensions.items()
                ]
            )

            result['overall_score'] = round(overall, 2)
            result['prompt'] = prompt_text
            result['evaluated_at'] = datetime.now().isoformat()
            result['model'] = 'system-agent'

            return result

        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æå¤±è´¥: {e}")
            logger.debug(f"å“åº”å†…å®¹: {response}")
            return self._fallback_evaluation(prompt_text)
        except Exception as e:
            logger.error(f"å“åº”è§£æå¤±è´¥: {e}")
            return self._fallback_evaluation(prompt_text)

    def _fallback_evaluation(self, prompt_text: str) -> Dict:
        """é™çº§è¯„ä¼°ï¼ˆå½“ Agent å¤±è´¥æ—¶ï¼‰"""
        # åŸºäºè§„åˆ™çš„åŸºç¡€è¯„åˆ†
        score = 50

        # é•¿åº¦è¯„åˆ†
        if 50 <= len(prompt_text) <= 500:
            score += 10

        # åŒ…å«ç‰¹å®šå…³é”®è¯åŠ åˆ†
        keywords = ['please', 'you', 'act as', 'role', 'task', 'generate', 'create', 'write']
        if any(kw in prompt_text.lower() for kw in keywords):
            score += 10

        # ç»“æ„åŒ–åŠ åˆ†
        if '\n' in prompt_text or ':' in prompt_text or ',' in prompt_text:
            score += 5

        score = min(100, max(0, score))

        return {
            'prompt': prompt_text,
            'dimensions': {
                'quality': score,
                'usefulness': score,
                'completeness': score,
                'innovation': 50
            },
            'overall_score': score,
            'reasoning': 'Agent è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™é™çº§è¯„ä¼°',
            'strengths': ['N/A'],
            'weaknesses': ['Agent è¯„ä¼°å¤±è´¥'],
            'suggestions': ['è¯·ç¨åé‡è¯•'],
            'evaluated_at': datetime.now().isoformat(),
            'model': 'fallback'
        }


class BatchEvaluator:
    """æ‰¹é‡è¯„ä¼°å™¨"""

    def __init__(self, config: Config):
        self.config = config
        self.evaluator = AgentEvaluator(config)

    def load_prompts(self, filepath: str) -> List[Dict]:
        """åŠ è½½æç¤ºè¯æ•°æ®"""
        prompts = []

        if not os.path.exists(filepath):
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return prompts

        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        prompts.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue

        logger.info(f"åŠ è½½ {len(prompts)} æ¡æç¤ºè¯")
        return prompts

    def evaluate_batch(self, prompts: List[Dict]) -> List[Dict]:
        """æ‰¹é‡è¯„ä¼°"""
        evaluated = []
        total = len(prompts)

        for i, prompt_data in enumerate(prompts, 1):
            prompt_text = prompt_data.get('prompt', prompt_data.get('content', ''))

            if not prompt_text:
                logger.warning(f"è·³è¿‡ç©ºæç¤ºè¯: {prompt_data}")
                continue

            logger.info(f"[{i}/{total}] è¯„ä¼°æç¤ºè¯: {prompt_text[:50]}...")

            evaluation = self.evaluator.evaluate_prompt(prompt_text)

            # åˆå¹¶åŸå§‹æ•°æ®å’Œè¯„ä¼°ç»“æœ
            merged = {**prompt_data, **evaluation}
            evaluated.append(merged)

            # æ‰¹æ¬¡é—´éš”
            if i % self.config.batch_size == 0:
                logger.info(f"å·²å¤„ç† {i} æ¡ï¼Œæš‚åœ 3 ç§’...")
                time.sleep(3)

        return evaluated

    def evaluate_file(
        self,
        input_file: str,
        output_file: Optional[str] = None
    ) -> Dict:
        """è¯„ä¼°æ•´ä¸ªæ–‡ä»¶"""
        logger.info(f"å¼€å§‹è¯„ä¼°æ–‡ä»¶: {input_file}")

        prompts = self.load_prompts(input_file)

        if not prompts:
            logger.warning("æ²¡æœ‰å¯è¯„ä¼°çš„æç¤ºè¯")
            return {}

        evaluated = self.evaluate_batch(prompts)

        # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if not output_file:
            base_name = os.path.basename(input_file)
            name, ext = os.path.splitext(base_name)
            output_file = os.path.join(
                self.config.reports_dir,
                f"{name}-agent-evaluated{ext}"
            )

        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            for item in evaluated:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

        logger.info(f"ä¿å­˜è¯„ä¼°ç»“æœ: {output_file}")

        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats = self._generate_stats(evaluated)

        report_file = output_file.replace('.jsonl', '-report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)

        logger.info(f"ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š: {report_file}")

        return stats

    def _generate_stats(self, evaluated: List[Dict]) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        scores = [item.get('overall_score', 0) for item in evaluated]

        stats = {
            "timestamp": datetime.now().isoformat(),
            "total_evaluated": len(evaluated),
            "score_statistics": {
                "min": min(scores),
                "max": max(scores),
                "average": sum(scores) / len(scores) if scores else 0,
                "median": sorted(scores)[len(scores) // 2] if scores else 0
            },
            "quality_distribution": {
                "excellent": sum(1 for s in scores if s >= self.config.thresholds['excellent']),
                "good": sum(1 for s in scores if self.config.thresholds['good'] <= s < self.config.thresholds['excellent']),
                "average": sum(1 for s in scores if self.config.thresholds['average'] <= s < self.config.thresholds['good']),
                "poor": sum(1 for s in scores if s < self.config.thresholds['average'])
            },
            "thresholds": self.config.thresholds
        }

        return stats


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("ğŸ¤– å¼€å§‹åŸºäºç³»ç»Ÿçº§ Agent çš„æç¤ºè¯è´¨é‡è¯„ä¼°")
    logger.info("=" * 80)

    # åŠ è½½é…ç½®
    config = Config()

    # è·å–è¾“å…¥æ–‡ä»¶
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        # é»˜è®¤æ–‡ä»¶
        input_file = os.path.join(config.input_dir, "extracted-prompts.jsonl")

    if not os.path.exists(input_file):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        logger.info("ç”¨æ³•: python3 evaluate-prompts-agent.py <input-file> [output-file]")
        return

    # è·å–è¾“å‡ºæ–‡ä»¶
    output_file = sys.argv[2] if len(sys.argv) > 2 else None

    # æ‰§è¡Œè¯„ä¼°
    batch_evaluator = BatchEvaluator(config)
    stats = batch_evaluator.evaluate_file(input_file, output_file)

    # æ‰“å°ç»“æœ
    logger.info("=" * 80)
    logger.info("âœ… è¯„ä¼°å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
    logger.info(f"  æ€»æ•°: {stats['total_evaluated']}")
    logger.info(f"  å¹³å‡åˆ†: {stats['score_statistics']['average']:.2f}")
    logger.info(f"  æœ€é«˜åˆ†: {stats['score_statistics']['max']}")
    logger.info(f"  æœ€ä½åˆ†: {stats['score_statistics']['min']}")
    logger.info(f"ğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
    logger.info(f"  ä¼˜ç§€ (â‰¥{config.thresholds['excellent']}): {stats['quality_distribution']['excellent']}")
    logger.info(f"  è‰¯å¥½ (â‰¥{config.thresholds['good']}): {stats['quality_distribution']['good']}")
    logger.info(f"  ä¸€èˆ¬ (â‰¥{config.thresholds['average']}): {stats['quality_distribution']['average']}")
    logger.info(f"  è¾ƒå·® (<{config.thresholds['average']}): {stats['quality_distribution']['poor']}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
