#!/usr/bin/env python3
"""
æµ‹è¯•ç‰ˆæœ¬ï¼šåªæ‰§è¡Œ 2 ä¸ªæŸ¥è¯¢ï¼Œç”¨äºå¿«é€ŸéªŒè¯
"""

import json
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import logging
import requests

# é…ç½®
DATA_DIR = Path("/root/clawd/data/prompts")
OUTPUT_DIR = DATA_DIR / "collected"
OUTPUT_FILE = OUTPUT_DIR / f"test-prompts-{datetime.now().strftime('%Y%m%d-%H%M%S')}.jsonl"
LOGS_DIR = Path("/root/clawd/logs")

# åˆ›å»ºç›®å½•
DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)

# æ—¥å¿—é…ç½®
logger = logging.getLogger("test_collect_prompts")
logger.setLevel(logging.INFO)
log_handler = logging.FileHandler(LOGS_DIR / "test-collect-prompts.log", encoding='utf-8')
log_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(log_handler)
logger.addHandler(logging.StreamHandler())

# SearXNG é…ç½®
SEARXNG_URL = os.getenv("SEARXNG_URL", "http://localhost:8080")

# æµ‹è¯•ç”¨ï¼šåªæœç´¢ 2 ä¸ªæŸ¥è¯¢
SEARCH_QUERIES = [
    "awesome-chatgpt-prompts github",
    "prompt engineering tutorial"
]

# é«˜è´¨é‡åŸŸåç™½åå•
HIGH_QUALITY_DOMAINS = {
    'github.com',
    'promptbase.com',
    'learnprompting.org',
}

# ä½è´¨é‡åŸŸåé»‘åå•
LOW_QUALITY_DOMAINS = {
    'pinterest.com',
    'instagram.com',
    'tiktok.com',
    'facebook.com',
    'twitter.com',
}


def search_searxng(query: str, limit: int = 5) -> List[Dict]:
    """ä½¿ç”¨ SearXNG æœç´¢"""
    params = {
        "q": query,
        "format": "json",
        "categories": "general",
    }

    try:
        response = requests.get(
            f"{SEARXNG_URL}/search",
            params=params,
            timeout=30,
            verify=False
        )
        response.raise_for_status()

        data = response.json()
        results = data.get("results", [])[:limit]

        logger.info(f"æœç´¢ '{query}': æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")

        return results

    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥ '{query}': {e}")
        return []


def is_high_quality_url(url: str) -> bool:
    """åˆ¤æ–­ URL æ˜¯å¦æ¥è‡ªé«˜è´¨é‡æ¥æº"""
    from urllib.parse import urlparse

    try:
        domain = urlparse(url).netloc.lower()

        # æ£€æŸ¥é»‘åå•
        if any(blacklisted in domain for blacklisted in LOW_QUALITY_DOMAINS):
            return False

        # æ£€æŸ¥ç™½åå•
        if any(whitelisted in domain for whitelisted in HIGH_QUALITY_DOMAINS):
            return True

        # é»˜è®¤å…è®¸
        return True

    except Exception:
        return True


def fetch_page_content(url: str, max_chars: int = 15000) -> Optional[str]:
    """è·å–é¡µé¢å†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        text = response.text

        # ç§»é™¤ HTML æ ‡ç­¾
        import html
        text = re.sub(r'<script[^>]*>.*?</script>', ' ', text, flags=re.DOTALL)
        text = re.sub(r'<style[^>]*>.*?</style>', ' ', text, flags=re.DOTALL)
        text = re.sub(r'<[^>]+>', ' ', text)
        text = html.unescape(text)

        # æ¸…ç†ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()

        return text[:max_chars]

    except Exception as e:
        logger.warning(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
        return None


def extract_prompts_from_content(content: str, max_prompts: int = 10) -> List[str]:
    """ä»å†…å®¹ä¸­æå–æç¤ºè¯"""
    prompts = []

    # æ¨¡å¼ 1: å¼•å·ä¸­çš„å†…å®¹
    quote_patterns = [
        r'"([^"]{40,800})"',
        r"'([^']{40,800})'",
        r'`([^`]{40,800})`',
    ]

    for pattern in quote_patterns:
        matches = re.findall(pattern, content)
        prompts.extend(matches)

    # æ¨¡å¼ 2: å†’å·åé¢çš„æè¿°æ€§æ–‡æœ¬
    colon_patterns = [
        r'(?:prompt|Prompt|PROMPT|example|Example)[\s:]+([^.!?]{40,800})',
        r'(?:prompt|Prompt|PROMPT)\s*[:=]\s*([^\n]{40,800})',
    ]

    for pattern in colon_patterns:
        matches = re.findall(pattern, content)
        prompts.extend(matches)

    # å»é‡
    unique_prompts = list(dict.fromkeys(prompts))

    # è¿‡æ»¤è´¨é‡
    filtered = []
    for p in unique_prompts:
        p_clean = p.strip()

        # é•¿åº¦è¿‡æ»¤
        if not (40 <= len(p_clean) <= 800):
            continue

        # å†…å®¹è´¨é‡è¿‡æ»¤
        alpha_ratio = sum(c.isalnum() or c.isspace() for c in p_clean) / len(p_clean)
        if alpha_ratio < 0.7:
            continue

        # æ£€æŸ¥æ˜¯å¦åŒ…å«åŠ¨ä½œåŠ¨è¯
        action_verbs = ['generate', 'write', 'create', 'design', 'build', 'make', 'act as', 'role', 'task']
        has_action = any(verb.lower() in p_clean.lower() for verb in action_verbs)
        if not has_action:
            continue

        filtered.append(p_clean)

    return filtered[:max_prompts]


def classify_prompt_type(prompt: str) -> str:
    """åˆ†ç±»æç¤ºè¯ç±»å‹"""
    prompt_lower = prompt.lower()

    image_keywords = [
        'image', 'photo', 'picture', 'portrait', 'painting', 'drawing',
        'illustration', 'midjourney', 'dall-e', 'stable diffusion',
        'render', 'visual', 'art', 'scene', 'landscape'
    ]

    video_keywords = [
        'video', 'animation', 'motion', 'animate', 'runway', 'pika',
        'kling', 'veo', 'clip', 'footage', 'film'
    ]

    text_keywords = [
        'write', 'essay', 'article', 'blog', 'content', 'story',
        'chatgpt', 'gpt', 'llm', 'text generation'
    ]

    image_score = sum(1 for kw in image_keywords if kw in prompt_lower)
    video_score = sum(1 for kw in video_keywords if kw in prompt_lower)
    text_score = sum(1 for kw in text_keywords if kw in prompt_lower)

    if video_score > image_score and video_score > text_score:
        return 'video-generation'
    elif image_score > text_score:
        return 'image-generation'
    elif text_score > 0:
        return 'text-generation'
    else:
        return 'general'


def calculate_quality_score(prompt: str) -> int:
    """è®¡ç®—æç¤ºè¯è´¨é‡åˆ†æ•°"""
    score = 0

    # é•¿åº¦è¯„åˆ†
    length = len(prompt)
    if 50 <= length <= 300:
        score += 30
    elif 301 <= length <= 500:
        score += 25
    elif 501 <= length <= 800:
        score += 15
    else:
        score += 5

    # å…³é”®è¯è¯„åˆ†
    quality_keywords = [
        'detailed', 'realistic', 'high quality', 'professional', 'creative',
        'specific', 'precise', 'clear', 'comprehensive', 'well-structured'
    ]
    score += min(20, sum(5 for kw in quality_keywords if kw in prompt.lower()))

    # åŠ¨ä½œåŠ¨è¯è¯„åˆ†
    action_verbs = ['generate', 'create', 'write', 'design', 'build', 'make']
    score += min(15, sum(5 for verb in action_verbs if verb in prompt.lower()))

    # ç»“æ„è¯„åˆ†
    if ',' in prompt:
        score += 10
    if ':' in prompt:
        score += 5
    if '\n' in prompt:
        score += 10

    return min(100, score)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("ğŸ” æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ SearXNG æœç´¢ AI æç¤ºè¯ï¼ˆä»… 2 ä¸ªæŸ¥è¯¢ï¼‰")
    logger.info("=" * 80)

    # å­˜å‚¨æ‰€æœ‰æ”¶é›†çš„æç¤ºè¯
    all_prompts = []
    seen_urls = set()

    # éå†æœç´¢æŸ¥è¯¢
    for i, query in enumerate(SEARCH_QUERIES, 1):
        logger.info(f"\n[{i}/{len(SEARCH_QUERIES)}] æœç´¢: {query}")

        # æœç´¢
        results = search_searxng(query, limit=5)

        if not results:
            logger.warning(f"  æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
            continue

        # éå†ç»“æœ
        for j, result in enumerate(results, 1):
            url = result.get('url', '')
            title = result.get('title', '')

            # æ£€æŸ¥ URL è´¨é‡
            if not is_high_quality_url(url):
                logger.debug(f"  [{j}] è·³è¿‡ä½è´¨é‡æ¥æº: {url}")
                continue

            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if url in seen_urls:
                logger.debug(f"  [{j}] è·³è¿‡å·²å¤„ç† URL: {url}")
                continue

            seen_urls.add(url)

            logger.info(f"  [{j}] å¤„ç†: {title}")
            logger.debug(f"      URL: {url}")

            # è·å–é¡µé¢å†…å®¹
            content = fetch_page_content(url)

            if not content:
                logger.warning(f"      è·å–å†…å®¹å¤±è´¥")
                continue

            # æå–æç¤ºè¯
            prompts = extract_prompts_from_content(content, max_prompts=10)

            if not prompts:
                logger.debug(f"      æœªæ‰¾åˆ°æç¤ºè¯")
                continue

            logger.info(f"      æ‰¾åˆ° {len(prompts)} ä¸ªæç¤ºè¯")

            # å¤„ç†æ¯ä¸ªæç¤ºè¯
            for prompt in prompts:
                prompt_type = classify_prompt_type(prompt)
                quality_score = calculate_quality_score(prompt)

                prompt_data = {
                    'content': prompt,
                    'title': title,
                    'source': 'searxng',
                    'url': url,
                    'type': prompt_type,
                    'quality_score': quality_score,
                    'collected_at': datetime.now().isoformat(),
                    'search_query': query
                }

                all_prompts.append(prompt_data)

        # æœç´¢é—´éš”
        time.sleep(2)

    # ä¿å­˜ç»“æœ
    logger.info(f"\n{'=' * 80}")
    logger.info(f"ğŸ“Š æµ‹è¯•å®Œæˆï¼")
    logger.info(f"{'=' * 80}")
    logger.info(f"æ€»å…±æ”¶é›†: {len(all_prompts)} ä¸ªæç¤ºè¯")

    # å†™å…¥æ–‡ä»¶
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for prompt in all_prompts:
            f.write(json.dumps(prompt, ensure_ascii=False) + '\n')

    logger.info(f"ä¿å­˜åˆ°: {OUTPUT_FILE}")

    # ç»Ÿè®¡ä¿¡æ¯
    type_counts = {}
    for prompt in all_prompts:
        ptype = prompt['type']
        type_counts[ptype] = type_counts.get(ptype, 0) + 1

    logger.info(f"\nç±»å‹åˆ†å¸ƒ:")
    for ptype, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  {ptype}: {count}")

    # è´¨é‡åˆ†å¸ƒ
    high_quality = sum(1 for p in all_prompts if p['quality_score'] >= 70)
    medium_quality = sum(1 for p in all_prompts if 50 <= p['quality_score'] < 70)
    low_quality = sum(1 for p in all_prompts if p['quality_score'] < 50)

    logger.info(f"\nè´¨é‡åˆ†å¸ƒ:")
    logger.info(f"  é«˜è´¨é‡ (â‰¥70): {high_quality}")
    logger.info(f"  ä¸­ç­‰ (50-69): {medium_quality}")
    logger.info(f"  ä½è´¨é‡ (<50): {low_quality}")

    # æ˜¾ç¤ºéƒ¨åˆ†ç¤ºä¾‹
    logger.info(f"\nğŸ“ ç¤ºä¾‹æç¤ºè¯ï¼ˆå‰ 3 ä¸ªï¼‰:")
    for i, prompt in enumerate(all_prompts[:3], 1):
        logger.info(f"\n{i}. [{prompt['type']}] åˆ†æ•°: {prompt['quality_score']}")
        logger.info(f"   {prompt['content'][:100]}...")
        logger.info(f"   æ¥æº: {prompt['url']}")

    logger.info(f"\nâœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
