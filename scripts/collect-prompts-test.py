#!/usr/bin/env python3
"""æµ‹è¯•ä½¿ç”¨ SearXNG æ”¶é›† AI æç¤ºè¯æ•°æ®"""

import json
import os
from datetime import datetime
import subprocess

SEARXNG_URL = "http://localhost:8080"
OUTPUT_FILE = "/root/clawd/data/prompts/collected.jsonl"

KEYWORDS = [
    "AI prompt engineering tips",
    "ChatGPT prompts",
    "Claude prompts",
    "best AI prompts 2026",
    "prompt templates"
]

def search_searxng(query: str, limit: int = 5) -> dict:
    """ä½¿ç”¨ SearXNG æœç´¢"""
    env = os.environ.copy()
    env["SEARXNG_URL"] = SEARXNG_URL

    result = subprocess.run(
        ["python3", "/root/clawd/skills/searxng/scripts/searxng.py", "search", query, "-n", str(limit), "--format", "json"],
        capture_output=True,
        text=True,
        env=env
    )

    if result.returncode == 0:
        return json.loads(result.stdout)
    else:
        print(f"Error searching for '{query}': {result.stderr}")
        return {"results": []}

def main():
    timestamp = datetime.now().isoformat()
    all_results = []

    for keyword in KEYWORDS:
        print(f"\nğŸ” æœç´¢: {keyword}")
        data = search_searxng(keyword, limit=5)

        for idx, result in enumerate(data.get("results", []), 1):
            entry = {
                "timestamp": timestamp,
                "search_query": keyword,
                "result_index": idx,
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "content": result.get("content", ""),
                "engine": result.get("engine", ""),
                "score": result.get("score", 0)
            }
            all_results.append(entry)
            print(f"  [{idx}] {result.get('title', 'N/A')}")

    # ä¿å­˜åˆ° JSONL æ–‡ä»¶
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œå…ˆè¯»å–ç°æœ‰å†…å®¹
    existing_entries = []
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    existing_entries.append(json.loads(line))

    # åˆå¹¶æ–°æ—§æ•°æ®
    all_data = existing_entries + all_results

    # å†™å›æ–‡ä»¶
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for entry in all_data:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')

    print(f"\nâœ… å®Œæˆï¼å…±æ”¶é›† {len(all_results)} æ¡æ–°æ•°æ®")
    print(f"ğŸ“ ä¿å­˜åˆ°: {OUTPUT_FILE}")
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(all_data)} æ¡")

    # ç®€å•çš„è´¨é‡è¯„ä¼°
    print(f"\nğŸ“ˆ è´¨é‡è¯„ä¼°:")
    avg_score = sum(r.get("score", 0) for r in all_results) / len(all_results) if all_results else 0
    print(f"  - å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {avg_score:.2f}")
    print(f"  - æœ‰å®Œæ•´å†…å®¹çš„: {sum(1 for r in all_results if r.get('content'))} æ¡")
    print(f"  - æœ‰ URL çš„: {sum(1 for r in all_results if r.get('url'))} æ¡")

if __name__ == "__main__":
    main()
