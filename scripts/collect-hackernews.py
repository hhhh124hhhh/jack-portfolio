#!/usr/bin/env python3
"""
Hacker News AI ç›¸å…³å†…å®¹æ”¶é›†è„šæœ¬
ä» Hacker News è·å– AI ç›¸å…³çš„æ–‡ç« å’Œè®¨è®º
"""

import json
import requests
from datetime import datetime
from typing import List, Dict

# é…ç½®
HN_API = "https://hn.algolia.com/api/v1"
SEARCH_QUERIES = [
    "prompt engineering",
    "ChatGPT prompt",
    "AI prompt template",
    "LLM prompt",
    "prompt best practices"
]

OUTPUT_FILE = "/root/clawd/data/prompts/hacker-news-ai.jsonl"
MIN_SCORE = 10  # æœ€å°‘ 10 åˆ†ï¼ˆHN è¯„åˆ†ï¼‰

def search_hn(query: str, limit: int = 20) -> List[Dict]:
    """æœç´¢ Hacker News"""
    url = f"{HN_API}/search"
    params = {
        "query": query,
        "hitsPerPage": limit,
        "tags": "story"  # åªè·å–æ•…äº‹
    }
    
    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        hits = data.get("hits", [])
        articles = []
        
        for hit in hits:
            articles.append({
                "id": hit.get("objectID", ""),
                "title": hit.get("title", ""),
                "url": hit.get("url", f"https://news.ycombinator.com/item?id={hit.get('objectID')}"),
                "author": hit.get("author", ""),
                "points": hit.get("points", 0),
                "num_comments": hit.get("num_comments", 0),
                "created_at": hit.get("created_at", ""),
                "query": query
            })
        
        return articles
    except Exception as e:
        print(f"âš ï¸  æœç´¢ '{query}' å¤±è´¥: {e}")
        return []

def extract_prompt_from_text(text: str) -> str:
    """å°è¯•ä»æ–‡æœ¬ä¸­æå– prompt"""
    if not text:
        return ""
    
    # HN æ–‡ç« é€šå¸¸æ²¡æœ‰å®Œæ•´ promptï¼Œè¿”å›ç©ºæˆ–æ ‡é¢˜
    # ä½†å¦‚æœæœ‰ä»£ç å—ï¼Œå¯ä»¥æå–
    import re
    code_blocks = re.findall(r'```[\s\S]*?```', text)
    
    if code_blocks:
        return code_blocks[0]
    
    return ""

def is_prompt_related(article: Dict) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸º prompt ç›¸å…³å†…å®¹"""
    title = article.get("title", "").lower()
    url = article.get("url", "").lower()
    
    # Prompt ç›¸å…³å…³é”®è¯
    prompt_keywords = [
        "prompt", "template", "example", "guide",
        "prompt engineering", "prompt template",
        "llm prompt", "gpt prompt"
    ]
    
    for keyword in prompt_keywords:
        if keyword in title or keyword in url:
            return True
    
    return False

def calculate_quality_score(article: Dict) -> int:
    """è®¡ç®—æ–‡ç« è´¨é‡åˆ†æ•°ï¼ˆ0-100ï¼‰"""
    score = 0
    
    # HN è¯„åˆ†
    points = article.get("points", 0)
    if points > 0:
        score += min(30, points)
    
    # è¯„è®ºæ•°
    comments = article.get("num_comments", 0)
    score += min(20, comments * 0.5)
    
    # æ ‡é¢˜é•¿åº¦
    title = article.get("title", "")
    if 50 < len(title) < 100:
        score += 10
    
    # URL æ˜¯å¦åŒ…å«ç›¸å…³å…³é”®è¯
    url = article.get("url", "")
    if "github.com" in url or "medium.com" in url or "substack.com" in url:
        score += 10
    
    # æœç´¢è¯çš„ç›¸å…³æ€§
    query = article.get("query", "").lower()
    if "prompt" in query:
        score += 10
    if "engineering" in query:
        score += 10
    
    return min(100, score)

def main():
    print(f"\n{'='*60}")
    print(f"ğŸ” Hacker News AI å†…å®¹æ”¶é›†")
    print(f"{'='*60}\n")
    
    all_articles = []
    
    # æœç´¢æ‰€æœ‰æŸ¥è¯¢
    for query in SEARCH_QUERIES:
        print(f"ğŸ” '{query}'...", end='', flush=True)
        
        articles = search_hn(query, limit=10)
        
        # è¿‡æ»¤ prompt ç›¸å…³å†…å®¹
        prompt_articles = [a for a in articles if is_prompt_related(a)]
        
        print(f" {len(prompt_articles)} æ¡ç›¸å…³æ–‡ç« ")
        
        # è¯„åˆ†
        for article in prompt_articles:
            article["quality_score"] = calculate_quality_score(article)
        
        all_articles.extend(prompt_articles)
    
    print(f"\nğŸ“Š æ€»å…±æ”¶é›†: {len(all_articles)} æ¡æ–‡ç« ")
    
    if not all_articles:
        print("âš ï¸  æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®")
        return
    
    # ç»Ÿè®¡
    avg_score = sum(a.get("quality_score", 0) for a in all_articles) / len(all_articles)
    queries_count = {}
    for a in all_articles:
        query = a.get("query", "unknown")
        queries_count[query] = queries_count.get(query, 0) + 1
    
    print(f"\nğŸ“ˆ è´¨é‡ç»Ÿè®¡:")
    print(f"  å¹³å‡åˆ†æ•°: {avg_score:.1f}")
    print(f"  é«˜è´¨é‡ï¼ˆâ‰¥80ï¼‰: {sum(1 for a in all_articles if a.get('quality_score', 0) >= 80)} æ¡")
    print(f"  ä¸­ç­‰è´¨é‡ï¼ˆâ‰¥60ï¼‰: {sum(1 for a in all_articles if a.get('quality_score', 0) >= 60)} æ¡")
    
    print(f"\nğŸ” æŸ¥è¯¢åˆ†å¸ƒ:")
    for query, count in queries_count.items():
        print(f"  '{query}': {count} æ¡")
    
    # ä¿å­˜æ•°æ®
    import os
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    # è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
    existing_entries = []
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    existing_entries.append(json.loads(line))
    
    # å»é‡
    existing_ids = {e["id"] for e in existing_entries}
    new_entries = [e for e in all_articles if e["id"] not in existing_ids]
    
    all_data = existing_entries + new_entries
    
    # å†™å›æ–‡ä»¶
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for entry in all_data:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    
    print(f"\nğŸ’¾ æ•°æ®ä¿å­˜:")
    print(f"  æ–°å¢: {len(new_entries)} æ¡")
    print(f"  æ€»è®¡: {len(all_data)} æ¡")
    print(f"  æ–‡ä»¶: {OUTPUT_FILE}")
    
    # æ˜¾ç¤º Top 10
    print(f"\nğŸ† Top 10 æ–‡ç« :")
    top_articles = sorted(all_articles, key=lambda x: x.get("quality_score", 0), reverse=True)[:10]
    
    for i, article in enumerate(top_articles, 1):
        print(f"\n{i}. [{article.get('quality_score', 0)}] {article.get('title', 'N/A')[:60]}...")
        print(f"   æ¥æº: Hacker News | ä½œè€…: @{article.get('author', 'N/A')}")
        print(f"   è¯„åˆ†: {article.get('points', 0)} | è¯„è®º: {article.get('num_comments', 0)}")
        print(f"   é“¾æ¥: {article.get('url', 'N/A')}")
    
    print(f"\n{'='*60}")
    print(f"âœ… Hacker News æ”¶é›†å®Œæˆï¼")
    print(f"{'='*60}\n")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
