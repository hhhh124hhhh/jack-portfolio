#!/usr/bin/env python3
"""
å°† V2 Prompts è½¬æ¢æˆ Skills
ä½¿ç”¨ completeness_score â‰¥ 70 çš„é«˜è´¨é‡æç¤ºè¯
"""

import json
import os
import re
import hashlib
from datetime import datetime
from collections import defaultdict

# é…ç½®
PROMPTS_DIR = "/root/clawd/data/prompts"
SKILLS_OUTPUT_DIR = "/root/clawd/dist/skills"

# é«˜è´¨é‡é˜ˆå€¼ - ä½¿ç”¨ completeness_score
MIN_COMPLETENESS_SCORE = 70

def deduplicate_prompts(prompts):
    """
    æç¤ºè¯å»é‡ç®—æ³•
    åŸºäºå†…å®¹ç›¸ä¼¼åº¦å»é‡
    """
    # ç¬¬ä¸€å±‚ï¼šå®Œå…¨ç›¸åŒçš„æç¤ºè¯
    seen_hashes = set()
    deduped = []

    for prompt in prompts:
        content = prompt.get('content', '').strip().lower()
        content_hash = hashlib.md5(content.encode()).hexdigest()

        if content_hash not in seen_hashes:
            seen_hashes.add(content_hash)
            deduped.append(prompt)

    # ç¬¬äºŒå±‚ï¼šç›¸ä¼¼åº¦å»é‡ï¼ˆç®€å•çš„è¯è¢‹æ¨¡å‹ï¼‰
    # å»é™¤åœç”¨è¯å’Œå¸¸è§è¯
    stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
                  'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
                  'can', 'could', 'should', 'may', 'might', 'must', 'with',
                  'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of',
                  'by', 'from', 'as', 'that', 'this', 'these', 'those'}

    # è®¡ç®—å•è¯é›†åˆç›¸ä¼¼åº¦
    def get_word_set(text):
        words = re.findall(r'\b\w+\b', text.lower())
        return set(word for word in words if word not in stop_words and len(word) > 2)

    final_deduped = []
    seen_word_sets = []

    for prompt in deduped:
        content = prompt.get('content', '')
        word_set = get_word_set(content)

        # æ£€æŸ¥ä¸å·²å­˜åœ¨çš„ç›¸ä¼¼åº¦
        is_duplicate = False
        for existing_set in seen_word_sets:
            # Jaccard ç›¸ä¼¼åº¦
            intersection = len(word_set & existing_set)
            union = len(word_set | existing_set)

            if union > 0 and intersection / union > 0.85:  # 85% ç›¸ä¼¼åº¦é˜ˆå€¼
                is_duplicate = True
                break

        if not is_duplicate:
            final_deduped.append(prompt)
            seen_word_sets.append(word_set)

    return final_deduped

def create_skill_from_prompt(prompt_data, skill_type_suffix):
    """ä» prompt åˆ›å»º skill"""
    content = prompt_data.get('content', '')
    title = prompt_data.get('title', 'AI ' + skill_type_suffix)
    source = prompt_data.get('source', '')
    url = prompt_data.get('url', '')
    prompt_type = prompt_data.get('prompt_type', 'general')
    completeness_score = prompt_data.get('completeness_score', 0)

    if not content:
        return None

    # ç”Ÿæˆå®Œæ•´çš„ description
    description = content[:500] + "..." if len(content) > 500 else content
    prompt_display = content[:1000] + "..." if len(content) > 1000 else content

    # ç”Ÿæˆå”¯ä¸€çš„ skill name
    content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
    skill_name_clean = title.lower()
    skill_name_clean = re.sub(r'[^a-z0-9\s-]', '-', skill_name_clean)
    skill_name_clean = re.sub(r'\s+', '-', skill_name_clean)
    skill_name_clean = re.sub(r'-+', '-', skill_name_clean)
    skill_name_clean = skill_name_clean.strip('-')
    skill_name_clean = skill_name_clean[:50]
    skill_name_final = f"{skill_name_clean}-{content_hash}"

    # ç”Ÿæˆ SKILL.md with proper YAML frontmatter
    skill_md = f"""---
name: {skill_name_final}
description: {description}
metadata: {{"clawdbot":{{"type":"{prompt_type}","source":"{source}","original_url":"{url}","completeness_score":{completeness_score}}}}}
---

# {title}

## æè¿°
{description}

## æ¥æº
- å¹³å°: {source}
- åŸå§‹é“¾æ¥: {url}
- ç±»å‹: {prompt_type}
- å®Œæ•´æ€§åˆ†æ•°: {completeness_score}/100

## Prompt
```
{prompt_display}
```

---

## æ ‡ç­¾
- AI
- {prompt_type}
- prompt
- é«˜è´¨é‡
- å®Œæ•´æ€§-{completeness_score}

---

*Skill generated by Clawdbot from V2 prompts*
"""

    # åˆ›å»º skill ç›®å½•
    skill_dir = os.path.join(SKILLS_OUTPUT_DIR, skill_name_final)
    os.makedirs(skill_dir, exist_ok=True)

    # ä¿å­˜ SKILL.md
    with open(os.path.join(skill_dir, "SKILL.md"), 'w', encoding='utf-8') as f:
        f.write(skill_md)

    # åˆ›å»º metadata.json
    metadata = {
        "name": title,
        "version": "1.0.0",
        "description": description,
        "author": "Clawdbot",
        "type": prompt_type,
        "source": source,
        "url": url,
        "completeness_score": completeness_score,
        "created_at": datetime.now().isoformat()
    }

    with open(os.path.join(skill_dir, "metadata.json"), 'w', encoding='utf-8') as f:
        f.write(json.dumps(metadata, indent=2, ensure_ascii=False))

    return {
        "name": skill_name_final,
        "path": skill_dir,
        "md_file": os.path.join(skill_dir, "SKILL.md"),
        "metadata": metadata,
        "prompt_type": prompt_type
    }

def main():
    print("=" * 80)
    print("ğŸ”„ è½¬æ¢ V2 Prompts ä¸º Skills (ä½¿ç”¨ completeness_score â‰¥ 70)")
    print("=" * 80)
    print()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    image_prompts_file = os.path.join(PROMPTS_DIR, "image-prompts-v2.jsonl")
    video_prompts_file = os.path.join(PROMPTS_DIR, "video-prompts-v2.jsonl")
    general_prompts_file = os.path.join(PROMPTS_DIR, "general-prompts-v2.jsonl")

    all_prompts = []
    all_skills = []
    stats = {
        "image_prompts": 0,
        "video_prompts": 0,
        "general_prompts": 0,
        "high_quality": 0,
        "converted_image": 0,
        "converted_video": 0,
        "converted_general": 0,
        "skipped_low_quality": 0
    }

    # 1. è¯»å–å›¾åƒæç¤ºè¯
    if os.path.exists(image_prompts_file):
        print("[1/3] è¯»å–å›¾åƒ Prompts...")
        with open(image_prompts_file, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    prompt_data = json.loads(line)
                    all_prompts.append(prompt_data)
                    stats["image_prompts"] += 1
                except Exception as e:
                    pass
        print(f"  âœ“ è¯»å–å®Œæˆ: {stats['image_prompts']} æ¡")
        print()

    # 2. è¯»å–è§†é¢‘æç¤ºè¯
    if os.path.exists(video_prompts_file):
        print("[2/3] è¯»å–è§†é¢‘ Prompts...")
        with open(video_prompts_file, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    prompt_data = json.loads(line)
                    all_prompts.append(prompt_data)
                    stats["video_prompts"] += 1
                except Exception as e:
                    pass
        print(f"  âœ“ è¯»å–å®Œæˆ: {stats['video_prompts']} æ¡")
        print()

    # 3. è¯»å–é€šç”¨æç¤ºè¯
    if os.path.exists(general_prompts_file):
        print("[3/3] è¯»å–é€šç”¨ Prompts...")
        with open(general_prompts_file, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    prompt_data = json.loads(line)
                    all_prompts.append(prompt_data)
                    stats["general_prompts"] += 1
                except Exception as e:
                    pass
        print(f"  âœ“ è¯»å–å®Œæˆ: {stats['general_prompts']} æ¡")
        print()

    print("=" * 80)
    print("ğŸ“Š æ•°æ®è´¨é‡åˆ†æ")
    print("=" * 80)

    # 4. å»é‡
    print(f"\n[4/5] å»é‡å¤„ç†...")
    original_count = len(all_prompts)
    all_prompts = deduplicate_prompts(all_prompts)
    deduped_count = len(all_prompts)
    duplicates_removed = original_count - deduped_count
    print(f"  âœ“ åŸå§‹æç¤ºè¯: {original_count}")
    print(f"  âœ“ å»é‡å: {deduped_count}")
    print(f"  âœ“ ç§»é™¤é‡å¤: {duplicates_removed}")
    print()

    # 5. ç­›é€‰é«˜è´¨é‡æç¤ºè¯
    print(f"[5/5] ç­›é€‰é«˜è´¨é‡æç¤ºè¯ (completeness_score â‰¥ {MIN_COMPLETENESS_SCORE})...")
    high_quality_prompts = [p for p in all_prompts if p.get('completeness_score', 0) >= MIN_COMPLETENESS_SCORE]
    low_quality_prompts = [p for p in all_prompts if p.get('completeness_score', 0) < MIN_COMPLETENESS_SCORE]

    stats["high_quality"] = len(high_quality_prompts)
    stats["skipped_low_quality"] = len(low_quality_prompts)

    print(f"  âœ“ é«˜è´¨é‡ (â‰¥{MIN_COMPLETENESS_SCORE}): {stats['high_quality']} æ¡")
    print(f"  âœ“ ä½è´¨é‡ (<{MIN_COMPLETENESS_SCORE}): {stats['skipped_low_quality']} æ¡")
    print()

    # 6. æŒ‰ç±»å‹åˆ†ç±»ç”ŸæˆæŠ€èƒ½
    print("=" * 80)
    print("ğŸ¨ æŒ‰ç±»å‹ç”Ÿæˆ Skills")
    print("=" * 80)
    print()

    type_stats = defaultdict(int)
    for prompt_data in high_quality_prompts:
        prompt_type = prompt_data.get('prompt_type', 'general')
        completeness_score = prompt_data.get('completeness_score', 0)

        # ç¡®å®šæŠ€èƒ½ç±»å‹åç¼€
        if prompt_type == 'image-generation':
            suffix = "Image Generation"
        elif prompt_type == 'video-generation':
            suffix = "Video Generation"
        elif prompt_type == 'text-generation':
            suffix = "Text Generation"
        else:
            suffix = "General"

        # åˆ›å»º skill
        skill = create_skill_from_prompt(prompt_data, suffix)
        if skill:
            all_skills.append(skill)
            type_stats[prompt_type] += 1

            if prompt_type == 'image-generation':
                stats["converted_image"] += 1
            elif prompt_type == 'video-generation':
                stats["converted_video"] += 1
            elif prompt_type == 'text-generation':
                stats["converted_general"] += 1
            else:
                stats["converted_general"] += 1

    print(f"âœ… Skills ç”Ÿæˆå®Œæˆ:")
    for ptype, count in sorted(type_stats.items()):
        print(f"  - {ptype}: {count} ä¸ª")
    print()

    # 7. æ‰“åŒ…æˆ .skill æ–‡ä»¶
    print("=" * 80)
    print("ğŸ“¦ æ‰“åŒ…æˆ .skill æ–‡ä»¶")
    print("=" * 80)
    print()

    os.makedirs(SKILLS_OUTPUT_DIR, exist_ok=True)

    packaged_count = 0
    for skill in all_skills:
        skill_name = skill["name"]
        skill_path = skill["path"]

        import zipfile
        skill_file = os.path.join(SKILLS_OUTPUT_DIR, f"{skill_name}.skill")

        try:
            with zipfile.ZipFile(skill_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(skill_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, skill_path)
                        zipf.write(file_path, arcname)

            packaged_count += 1
            print(f"  âœ“ {skill_name}.skill ({skill['prompt_type']}, score: {skill['metadata'].get('completeness_score', 0)})")
        except Exception as e:
            print(f"  âš ï¸  æ‰“åŒ…å¤±è´¥ {skill_name}: {e}")

    print()
    print(f"âœ… æ‰“åŒ…å®Œæˆ: {packaged_count} ä¸ª .skill æ–‡ä»¶")
    print()

    # 8. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print("=" * 80)
    print("ğŸ“„ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
    print("=" * 80)
    print()

    timestamp = datetime.now().strftime('%Y-%m-%d')
    report_file = os.path.join(PROMPTS_DIR, f"v2-skill-generation-{timestamp}.json")

    report = {
        "timestamp": datetime.now().isoformat(),
        "config": {
            "min_completeness_score": MIN_COMPLETENESS_SCORE,
            "deduplication": "enabled",
            "similarity_threshold": 0.85
        },
        "stats": stats,
        "deduplication": {
            "original_count": original_count,
            "deduped_count": deduped_count,
            "duplicates_removed": duplicates_removed
        },
        "quality_filtering": {
            "high_quality": len(high_quality_prompts),
            "low_quality": len(low_quality_prompts),
            "pass_rate": f"{len(high_quality_prompts)/deduped_count*100:.1f}%" if deduped_count > 0 else "0%"
        },
        "type_distribution": dict(type_stats),
        "total_prompts": stats['image_prompts'] + stats['video_prompts'] + stats['general_prompts'],
        "total_converted": stats['converted_image'] + stats['converted_video'] + stats['converted_general'],
        "total_packaged": packaged_count,
        "output_dir": SKILLS_OUTPUT_DIR
    }

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(json.dumps(report, indent=2, ensure_ascii=False))

    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print()

    # 9. ç”Ÿæˆ Markdown å¯è¯»æŠ¥å‘Š
    markdown_report_file = os.path.join(PROMPTS_DIR, f"v2-skill-generation-{timestamp}.md")

    markdown_report = f"""# V2 Prompts è½¬ Skills ç”ŸæˆæŠ¥å‘Š

**æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š å¤„ç†ç»Ÿè®¡

### è¾“å…¥æ•°æ®
| æ–‡ä»¶ | æ•°é‡ |
|------|------|
| å›¾åƒ Prompts | {stats['image_prompts']} |
| è§†é¢‘ Prompts | {stats['video_prompts']} |
| é€šç”¨ Prompts | {stats['general_prompts']} |
| **æ€»è®¡** | **{stats['image_prompts'] + stats['video_prompts'] + stats['general_prompts']}** |

### æ•°æ®å¤„ç†
| é˜¶æ®µ | æ•°é‡ |
|------|------|
| åŸå§‹æç¤ºè¯ | {original_count} |
| å»é‡å | {deduped_count} |
| ç§»é™¤é‡å¤ | {duplicates_removed} |
| é«˜è´¨é‡ (â‰¥{MIN_COMPLETENESS_SCORE}) | {len(high_quality_prompts)} |
| ä½è´¨é‡ (<{MIN_COMPLETENESS_SCORE}) | {len(low_quality_prompts)} |
| **é€šè¿‡ç‡** | **{len(high_quality_prompts)/deduped_count*100:.1f}%** |

### Skill ç”Ÿæˆ
| ç±»å‹ | æ•°é‡ |
|------|------|
"""
    for ptype, count in sorted(type_stats.items()):
        markdown_report += f"| {ptype} | {count} |\n"

    markdown_report += f"""| **æ€»è®¡** | **{stats['converted_image'] + stats['converted_video'] + stats['converted_general']}** |
| æ‰“åŒ…æ–‡ä»¶ | {packaged_count} .skill |

## ğŸ¯ é…ç½®å‚æ•°

- **æœ€ä½å®Œæ•´æ€§åˆ†æ•°**: {MIN_COMPLETENESS_SCORE}
- **å»é‡ç®—æ³•**: å·²å¯ç”¨
- **ç›¸ä¼¼åº¦é˜ˆå€¼**: 85% (Jaccard ç›¸ä¼¼åº¦)
- **åœç”¨è¯è¿‡æ»¤**: å·²å¯ç”¨

## ğŸ“ˆ å®Œæ•´æ€§åˆ†æ•°åˆ†å¸ƒ

"""

    # è®¡ç®—åˆ†æ•°åˆ†å¸ƒ
    if high_quality_prompts:
        scores = [p.get('completeness_score', 0) for p in high_quality_prompts]
        excellent = sum(1 for s in scores if s >= 90)
        good = sum(1 for s in scores if 80 <= s < 90)
        medium = sum(1 for s in scores if 70 <= s < 80)

        markdown_report += f"""
| åˆ†æ•°åŒºé—´ | æ•°é‡ | å æ¯” |
|----------|------|------|
| ä¼˜ç§€ (â‰¥90) | {excellent} | {excellent/len(scores)*100:.1f}% |
| è‰¯å¥½ (80-89) | {good} | {good/len(scores)*100:.1f}% |
| åŠæ ¼ (70-79) | {medium} | {medium/len(scores)*100:.1f}% |
| **æ€»è®¡** | **{len(scores)}** | 100% |
"""

    markdown_report += f"""
## ğŸ”§ å»é‡ç®—æ³•

### ç¬¬ä¸€å±‚ï¼šå®Œå…¨é‡å¤
- ä½¿ç”¨ MD5 å“ˆå¸Œæ£€æµ‹å®Œå…¨ç›¸åŒçš„æç¤ºè¯
- ç§»é™¤æ‰€æœ‰å®Œå…¨é‡å¤çš„æ¡ç›®

### ç¬¬äºŒå±‚ï¼šç›¸ä¼¼åº¦å»é‡
- ä½¿ç”¨ Jaccard ç›¸ä¼¼åº¦ï¼ˆè¯è¢‹æ¨¡å‹ï¼‰
- å»é™¤åœç”¨è¯ï¼ˆå¦‚ "the", "and", "is" ç­‰ï¼‰
- ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š85%
- ä¿ç•™è´¨é‡æ›´é«˜ï¼ˆcompleteness_scoreï¼‰çš„ç‰ˆæœ¬

## ğŸ“ è¾“å‡ºæ–‡ä»¶

- **Skills ç›®å½•**: {SKILLS_OUTPUT_DIR}
- **JSON æŠ¥å‘Š**: {report_file}
- **Markdown æŠ¥å‘Š**: {markdown_report_file}

## ğŸ“ è´¨é‡æ ‡å‡†

### æç¤ºè¯å®Œæ•´æ€§è¯„åˆ†ï¼ˆ0-100ï¼‰
1. **é•¿åº¦é€‚å½“** (25åˆ†) - 50-300å­—ç¬¦æœ€ä¼˜
2. **åŒ…å«ä¸»è¯­** (15åˆ†) - æè¿°å¯¹è±¡
3. **æè¿°é£æ ¼** (15åˆ†) - é£æ ¼å…³é”®è¯
4. **æè¿°ç¯å¢ƒ** (15åˆ†) - ç¯å¢ƒæè¿°
5. **å…‰ç…§æè¿°** (10åˆ†) - å…‰ç…§å…³é”®è¯
6. **æƒ…ç»ªæ°›å›´** (10åˆ†) - æƒ…ç»ªæè¿°
7. **æŠ€æœ¯å‚æ•°** (10åˆ†) - å‚æ•°æ ‡è®°

## âœ… ç”Ÿæˆå®Œæˆ

æ‰€æœ‰é«˜è´¨é‡æç¤ºè¯å·²æˆåŠŸè½¬æ¢ä¸º Skillsï¼Œå¹¶æŒ‰ç±»å‹åˆ†ç±»æ‰“åŒ…ã€‚

---

*æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ | V2 Skill Conversion*
"""

    with open(markdown_report_file, 'w', encoding='utf-8') as f:
        f.write(markdown_report)

    print(f"âœ… Markdown æŠ¥å‘Šå·²ä¿å­˜: {markdown_report_file}")
    print()

    # æœ€ç»ˆæ€»ç»“
    print("=" * 80)
    print("âœ… è½¬æ¢å®Œæˆï¼")
    print("=" * 80)
    print()
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  è¾“å…¥ Prompts:")
    print(f"    å›¾åƒ: {stats['image_prompts']} æ¡")
    print(f"    è§†é¢‘: {stats['video_prompts']} æ¡")
    print(f"    é€šç”¨: {stats['general_prompts']} æ¡")
    print(f"    æ€»è®¡: {stats['image_prompts'] + stats['video_prompts'] + stats['general_prompts']} æ¡")
    print()
    print(f"  å»é‡å¤„ç†:")
    print(f"    åŸå§‹: {original_count} æ¡")
    print(f"    å»é‡å: {deduped_count} æ¡")
    print(f"    ç§»é™¤: {duplicates_removed} æ¡é‡å¤")
    print()
    print(f"  è´¨é‡ç­›é€‰ (â‰¥{MIN_COMPLETENESS_SCORE}):")
    print(f"    é«˜è´¨é‡: {len(high_quality_prompts)} æ¡")
    print(f"    ä½è´¨é‡: {len(low_quality_prompts)} æ¡")
    print()
    print(f"  Skill ç”Ÿæˆ:")
    print(f"    å›¾åƒç”Ÿæˆ: {stats['converted_image']} ä¸ª")
    print(f"    è§†é¢‘ç”Ÿæˆ: {stats['converted_video']} ä¸ª")
    print(f"    æ–‡æœ¬/é€šç”¨: {stats['converted_general']} ä¸ª")
    print(f"    æ€»è®¡: {stats['converted_image'] + stats['converted_video'] + stats['converted_general']} ä¸ª")
    print()
    print(f"  æ‰“åŒ…: {packaged_count} ä¸ª .skill æ–‡ä»¶")
    print()
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {SKILLS_OUTPUT_DIR}")
    print(f"ğŸ“„ æŠ¥å‘Š: {markdown_report_file}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
