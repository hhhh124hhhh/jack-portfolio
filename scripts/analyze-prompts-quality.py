#!/usr/bin/env python3
"""
åˆ†ææç¤ºè¯æ”¶é›†è´¨é‡ï¼Œè¯†åˆ«é—®é¢˜å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import json
from pathlib import Path
from collections import Counter
import re

DATA_FILE = Path("/root/clawd/data/prompts/collected/test-prompts-20260131-152033.jsonl")

# è¯»å–æ•°æ®
prompts = []
with open(DATA_FILE) as f:
    for line in f:
        prompts.append(json.loads(line))

print(f"æ€»å…±æ”¶é›†äº† {len(prompts)} æ¡æç¤ºè¯")
print()

# 1. æ£€æŸ¥é‡å¤ï¼ˆåŸºäº contentï¼‰
content_hashes = {}
duplicates = []
for i, p in enumerate(prompts):
    content_hash = hash(p['content'])
    if content_hash in content_hashes:
        duplicates.append((content_hashes[content_hash], i))
    else:
        content_hashes[content_hash] = i

if duplicates:
    print(f"ğŸ” å‘ç° {len(duplicates)} ç»„é‡å¤å†…å®¹ï¼š")
    for dup_idx, dup2 in duplicates:
        p1 = prompts[dup_idx]
        p2 = prompts[dup2]
        print(f"  é‡å¤ #{dup_idx} å’Œ #{dup2}")
        print(f"    URL: {p1['url']}")
        print(f"    å†…å®¹: {p1['content'][:50]}...")
        print()
else:
    print("âœ… æ²¡æœ‰å‘ç°å®Œå…¨é‡å¤çš„å†…å®¹")
print()

# 2. æ£€æŸ¥æˆªæ–­é—®é¢˜
truncated = []
for i, p in enumerate(prompts):
    content = p['content']
    # æ£€æŸ¥æ˜¯å¦ä»¥ä¸å®Œæ•´çš„å†…å®¹ç»“å°¾
    if len(content) > 20 and content[-20:].count(' ') < 2:
        truncated.append(i)

if truncated:
    print(f"ğŸ” å‘ç° {len(truncated)} æ¡å¯èƒ½æˆªæ–­çš„å†…å®¹ï¼ˆç´¢å¼•ï¼‰ï¼š{truncated[:10]}...")
    for idx in truncated[:3]:
        p = prompts[idx]
        print(f"\n  ç¤ºä¾‹ #{idx}:")
        print(f"    URL: {p['url']}")
        print(f"    å†…å®¹æœ«å°¾: {p['content'][-100:]}")
else:
    print("âœ… æ²¡æœ‰å‘ç°æ˜æ˜¾çš„æˆªæ–­é—®é¢˜")
print()

# 3. æ£€æŸ¥æ— å…³å†…å®¹ï¼ˆå¯¼èˆªæ ã€é¡µè„šï¼‰
# å¯¼èˆªæ å¸¸è§è¯
nav_keywords = ['Home', 'About', 'GitHub', 'Discord', 'Login', 'Services', 'Contact', 'Privacy']
footer_keywords = ['Copyright', 'All rights reserved', 'Terms of Service', 'Privacy Policy']

unrelated = []
for i, p in enumerate(prompts):
    content = p['content']
    # å¦‚æœå†…å®¹å¤ªçŸ­ä¸”åŒ…å«å¯¼èˆªå…³é”®è¯
    if len(content) < 100 and any(kw in content for kw in nav_keywords):
        unrelated.append((i, 'nav'))
    # å¦‚æœåŒ…å«é¡µè„šå…³é”®è¯
    if any(kw in content for kw in footer_keywords):
        unrelated.append((i, 'footer'))

if unrelated:
    print(f"ğŸ” å‘ç° {len(set(i for i, _ in unrelated))} æ¡å¯èƒ½æ— å…³çš„å†…å®¹")
    nav_count = len(set(i for i, t in unrelated if t == 'nav'))
    footer_count = len(set(i for i, t in unrelated if t == 'footer'))
    print(f"  å¯¼èˆªæ ç±»: {nav_count} æ¡")
    print(f"  é¡µè„šç±»: {footer_count} æ¡")
    print("\nç¤ºä¾‹:")
    for idx, type_ in unrelated[:3]:
        p = prompts[idx]
        print(f"\n  #{idx} ({type_}):")
        print(f"    URL: {p['url']}")
        print(f"    å†…å®¹: {p['content'][:80]}...")
else:
    print("âœ… æ²¡æœ‰å‘ç°æ˜æ˜¾çš„æ— å…³å†…å®¹")
print()

# 4. è´¨é‡è¯„åˆ†åˆ†æ
scores = [p['quality_score'] for p in prompts]
avg_score = sum(scores) / len(scores)
print(f"ğŸ“Š è´¨é‡è¯„åˆ†ç»Ÿè®¡ï¼š")
print(f"  å¹³å‡åˆ†: {avg_score:.1f}")
print(f"  æœ€é«˜åˆ†: {max(scores)}")
print(f"  æœ€ä½åˆ†: {min(scores)}")
print(f"  è¯„åˆ†åˆ†å¸ƒ: {sorted(Counter(scores).items())}")
print()

# 5. é•¿åº¦åˆ†æ
lengths = [len(p['content']) for p in prompts]
avg_length = sum(lengths) / len(lengths)
print(f"ğŸ“ å†…å®¹é•¿åº¦ç»Ÿè®¡ï¼š")
print(f"  å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
print(f"  æœ€é•¿: {max(lengths)}")
print(f"  æœ€çŸ­: {min(lengths)}")
print(f"  < 50 å­—ç¬¦: {sum(1 for l in lengths if l < 50)} æ¡")
print(f"  > 500 å­—ç¬¦: {sum(1 for l in lengths if l > 500)} æ¡")
print()

# 6. URL ç»Ÿè®¡
urls = [p['url'] for p in prompts]
url_counter = Counter(urls)
print(f"ğŸŒ URL ç»Ÿè®¡ï¼š")
print(f"  å”¯ä¸€ URL: {len(url_counter)}")
print(f"  æœ€å¸¸è§ URL:")
for url, count in url_counter.most_common(5):
    print(f"    {url} ({count} æ¬¡)")
