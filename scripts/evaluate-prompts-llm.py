#!/usr/bin/env python3
"""
åŸºäº LLM çš„æç¤ºè¯è´¨é‡è¯„åˆ†ç³»ç»Ÿ

åŠŸèƒ½ç‰¹æ€§ï¼š
- ä½¿ç”¨ Claude API è¿›è¡Œè¯­ä¹‰è¯„ä¼°
- è¯„ä¼°æç¤ºè¯è´¨é‡ã€å®ç”¨æ€§ã€å®Œæ•´æ€§ã€åˆ›æ–°æ€§
- è¾“å‡º 0-100 åˆ†å’Œè¯¦ç»†è¯„ä¼°ç†ç”±
- æ‰¹é‡å¤„ç†å·²æœ‰æ•°æ®
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import logging
import time

try:
    import yaml
except ImportError:
    print("âš ï¸  è¯·å®‰è£… PyYAML: pip install pyyaml")
    sys.exit(1)

try:
    import requests
except ImportError:
    print("âš ï¸  è¯·å®‰è£… requests: pip install requests")
    sys.exit(1)

# æ—¥å¿—é…ç½®
def setup_logging(log_dir: str = "/root/clawd/logs") -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, "evaluate-prompts-llm.log")

    logger = logging.getLogger("evaluate_prompts_llm")
    logger.setLevel(logging.INFO)

    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger

logger = setup_logging()


class Config:
    """é…ç½®ç®¡ç†"""

    def __init__(self, config_path: str = "/root/clawd/config/prompts-config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()

    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(self.config_path):
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._default_config()

        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'llm_evaluation': {
                'api_endpoint': 'https://api.anthropic.com/v1/messages',
                'api_key_env': 'ANTHROPIC_API_KEY',
                'model': 'claude-3-5-sonnet-20241022',
                'dimensions': {
                    'quality': {'weight': 0.35, 'description': 'æç¤ºè¯çš„è´¨é‡å’Œæ¸…æ™°åº¦'},
                    'usefulness': {'weight': 0.30, 'description': 'æç¤ºè¯çš„å®ç”¨æ€§'},
                    'completeness': {'weight': 0.20, 'description': 'æç¤ºè¯çš„å®Œæ•´æ€§'},
                    'innovation': {'weight': 0.15, 'description': 'æç¤ºè¯çš„åˆ›æ–°æ€§'}
                },
                'thresholds': {
                    'excellent': 85,
                    'good': 70,
                    'average': 50
                },
                'batch_size': 10,
                'max_retries': 3,
                'retry_delay_seconds': 2
            },
            'output': {
                'data_dir': '/root/clawd/data/prompts',
                'reports_dir': '/root/clawd/reports'
            }
        }

    @property
    def anthropic_api_key(self) -> Optional[str]:
        """è·å– Anthropic API Key"""
        key = os.getenv(self.config['llm_evaluation']['api_key_env'])
        if not key:
            logger.error(f"ç¯å¢ƒå˜é‡ {self.config['llm_evaluation']['api_key_env']} æœªè®¾ç½®")
        return key

    @property
    def api_endpoint(self) -> str:
        return self.config['llm_evaluation']['api_endpoint']

    @property
    def model(self) -> str:
        return self.config['llm_evaluation']['model']

    @property
    def dimensions(self) -> Dict:
        return self.config['llm_evaluation']['dimensions']

    @property
    def thresholds(self) -> Dict:
        return self.config['llm_evaluation']['thresholds']

    @property
    def batch_size(self) -> int:
        return self.config['llm_evaluation']['batch_size']

    @property
    def max_retries(self) -> int:
        return self.config['llm_evaluation']['max_retries']

    @property
    def retry_delay(self) -> int:
        return self.config['llm_evaluation']['retry_delay_seconds']

    @property
    def input_dir(self) -> str:
        return self.config['output']['data_dir']

    @property
    def reports_dir(self) -> str:
        return self.config['output']['reports_dir']


class LLMEvaluator:
    """LLM è¯„ä¼°å™¨"""

    def __init__(self, config: Config):
        self.config = config

    def evaluate_prompt(self, prompt_text: str) -> Dict:
        """è¯„ä¼°å•ä¸ªæç¤ºè¯"""
        evaluation_prompt = self._build_evaluation_prompt(prompt_text)

        for attempt in range(self.config.max_retries):
            try:
                response = self._call_claude_api(evaluation_prompt)
                result = self._parse_response(response, prompt_text)
                return result

            except Exception as e:
                logger.warning(f"è¯„ä¼°å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.max_retries}): {e}")

                if attempt < self.config.max_retries - 1:
                    time.sleep(self.config.retry_delay)
                else:
                    logger.error(f"è¯„ä¼°å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    return self._fallback_evaluation(prompt_text)

    def _build_evaluation_prompt(self, prompt_text: str) -> str:
        """æ„å»ºè¯„ä¼°æç¤ºè¯"""
        dimensions_desc = "\n".join([
            f"- {name}: {info['description']} (æƒé‡ {info['weight']})"
            for name, info in self.config.dimensions.items()
        ])

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æç¤ºè¯è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æç¤ºè¯è¿›è¡Œè´¨é‡è¯„ä¼°ã€‚

æç¤ºè¯:
```
{prompt_text}
```

è¯„ä¼°ç»´åº¦:
{dimensions_desc}

è¯·æä¾›ä»¥ä¸‹æ ¼å¼çš„ JSON è¾“å‡ºï¼ˆä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼‰:
{{
  "dimensions": {{
    "quality": <0-100 åˆ†>,
    "usefulness": <0-100 åˆ†>,
    "completeness": <0-100 åˆ†>,
    "innovation": <0-100 åˆ†>
  }},
  "overall_score": <0-100 åŠ æƒæ€»åˆ†>,
  "reasoning": "<è¯¦ç»†è¯„ä¼°ç†ç”±>",
  "strengths": ["<ä¼˜ç‚¹1>", "<ä¼˜ç‚¹2>"],
  "weaknesses": ["<ä¸è¶³1>", "<ä¸è¶³2>"],
  "suggestions": ["<æ”¹è¿›å»ºè®®1>", "<æ”¹è¿›å»ºè®®2>"]
}}

è¯„ä¼°æ ‡å‡†:
- è´¨é‡ (Quality): æç¤ºè¯æ˜¯å¦æ¸…æ™°ã€å…·ä½“ã€æ— æ­§ä¹‰
- å®ç”¨æ€§ (Usefulness): æç¤ºè¯æ˜¯å¦å…·æœ‰å®é™…åº”ç”¨ä»·å€¼
- å®Œæ•´æ€§ (Completeness): æç¤ºè¯æ˜¯å¦åŒ…å«å¿…è¦çš„ä¿¡æ¯å’Œä¸Šä¸‹æ–‡
- åˆ›æ–°æ€§ (Innovation): æç¤ºè¯æ˜¯å¦æœ‰ç‹¬ç‰¹çš„åˆ›æ„æˆ–è§’åº¦

è¯·åªè¾“å‡º JSONï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ ¼å¼åŒ–æ–‡æœ¬ã€‚"""

    def _call_claude_api(self, prompt: str) -> str:
        """è°ƒç”¨ Claude API"""
        headers = {
            "x-api-key": self.config.anthropic_api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json"
        }

        body = {
            "model": self.config.model,
            "max_tokens": 2000,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }

        response = requests.post(
            self.config.api_endpoint,
            headers=headers,
            json=body,
            timeout=60
        )
        response.raise_for_status()

        data = response.json()
        return data['content'][0]['text']

    def _parse_response(self, response: str, prompt_text: str) -> Dict:
        """è§£æ API å“åº”"""
        try:
            # æå– JSON
            json_match = response.strip()
            if json_match.startswith('```json'):
                json_match = json_match[7:-3].strip()
            elif json_match.startswith('```'):
                json_match = json_match[3:-3].strip()

            result = json.loads(json_match)

            # è®¡ç®—åŠ æƒæ€»åˆ†
            dimensions = result.get('dimensions', {})
            overall = sum(
                dimensions.get(dim, 0) * weight
                for dim, weight in [
                    (name, info['weight'])
                    for name, info in self.config.dimensions.items()
                ]
            )

            result['overall_score'] = round(overall, 2)
            result['prompt'] = prompt_text
            result['evaluated_at'] = datetime.now().isoformat()
            result['model'] = self.config.model

            return result

        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æå¤±è´¥: {e}")
            logger.debug(f"å“åº”å†…å®¹: {response}")
            return self._fallback_evaluation(prompt_text)
        except Exception as e:
            logger.error(f"å“åº”è§£æå¤±è´¥: {e}")
            return self._fallback_evaluation(prompt_text)

    def _fallback_evaluation(self, prompt_text: str) -> Dict:
        """é™çº§è¯„ä¼°ï¼ˆå½“ API å¤±è´¥æ—¶ï¼‰"""
        # åŸºäºè§„åˆ™çš„åŸºç¡€è¯„åˆ†
        score = 50

        # é•¿åº¦è¯„åˆ†
        if 50 <= len(prompt_text) <= 500:
            score += 10

        # åŒ…å«ç‰¹å®šå…³é”®è¯åŠ åˆ†
        keywords = ['please', 'you', 'act as', 'role', 'task']
        if any(kw in prompt_text.lower() for kw in keywords):
            score += 10

        # ç»“æ„åŒ–åŠ åˆ†
        if '\n' in prompt_text or ':' in prompt_text:
            score += 5

        score = min(100, max(0, score))

        return {
            'prompt': prompt_text,
            'dimensions': {
                'quality': score,
                'usefulness': score,
                'completeness': score,
                'innovation': 50
            },
            'overall_score': score,
            'reasoning': 'API å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™é™çº§è¯„ä¼°',
            'strengths': ['N/A'],
            'weaknesses': ['API è¯„ä¼°å¤±è´¥'],
            'suggestions': ['è¯·ç¨åé‡è¯•'],
            'evaluated_at': datetime.now().isoformat(),
            'model': 'fallback'
        }


class BatchEvaluator:
    """æ‰¹é‡è¯„ä¼°å™¨"""

    def __init__(self, config: Config):
        self.config = config
        self.evaluator = LLMEvaluator(config)

    def load_prompts(self, filepath: str) -> List[Dict]:
        """åŠ è½½æç¤ºè¯æ•°æ®"""
        prompts = []

        if not os.path.exists(filepath):
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return prompts

        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        prompts.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue

        logger.info(f"åŠ è½½ {len(prompts)} æ¡æç¤ºè¯")
        return prompts

    def evaluate_batch(self, prompts: List[Dict]) -> List[Dict]:
        """æ‰¹é‡è¯„ä¼°"""
        evaluated = []
        total = len(prompts)

        for i, prompt_data in enumerate(prompts, 1):
            prompt_text = prompt_data.get('prompt', '')

            logger.info(f"[{i}/{total}] è¯„ä¼°æç¤ºè¯: {prompt_text[:50]}...")

            evaluation = self.evaluator.evaluate_prompt(prompt_text)

            # åˆå¹¶åŸå§‹æ•°æ®å’Œè¯„ä¼°ç»“æœ
            merged = {**prompt_data, **evaluation}
            evaluated.append(merged)

            # æ‰¹æ¬¡é—´éš”ï¼Œé¿å… API é™æµ
            if i % self.config.batch_size == 0:
                logger.info(f"å·²å¤„ç† {i} æ¡ï¼Œæš‚åœ 2 ç§’...")
                time.sleep(2)

        return evaluated

    def evaluate_file(
        self,
        input_file: str,
        output_file: Optional[str] = None
    ) -> Dict:
        """è¯„ä¼°æ•´ä¸ªæ–‡ä»¶"""
        logger.info(f"å¼€å§‹è¯„ä¼°æ–‡ä»¶: {input_file}")

        prompts = self.load_prompts(input_file)

        if not prompts:
            logger.warning("æ²¡æœ‰å¯è¯„ä¼°çš„æç¤ºè¯")
            return {}

        evaluated = self.evaluate_batch(prompts)

        # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if not output_file:
            base_name = os.path.basename(input_file)
            name, ext = os.path.splitext(base_name)
            output_file = os.path.join(
                self.config.reports_dir,
                f"{name}-evaluated{ext}"
            )

        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            for item in evaluated:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

        logger.info(f"ä¿å­˜è¯„ä¼°ç»“æœ: {output_file}")

        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats = self._generate_stats(evaluated)

        report_file = output_file.replace('.jsonl', '-report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)

        logger.info(f"ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š: {report_file}")

        return stats

    def _generate_stats(self, evaluated: List[Dict]) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        scores = [item.get('overall_score', 0) for item in evaluated]

        stats = {
            "timestamp": datetime.now().isoformat(),
            "total_evaluated": len(evaluated),
            "score_statistics": {
                "min": min(scores),
                "max": max(scores),
                "average": sum(scores) / len(scores) if scores else 0,
                "median": sorted(scores)[len(scores) // 2] if scores else 0
            },
            "quality_distribution": {
                "excellent": sum(1 for s in scores if s >= self.config.thresholds['excellent']),
                "good": sum(1 for s in scores if self.config.thresholds['good'] <= s < self.config.thresholds['excellent']),
                "average": sum(1 for s in scores if self.config.thresholds['average'] <= s < self.config.thresholds['good']),
                "poor": sum(1 for s in scores if s < self.config.thresholds['average'])
            },
            "thresholds": self.config.thresholds
        }

        return stats


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("ğŸ¤– å¼€å§‹åŸºäº LLM çš„æç¤ºè¯è´¨é‡è¯„ä¼°")
    logger.info("=" * 80)

    # åŠ è½½é…ç½®
    config = Config()

    # æ£€æŸ¥ API Key
    if not config.anthropic_api_key:
        logger.error("âŒ ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        logger.info("æç¤º: export ANTHROPIC_API_KEY='your-key-here'")
        return

    # è·å–è¾“å…¥æ–‡ä»¶
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        # é»˜è®¤æ–‡ä»¶
        input_file = os.path.join(config.input_dir, "extracted-prompts.jsonl")

    if not os.path.exists(input_file):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        logger.info("ç”¨æ³•: python3 evaluate-prompts-llm.py <input-file> [output-file]")
        return

    # è·å–è¾“å‡ºæ–‡ä»¶
    output_file = sys.argv[2] if len(sys.argv) > 2 else None

    # æ‰§è¡Œè¯„ä¼°
    batch_evaluator = BatchEvaluator(config)
    stats = batch_evaluator.evaluate_file(input_file, output_file)

    # æ‰“å°ç»“æœ
    logger.info("=" * 80)
    logger.info("âœ… è¯„ä¼°å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
    logger.info(f"  æ€»æ•°: {stats['total_evaluated']}")
    logger.info(f"  å¹³å‡åˆ†: {stats['score_statistics']['average']:.2f}")
    logger.info(f"  æœ€é«˜åˆ†: {stats['score_statistics']['max']}")
    logger.info(f"  æœ€ä½åˆ†: {stats['score_statistics']['min']}")
    logger.info(f"ğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
    logger.info(f"  ä¼˜ç§€ (â‰¥{config.thresholds['excellent']}): {stats['quality_distribution']['excellent']}")
    logger.info(f"  è‰¯å¥½ (â‰¥{config.thresholds['good']}): {stats['quality_distribution']['good']}")
    logger.info(f"  ä¸€èˆ¬ (â‰¥{config.thresholds['average']}): {stats['quality_distribution']['average']}")
    logger.info(f"  è¾ƒå·® (<{config.thresholds['average']}): {stats['quality_distribution']['poor']}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
