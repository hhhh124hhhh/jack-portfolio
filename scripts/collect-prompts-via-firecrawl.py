#!/usr/bin/env python3
"""ä½¿ç”¨ Firecrawl API æ”¶é›† AI æç¤ºè¯æ•°æ®
è§£å†³ 403 é”™è¯¯å’Œåçˆ¬è™«ä¿æŠ¤
"""

import json
import os
import time
import requests
from datetime import datetime
from typing import List, Dict, Any

try:
    from firecrawl import Firecrawl
except ImportError:
    print("âŒ firecrawl-py æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install firecrawl-py")
    exit(1)

OUTPUT_FILE = "/root/clawd/data/prompts/firecrawl-prompts.jsonl"

# é…ç½®
API_KEY = os.environ.get("FIRECRAWL_API_KEY")
if not API_KEY:
    print("âŒ FIRECRAWL_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    exit(1)

# è¦æŠ“å–çš„ç”µå•†è§†é¢‘å’Œå›¾ç‰‡ç”Ÿæˆç›¸å…³ç½‘ç«™åˆ—è¡¨
URLS_TO_SCRAPE = [
    # ç”µå•†è§†é¢‘ç”Ÿæˆæ•™ç¨‹
    "https://www.adobe.com/express/feature/video/create/ecommerce-video",
    "https://www.canva.com/templates/s/ecommerce-product-videos/",
    "https://www.wondershare.com/ecommerce-video-maker.html",

    # Sora å’Œ OpenAI è§†é¢‘ç”Ÿæˆ
    "https://openai.com/sora",
    "https://platform.openai.com/docs/guides/sora",
    "https://github.com/openai/sora",

    # Google Veo å’Œè§†é¢‘ç”Ÿæˆ
    "https://deepmind.google/technologies/veo/",
    "https://cloud.google.com/vertex-ai/generative-ai/docs/video/veo",

    # ç”µå•†å›¾ç‰‡ç”Ÿæˆ
    "https://www.midjourney.com/docs/quick-start",
    "https://platform.openai.com/docs/guides/dall-e-3",
    "https://stability.ai/blog/stable-diffusion-3-release",

    # äº§å“æ‘„å½± AI
    "https://www.peppertype.ai/ai-product-photography",
    "https://www.flair.ai/",
    "https://www.pebblely.com/",

    # ç”µå•† AI å·¥å…·
    "https://www.claid.ai/",
    "https://www.photoroom.com/",
    "https://www.remove.bg/",

    # GitHub ç”µå•†/è§†é¢‘/å›¾ç‰‡ç”Ÿæˆèµ„æº
    "https://github.com/topics/ecommerce-ai",
    "https://github.com/topics/video-generation-ai",
    "https://github.com/topics/image-generation-ai",
]

# æœç´¢æŸ¥è¯¢
SEARCH_QUERIES = [
    "e-commerce video generation AI prompts",
    "product video AI prompts 2026",
    "Sora 2 video prompts for marketing",
    "Google Veo video generation prompts",
    "e-commerce image generation prompts",
    "product photo AI prompts Midjourney",
    "DALL-E 3 e-commerce prompts",
    "Stable Diffusion product photography",
    "AI video editing prompts for e-commerce",
    "text to video prompts workflow",
]

def scrape_with_jina_ai(url: str) -> Dict[str, Any]:
    """ä½¿ç”¨ Jina AI Reader ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
    try:
        print(f"  ğŸŒ å°è¯• Jina AI Reader: {url}")

        # Jina AI Reader API
        jina_url = f"https://r.jina.ai/http://{url.replace('https://', '').replace('http://', '')}"
        
        response = requests.get(jina_url, timeout=30)
        response.raise_for_status()
        
        content = response.text
        
        if not content or len(content.strip()) < 50:
            return {
                "url": url,
                "title": "",
                "content": "",
                "word_count": 0,
                "success": False,
                "error": "Jina AI content too short",
                "method": "jina-ai"
            }
        
        # å°è¯•æå–æ ‡é¢˜
        title = ""
        lines = content.split('\n')
        if lines:
            # ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯æ ‡é¢˜
            title = lines[0].strip('#').strip()
        
        return {
            "url": url,
            "title": title,
            "content": content[:15000],  # é™åˆ¶å­—ç¬¦æ•°
            "word_count": len(content.split()),
            "success": True,
            "method": "jina-ai"
        }
        
    except Exception as e:
        print(f"  âŒ Jina AI Reader å¤±è´¥: {e}")
        return {"url": url, "success": False, "error": f"Jina AI: {str(e)}"}

def scrape_url(url: str, app: Firecrawl) -> Dict[str, Any]:
    """æŠ“å–å•ä¸ª URL"""
    try:
        print(f"  ğŸ” æŠ“å–: {url}")

        result = app.scrape(
            url,
            formats=["markdown"],  # ä¼˜åŒ–ï¼šåªæå– markdownï¼Œæ›´å¿«
            only_main_content=True,
            wait_for=5000,  # å¢åŠ åˆ° 5 ç§’ï¼Œè®© JS å®Œå…¨æ¸²æŸ“
            timeout=60000,  # å¢åŠ åˆ° 60 ç§’è¶…æ—¶
            max_age=86400,  # 1 å¤©ç¼“å­˜
        )

        if result and hasattr(result, 'markdown'):
            # metadata æ˜¯å¯¹è±¡ï¼Œä¸æ˜¯å­—å…¸
            title = ""
            if hasattr(result, 'metadata') and result.metadata:
                if hasattr(result.metadata, 'title'):
                    title = result.metadata.title
                elif hasattr(result.metadata, 'get'):
                    title = result.metadata.get("title", "")
                else:
                    title = str(result.metadata)[:100]

            markdown_content = result.markdown
            
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œæ ‡è®°ä¸ºå¤±è´¥
            if not markdown_content or len(markdown_content.strip()) < 50:
                print(f"  âš ï¸  å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º: {len(markdown_content)} å­—ç¬¦")
                return {
                    "url": url, 
                    "title": title,
                    "content": "",
                    "word_count": 0,
                    "success": False, 
                    "error": "Content too short or empty"
                }
            
            return {
                "url": url,
                "title": title,
                "content": markdown_content,
                "word_count": len(markdown_content.split()),
                "success": True
            }
        else:
            print(f"  âŒ æŠ“å–å¤±è´¥: æ— å†…å®¹è¿”å›")
            return {"url": url, "success": False, "error": "No content returned"}

    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥: {e}")
        # å¦‚æœé‡åˆ° 403 æˆ–åçˆ¬è™«é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨ stealth æ¨¡å¼
        if "403" in str(e) or "401" in str(e) or "bot" in str(e).lower():
            print(f"  ğŸ”„ å°è¯•ä½¿ç”¨ stealth æ¨¡å¼...")
            try:
                result = app.scrape(
                    url,
                    formats=["markdown"],
                    only_main_content=True,
                    proxy="stealth",  # ä½¿ç”¨ stealth æ¨¡å¼
                    timeout=30000,
                )

                if result and hasattr(result, 'markdown'):
                    # metadata æ˜¯å¯¹è±¡ï¼Œä¸æ˜¯å­—å…¸
                    title = ""
                    if hasattr(result, 'metadata') and result.metadata:
                        if hasattr(result.metadata, 'title'):
                            title = result.metadata.title
                        elif hasattr(result.metadata, 'get'):
                            title = result.metadata.get("title", "")
                        else:
                            title = str(result.metadata)[:100]

                    return {
                        "url": url,
                        "title": title,
                        "content": result.markdown,
                        "word_count": len(result.markdown.split()),
                        "success": True,
                        "stealth_used": True,
                        "method": "firecrawl-stealth"
                    }
            except Exception as e2:
                print(f"  âŒ Stealth æ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
        
        # æœ€åçš„å›é€€ï¼šJina AI Reader
        print(f"  ğŸ”„ å°è¯• Jina AI Reader ä½œä¸ºæœ€åå›é€€...")
        jina_result = scrape_with_jina_ai(url)
        if jina_result.get("success"):
            return jina_result
        
        return {"url": url, "success": False, "error": str(e)}

def search_firecrawl(query: str, app: Firecrawl, limit: int = 5) -> List[Dict[str, Any]]:
    """ä½¿ç”¨ Firecrawl æœç´¢"""
    try:
        print(f"\nğŸ” æœç´¢: {query}")

        results = app.search(
            query,
            limit=limit,
            scrape_options={
                "formats": ["markdown"],
                "only_main_content": True
            }
        )

        formatted_results = []
        for result in results:
            formatted_results.append({
                "url": result.url if hasattr(result, 'url') else "",
                "title": result.title if hasattr(result, 'title') else "",
                "content": result.markdown if hasattr(result, 'markdown') else "",
                "word_count": len(result.markdown.split()) if hasattr(result, 'markdown') else 0,
                "success": True,
                "source": "search"
            })
            print(f"  âœ… {result.title if hasattr(result, 'title') else 'N/A'}")

        return formatted_results

    except Exception as e:
        print(f"  âŒ æœç´¢å¤±è´¥: {e}")
        return []

def extract_prompts_from_content(content: str, title: str, url: str) -> List[str]:
    """ä»å†…å®¹ä¸­æå–æç¤ºè¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
    prompts = []

    # æŸ¥æ‰¾ä»£ç å—ä¸­çš„æç¤ºè¯
    import re
    code_blocks = re.findall(r'```(?:python|javascript|json|text|bash)?\n(.*?)```', content, re.DOTALL)
    for block in code_blocks:
        block = block.strip()
        if len(block) > 50 and ('prompt' in block.lower() or 'æŒ‡ä»¤' in block):
            prompts.append(block)

    # æŸ¥æ‰¾å¼•ç”¨çš„æç¤ºè¯
    quoted = re.findall(r'"([^"]{30,200})"', content)
    for quote in quoted:
        if any(word in quote.lower() for word in ['act as', 'you are', 'please', 'å¸®æˆ‘', 'æ‰®æ¼”']):
            prompts.append(quote)

    # å»é‡
    prompts = list(set(prompts))

    return prompts[:5]  # æœ€å¤šè¿”å› 5 ä¸ª

def calculate_quality_score(content: str) -> int:
    """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•° (0-100)"""
    score = 0
    
    # é•¿åº¦è¯„åˆ†
    length = len(content)
    if 100 <= length <= 500:
        score += 20
    elif 501 <= length <= 1000:
        score += 30
    elif 1001 <= length <= 2000:
        score += 25
    elif 2001 <= length <= 5000:
        score += 15
    elif length > 5000:
        score += 10
    
    # å…³é”®è¯è¯„åˆ†
    quality_keywords = [
        'prompt', 'generate', 'create', 'write', 'design',
        'best', 'effective', 'professional', 'guide', 'tutorial'
    ]
    content_lower = content.lower()
    score += min(30, sum(3 for kw in quality_keywords if kw in content_lower))
    
    # ç»“æ„è¯„åˆ†
    if '\n\n' in content:
        score += 10  # æœ‰æ®µè½åˆ†éš”
    if any(marker in content for marker in ['##', '###', '**', '1.', '2.']):
        score += 15  # æœ‰æ ‡é¢˜æˆ–åˆ—è¡¨
    
    return min(100, score)

def main():
    timestamp = datetime.now().isoformat()
    all_entries = []

    # åˆå§‹åŒ– Firecrawl
    print("\nğŸ”¥ åˆå§‹åŒ– Firecrawl...")
    app = Firecrawl(api_key=API_KEY)

    # é˜¶æ®µ 1: æŠ“å–é¢„å®šä¹‰çš„ URL
    print("\n" + "="*60)
    print("[1/2] æŠ“å–é¢„å®šä¹‰ç½‘ç«™")
    print("="*60)

    scraped_count = 0
    failed_count = 0

    for idx, url in enumerate(URLS_TO_SCRAPE, 1):
        print(f"\n[{idx}/{len(URLS_TO_SCRAPE)}]")

        result = scrape_url(url, app)

        if result.get("success"):
            scraped_count += 1
            
            # è·å–å®Œæ•´å†…å®¹
            full_content = result.get("content", "")
            title = result.get("title", "")
            
            # ä¿å­˜é¡µé¢å†…å®¹æœ¬èº«ä½œä¸ºæç¤ºè¯
            # å¦‚æœå†…å®¹å¤ªé•¿åˆ™æˆªå–
            content_to_save = full_content[:15000] if len(full_content) > 15000 else full_content
            
            if content_to_save:
                # è®¡ç®—è´¨é‡åˆ†æ•°
                quality_score = calculate_quality_score(content_to_save)
                
                entry = {
                    "timestamp": timestamp,
                    "source": "firecrawl",
                    "method": "scrape",
                    "url": url,
                    "title": title,
                    "content": content_to_save,
                    "word_count": len(content_to_save.split()),
                    "quality_score": quality_score,
                    "success": True,
                    "stealth_used": result.get("stealth_used", False)
                }
                all_entries.append(entry)
                print(f"  âœ… ä¿å­˜é¡µé¢å†…å®¹ ({len(content_to_save)} å­—ç¬¦, è´¨é‡åˆ†æ•°: {quality_score})")
            
            # é¢å¤–æå–æç¤ºè¯ï¼ˆä½œä¸ºè¡¥å……ï¼‰
            prompts = extract_prompts_from_content(full_content, title, url)
            
            if prompts:
                print(f"  ğŸ’¡ é¢å¤–æå– {len(prompts)} ä¸ªæç¤ºè¯")
                
                # åªä¿å­˜å‰ 3 ä¸ªé¢å¤–æç¤ºè¯
                for i, prompt in enumerate(prompts[:3], 1):
                    # é¿å…ä¸ä¸»å†…å®¹é‡å¤ï¼ˆç®€å•æ£€æŸ¥ï¼‰
                    if len(prompt) < len(full_content) * 0.5:  # æç¤ºè¯æ¯”æ­£æ–‡çŸ­å¾ˆå¤š
                        quality_score = calculate_quality_score(prompt)
                        
                        entry = {
                            "timestamp": timestamp,
                            "source": "firecrawl",
                            "method": "scrape",
                            "url": url,
                            "title": f"{title} (extracted-{i})",
                            "content": prompt,
                            "quality_score": quality_score,
                            "success": True,
                            "stealth_used": result.get("stealth_used", False)
                        }
                        all_entries.append(entry)
        else:
            failed_count += 1
            entry = {
                "timestamp": timestamp,
                "source": "firecrawl",
                "method": "scrape",
                "url": url,
                "success": False,
                "error": result.get("error", "Unknown error")
            }
            all_entries.append(entry)

        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(1)

    # é˜¶æ®µ 2: æœç´¢æŸ¥è¯¢
    print("\n" + "="*60)
    print("[2/2] æœç´¢ AI æç¤ºè¯")
    print("="*60)

    search_count = 0
    for idx, query in enumerate(SEARCH_QUERIES, 1):
        print(f"\n[{idx}/{len(SEARCH_QUERIES)}]")

        results = search_firecrawl(query, app, limit=3)

        search_count += len(results)

        for result in results:
            # è·å–å®Œæ•´å†…å®¹
            full_content = result.get("content", "")
            title = result.get("title", "")
            url = result.get("url", "")
            
            # ä¿å­˜é¡µé¢å†…å®¹æœ¬èº«ä½œä¸ºæç¤ºè¯
            # å¦‚æœå†…å®¹å¤ªé•¿åˆ™æˆªå–
            content_to_save = full_content[:15000] if len(full_content) > 15000 else full_content
            
            if content_to_save:
                # è®¡ç®—è´¨é‡åˆ†æ•°
                quality_score = calculate_quality_score(content_to_save)
                
                entry = {
                    "timestamp": timestamp,
                    "source": "firecrawl",
                    "method": "search",
                    "search_query": query,
                    "url": url,
                    "title": title,
                    "content": content_to_save,
                    "word_count": len(content_to_save.split()),
                    "quality_score": quality_score,
                    "success": True
                }
                all_entries.append(entry)
                print(f"  âœ… ä¿å­˜é¡µé¢å†…å®¹ ({len(content_to_save)} å­—ç¬¦, è´¨é‡åˆ†æ•°: {quality_score})")
            
            # é¢å¤–æå–æç¤ºè¯ï¼ˆä½œä¸ºè¡¥å……ï¼‰
            prompts = extract_prompts_from_content(full_content, title, url)
            
            if prompts:
                print(f"  ğŸ’¡ é¢å¤–æå– {len(prompts)} ä¸ªæç¤ºè¯")
                
                # åªä¿å­˜å‰ 3 ä¸ªé¢å¤–æç¤ºè¯
                for i, prompt in enumerate(prompts[:3], 1):
                    # é¿å…ä¸ä¸»å†…å®¹é‡å¤
                    if len(prompt) < len(full_content) * 0.5:  # æç¤ºè¯æ¯”æ­£æ–‡çŸ­å¾ˆå¤š
                        quality_score = calculate_quality_score(prompt)
                        
                        entry = {
                            "timestamp": timestamp,
                            "source": "firecrawl",
                            "method": "search",
                            "search_query": query,
                            "url": url,
                            "title": f"{title} (extracted-{i})",
                            "content": prompt,
                            "quality_score": quality_score,
                            "success": True
                        }
                        all_entries.append(entry)

        time.sleep(2)

    # ä¿å­˜åˆ° JSONL
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    # è¯»å–ç°æœ‰æ•°æ®
    existing_entries = []
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    existing_entries.append(json.loads(line))

    # åˆå¹¶æ•°æ®
    all_data = existing_entries + all_entries

    # å†™å›æ–‡ä»¶
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for entry in all_data:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')

    # ç»Ÿè®¡
    total_prompts = sum(e.get("prompts_found", 0) for e in all_entries if e.get("success"))
    stealth_used_count = sum(1 for e in all_entries if e.get("stealth_used"))

    print("\n" + "="*60)
    print("âœ… æ”¶é›†å®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"  â€¢ æŠ“å–æˆåŠŸ: {scraped_count}/{len(URLS_TO_SCRAPE)}")
    print(f"  â€¢ æœç´¢ç»“æœ: {search_count} æ¡")
    print(f"  â€¢ å¤±è´¥: {failed_count}")
    print(f"  â€¢ æå–çš„æç¤ºè¯: {total_prompts} ä¸ª")
    print(f"  â€¢ ä½¿ç”¨ stealth æ¨¡å¼: {stealth_used_count} æ¬¡")
    print(f"\nğŸ“ æ–‡ä»¶: {OUTPUT_FILE}")
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(all_data)} æ¡")

    return {
        "scraped": scraped_count,
        "searched": search_count,
        "failed": failed_count,
        "prompts": total_prompts,
        "stealth": stealth_used_count
    }

if __name__ == "__main__":
    main()
