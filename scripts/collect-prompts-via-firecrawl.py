#!/usr/bin/env python3
"""ä½¿ç”¨ Firecrawl API æ”¶é›† AI æç¤ºè¯æ•°æ®
è§£å†³ 403 é”™è¯¯å’Œåçˆ¬è™«ä¿æŠ¤
"""

import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any

try:
    from firecrawl import Firecrawl
except ImportError:
    print("âŒ firecrawl-py æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install firecrawl-py")
    exit(1)

OUTPUT_FILE = "/root/clawd/data/prompts/firecrawl-prompts.jsonl"

# é…ç½®
API_KEY = os.environ.get("FIRECRAWL_API_KEY")
if not API_KEY:
    print("âŒ FIRECRAWL_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    exit(1)

# è¦æŠ“å–çš„ AI æç¤ºè¯ç›¸å…³ç½‘ç«™åˆ—è¡¨
URLS_TO_SCRAPE = [
    # AI æç¤ºè¯æ•™ç¨‹å’ŒæŒ‡å—
    "https://www.promptingguide.ai/",
    "https://platform.openai.com/docs/guides/prompt-engineering",
    "https://github.com/dair-ai/Prompt-Engineering-Guide",
    "https://www.deeplearning.ai/ai-notes/prompt-engineering/",
    "https://github.com/f/awesome-chatgpt-prompts",
    "https://github.com/mattnigh/ChatGPT3-Free-Prompt-List",

    # AI å·¥å…·å¹³å°
    "https://www.promptbase.com/",
    "https://flowgpt.com/",
    "https://huggingface.co/prompts",

    # æ•™ç¨‹å’Œåšå®¢
    "https://simonwillison.net/tags/llm/",
    "https://www.anthropic.com/index/prompt-engineering",
    "https://docs.anthropic.com/claude/docs/prompt-engineering",
    "https://www.kdnuggets.com/tag/prompt-engineering",

    # ç¤¾åŒºèµ„æº
    "https://www.reddit.com/r/ChatGPTPromptGenius/",
    "https://www.reddit.com/r/LocalLLaMA/",
    "https://www.reddit.com/r/PromptEngineering/",
]

# æœç´¢æŸ¥è¯¢
SEARCH_QUERIES = [
    "AI prompt engineering best practices 2026",
    "ChatGPT prompts for developers",
    "Claude prompt techniques",
    "AI art prompts midjourney",
    "prompt templates for business",
]

def scrape_url(url: str, app: Firecrawl) -> Dict[str, Any]:
    """æŠ“å–å•ä¸ª URL"""
    try:
        print(f"  ğŸ” æŠ“å–: {url}")

        result = app.scrape(
            url,
            formats=["markdown"],
            only_main_content=True,
            wait_for=3000,  # ç­‰å¾… 3 ç§’è®© JS æ¸²æŸ“
            timeout=30000,
            max_age=86400,  # 1 å¤©ç¼“å­˜
        )

        if result and hasattr(result, 'markdown'):
            # metadata æ˜¯å¯¹è±¡ï¼Œä¸æ˜¯å­—å…¸
            title = ""
            if hasattr(result, 'metadata') and result.metadata:
                if hasattr(result.metadata, 'title'):
                    title = result.metadata.title
                elif hasattr(result.metadata, 'get'):
                    title = result.metadata.get("title", "")
                else:
                    title = str(result.metadata)[:100]

            return {
                "url": url,
                "title": title,
                "content": result.markdown,
                "word_count": len(result.markdown.split()),
                "success": True
            }
        else:
            print(f"  âŒ æŠ“å–å¤±è´¥: æ— å†…å®¹è¿”å›")
            return {"url": url, "success": False, "error": "No content"}

    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥: {e}")
        # å¦‚æœé‡åˆ° 403 æˆ–åçˆ¬è™«é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨ stealth æ¨¡å¼
        if "403" in str(e) or "401" in str(e) or "bot" in str(e).lower():
            print(f"  ğŸ”„ å°è¯•ä½¿ç”¨ stealth æ¨¡å¼...")
            try:
                result = app.scrape(
                    url,
                    formats=["markdown"],
                    only_main_content=True,
                    proxy="stealth",  # ä½¿ç”¨ stealth æ¨¡å¼
                    timeout=30000,
                )

                if result and hasattr(result, 'markdown'):
                    # metadata æ˜¯å¯¹è±¡ï¼Œä¸æ˜¯å­—å…¸
                    title = ""
                    if hasattr(result, 'metadata') and result.metadata:
                        if hasattr(result.metadata, 'title'):
                            title = result.metadata.title
                        elif hasattr(result.metadata, 'get'):
                            title = result.metadata.get("title", "")
                        else:
                            title = str(result.metadata)[:100]

                    return {
                        "url": url,
                        "title": title,
                        "content": result.markdown,
                        "word_count": len(result.markdown.split()),
                        "success": True,
                        "stealth_used": True
                    }
            except Exception as e2:
                print(f"  âŒ Stealth æ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
                return {"url": url, "success": False, "error": str(e2)}

        return {"url": url, "success": False, "error": str(e)}

def search_firecrawl(query: str, app: Firecrawl, limit: int = 5) -> List[Dict[str, Any]]:
    """ä½¿ç”¨ Firecrawl æœç´¢"""
    try:
        print(f"\nğŸ” æœç´¢: {query}")

        results = app.search(
            query,
            limit=limit,
            scrape_options={
                "formats": ["markdown"],
                "only_main_content": True
            }
        )

        formatted_results = []
        for result in results:
            formatted_results.append({
                "url": result.url if hasattr(result, 'url') else "",
                "title": result.title if hasattr(result, 'title') else "",
                "content": result.markdown if hasattr(result, 'markdown') else "",
                "word_count": len(result.markdown.split()) if hasattr(result, 'markdown') else 0,
                "success": True,
                "source": "search"
            })
            print(f"  âœ… {result.title if hasattr(result, 'title') else 'N/A'}")

        return formatted_results

    except Exception as e:
        print(f"  âŒ æœç´¢å¤±è´¥: {e}")
        return []

def extract_prompts_from_content(content: str, title: str, url: str) -> List[str]:
    """ä»å†…å®¹ä¸­æå–æç¤ºè¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
    prompts = []

    # æŸ¥æ‰¾ä»£ç å—ä¸­çš„æç¤ºè¯
    import re
    code_blocks = re.findall(r'```(?:python|javascript|json|text|bash)?\n(.*?)```', content, re.DOTALL)
    for block in code_blocks:
        block = block.strip()
        if len(block) > 50 and ('prompt' in block.lower() or 'æŒ‡ä»¤' in block):
            prompts.append(block)

    # æŸ¥æ‰¾å¼•ç”¨çš„æç¤ºè¯
    quoted = re.findall(r'"([^"]{30,200})"', content)
    for quote in quoted:
        if any(word in quote.lower() for word in ['act as', 'you are', 'please', 'å¸®æˆ‘', 'æ‰®æ¼”']):
            prompts.append(quote)

    # å»é‡
    prompts = list(set(prompts))

    return prompts[:5]  # æœ€å¤šè¿”å› 5 ä¸ª

def main():
    timestamp = datetime.now().isoformat()
    all_entries = []

    # åˆå§‹åŒ– Firecrawl
    print("\nğŸ”¥ åˆå§‹åŒ– Firecrawl...")
    app = Firecrawl(api_key=API_KEY)

    # é˜¶æ®µ 1: æŠ“å–é¢„å®šä¹‰çš„ URL
    print("\n" + "="*60)
    print("[1/2] æŠ“å–é¢„å®šä¹‰ç½‘ç«™")
    print("="*60)

    scraped_count = 0
    failed_count = 0

    for idx, url in enumerate(URLS_TO_SCRAPE, 1):
        print(f"\n[{idx}/{len(URLS_TO_SCRAPE)}]")

        result = scrape_url(url, app)

        if result.get("success"):
            scraped_count += 1

            # æå–æç¤ºè¯
            prompts = extract_prompts_from_content(
                result.get("content", ""),
                result.get("title", ""),
                url
            )

            entry = {
                "timestamp": timestamp,
                "source": "firecrawl",
                "method": "scrape",
                "url": url,
                "title": result.get("title", ""),
                "content": result.get("content", "")[:2000],  # é™åˆ¶å†…å®¹é•¿åº¦
                "word_count": result.get("word_count", 0),
                "prompts_found": len(prompts),
                "prompts": prompts[:3],  # ä¿å­˜å‰ 3 ä¸ªæç¤ºè¯
                "stealth_used": result.get("stealth_used", False)
            }
            all_entries.append(entry)
        else:
            failed_count += 1
            entry = {
                "timestamp": timestamp,
                "source": "firecrawl",
                "method": "scrape",
                "url": url,
                "success": False,
                "error": result.get("error", "Unknown error")
            }
            all_entries.append(entry)

        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(1)

    # é˜¶æ®µ 2: æœç´¢æŸ¥è¯¢
    print("\n" + "="*60)
    print("[2/2] æœç´¢ AI æç¤ºè¯")
    print("="*60)

    search_count = 0
    for idx, query in enumerate(SEARCH_QUERIES, 1):
        print(f"\n[{idx}/{len(SEARCH_QUERIES)}]")

        results = search_firecrawl(query, app, limit=3)

        search_count += len(results)

        for result in results:
            prompts = extract_prompts_from_content(
                result.get("content", ""),
                result.get("title", ""),
                result.get("url", "")
            )

            entry = {
                "timestamp": timestamp,
                "source": "firecrawl",
                "method": "search",
                "search_query": query,
                "url": result.get("url", ""),
                "title": result.get("title", ""),
                "content": result.get("content", "")[:2000],
                "word_count": result.get("word_count", 0),
                "prompts_found": len(prompts),
                "prompts": prompts[:3]
            }
            all_entries.append(entry)

        time.sleep(2)

    # ä¿å­˜åˆ° JSONL
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    # è¯»å–ç°æœ‰æ•°æ®
    existing_entries = []
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    existing_entries.append(json.loads(line))

    # åˆå¹¶æ•°æ®
    all_data = existing_entries + all_entries

    # å†™å›æ–‡ä»¶
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for entry in all_data:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')

    # ç»Ÿè®¡
    total_prompts = sum(e.get("prompts_found", 0) for e in all_entries if e.get("success"))
    stealth_used_count = sum(1 for e in all_entries if e.get("stealth_used"))

    print("\n" + "="*60)
    print("âœ… æ”¶é›†å®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"  â€¢ æŠ“å–æˆåŠŸ: {scraped_count}/{len(URLS_TO_SCRAPE)}")
    print(f"  â€¢ æœç´¢ç»“æœ: {search_count} æ¡")
    print(f"  â€¢ å¤±è´¥: {failed_count}")
    print(f"  â€¢ æå–çš„æç¤ºè¯: {total_prompts} ä¸ª")
    print(f"  â€¢ ä½¿ç”¨ stealth æ¨¡å¼: {stealth_used_count} æ¬¡")
    print(f"\nğŸ“ æ–‡ä»¶: {OUTPUT_FILE}")
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(all_data)} æ¡")

    return {
        "scraped": scraped_count,
        "searched": search_count,
        "failed": failed_count,
        "prompts": total_prompts,
        "stealth": stealth_used_count
    }

if __name__ == "__main__":
    main()
