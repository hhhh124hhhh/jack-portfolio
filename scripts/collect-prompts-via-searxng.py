#!/usr/bin/env python3
"""
ä½¿ç”¨ SearXNG æœç´¢é«˜è´¨é‡ AI æç¤ºè¯

åŠŸèƒ½ç‰¹æ€§ï¼š
- ä½¿ç”¨æœ¬åœ° SearXNG å®ä¾‹æœç´¢ AI æç¤ºè¯
- ä»å¤šä¸ªæ¥æºæ”¶é›†æç¤ºè¯ï¼ˆGitHubã€åšå®¢ã€ä¸“ä¸šç½‘ç«™ï¼‰
- é«˜è´¨é‡æå–å’ŒéªŒè¯
- æ”¯æŒå¤šç§æœç´¢ç­–ç•¥
"""

import json
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import logging
import requests

# é…ç½®
DATA_DIR = Path("/root/clawd/data/prompts")
OUTPUT_DIR = DATA_DIR / "collected"
OUTPUT_FILE = OUTPUT_DIR / f"prompts-from-searxng-{datetime.now().strftime('%Y%m%d')}.jsonl"
LOGS_DIR = Path("/root/clawd/logs")

# åˆ›å»ºç›®å½•
DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)

# æ—¥å¿—é…ç½®
logger = logging.getLogger("collect_prompts_searxng")
logger.setLevel(logging.INFO)
log_handler = logging.FileHandler(LOGS_DIR / "collect-prompts-searxng.log", encoding='utf-8')
log_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(log_handler)
logger.addHandler(logging.StreamHandler())

# SearXNG é…ç½®
SEARXNG_URL = os.getenv("SEARXNG_URL", "http://localhost:8080")

# æœç´¢å…³é”®è¯ - é’ˆå¯¹é«˜è´¨é‡æ¥æº
SEARCH_QUERIES = [
    # ä¸“ä¸šæç¤ºè¯ç½‘ç«™
    "PromptBase best prompts",
    "awesome-chatgpt-prompts github",
    "LearnPrompting engineering guide",

    # æŠ€æœ¯åšå®¢å’Œæ•™ç¨‹
    "Midjourney prompt tutorial examples",
    "DALL-E 3 prompt examples guide",
    "Stable Diffusion prompt engineering",
    "AI art prompt best practices",

    # GitHub èµ„æº
    "site:github.com \"prompt engineering\"",
    "site:github.com \"AI prompts\"",
    "site:github.com \"ChatGPT prompts\"",

    # æŠ€æœ¯å¹³å°
    "site:medium.com \"prompt engineering\"",
    "site:dev.to \"AI prompts\"",
    "site:hashnode.com \"prompt guide\"",

    # è§†é¢‘ç”Ÿæˆ
    "Runway ML prompt examples",
    "Pika Labs prompt guide",
    "Kling AI prompt examples",
    "Veo video prompts",

    # ç‰¹å®šç”¨é€”
    "professional ChatGPT prompts",
    "business AI prompt templates",
    "educational AI prompts",
]

# é«˜è´¨é‡åŸŸåç™½åå•
HIGH_QUALITY_DOMAINS = {
    'github.com',
    'promptbase.com',
    'learnprompting.org',
    'midjourney.com',
    'openai.com',
    'stability.ai',
    'huggingface.co',
    'medium.com',
    'dev.to',
    'hashnode.com',
    'towardsdatascience.com',
    'analyticsvidhya.com',
}

# ä½è´¨é‡åŸŸåé»‘åå•
LOW_QUALITY_DOMAINS = {
    'pinterest.com',  # å¤ªå¤šä½è´¨é‡å›¾ç‰‡
    'instagram.com',  # ç¤¾äº¤åª’ä½“
    'tiktok.com',  # ç¤¾äº¤åª’ä½“
    'facebook.com',  # ç¤¾äº¤åª’ä½“
    'twitter.com',  # ç¤¾äº¤åª’ä½“ï¼ˆä½¿ç”¨ twitter-search skillï¼‰
}


def search_searxng(query: str, limit: int = 10) -> List[Dict]:
    """
    ä½¿ç”¨ SearXNG æœç´¢

    Args:
        query: æœç´¢æŸ¥è¯¢
        limit: è¿”å›ç»“æœæ•°é‡

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    params = {
        "q": query,
        "format": "json",
        "categories": "general",
    }

    try:
        response = requests.get(
            f"{SEARXNG_URL}/search",
            params=params,
            timeout=30,
            verify=False
        )
        response.raise_for_status()

        data = response.json()
        results = data.get("results", [])[:limit]

        logger.info(f"æœç´¢ '{query}': æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")

        return results

    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥ '{query}': {e}")
        return []


def is_high_quality_url(url: str) -> bool:
    """
    åˆ¤æ–­ URL æ˜¯å¦æ¥è‡ªé«˜è´¨é‡æ¥æº

    Args:
        url: ç›®æ ‡ URL

    Returns:
        æ˜¯å¦ä¸ºé«˜è´¨é‡æ¥æº
    """
    from urllib.parse import urlparse

    try:
        domain = urlparse(url).netloc.lower()

        # æ£€æŸ¥é»‘åå•
        if any(blacklisted in domain for blacklisted in LOW_QUALITY_DOMAINS):
            return False

        # æ£€æŸ¥ç™½åå•
        if any(whitelisted in domain for whitelisted in HIGH_QUALITY_DOMAINS):
            return True

        # é»˜è®¤å…è®¸
        return True

    except Exception:
        return True


def fetch_page_content(url: str, max_chars: int = 15000) -> Optional[str]:
    """
    è·å–é¡µé¢å†…å®¹ï¼ˆä½¿ç”¨ readability-lxml æå–ä¸»è¦å†…å®¹ï¼‰

    Args:
        url: ç›®æ ‡ URL
        max_chars: æœ€å¤§å­—ç¬¦æ•°

    Returns:
        é¡µé¢æ–‡æœ¬å†…å®¹
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        html_content = response.text

        # ä½¿ç”¨ readability-lxml æå–ä¸»è¦å†…å®¹
        try:
            from readability import Document
            doc = Document(html_content)
            text = doc.summary()

            # å¦‚æœ readability æå–å¤±è´¥ï¼Œå°è¯• trafilatura
            if not text or len(text) < 100:
                import trafilatura
                text = trafilatura.extract(html_content, include_comments=False, include_tables=False)

        except Exception as e:
            logger.debug(f"Readability/trafilatura å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•: {e}")
            # å›é€€åˆ°ç®€å•æ–¹æ³•
            import html
            text = re.sub(r'<script[^>]*>.*?</script>', ' ', html_content, flags=re.DOTALL)
            text = re.sub(r'<style[^>]*>.*?</style>', ' ', text, flags=re.DOTALL)
            text = re.sub(r'<[^>]+>', ' ', text)
            text = html.unescape(text)
            text = re.sub(r'\s+', ' ', text).strip()

        return text[:max_chars] if text else None

    except Exception as e:
        logger.warning(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
        return None


def extract_prompts_from_content(content: str, max_prompts: int = 15) -> List[str]:
    """
    ä»å†…å®¹ä¸­æå–æç¤ºè¯

    Args:
        content: é¡µé¢å†…å®¹
        max_prompts: æœ€å¤§æå–æ•°é‡

    Returns:
        æç¤ºè¯åˆ—è¡¨
    """
    prompts = []

    # æ¨¡å¼ 1: å¼•å·ä¸­çš„å†…å®¹ï¼ˆæ›´ä¸¥æ ¼çš„åŒ¹é…ï¼‰
    quote_patterns = [
        r'"([^"]{40,800})"',  # åŒå¼•å·ï¼Œè¾ƒé•¿
        r"'([^']{40,800})",   # å•å¼•å·
        r'`([^`]{40,800})`',  # åå¼•å·
    ]

    for pattern in quote_patterns:
        matches = re.findall(pattern, content)
        prompts.extend(matches)

    # æ¨¡å¼ 2: å†’å·åé¢çš„æè¿°æ€§æ–‡æœ¬
    colon_patterns = [
        r'(?:prompt|Prompt|PROMPT|example|Example)[\s:]+([^.!?]{40,800})',
        r'(?:prompt|Prompt|PROMPT)\s*[:=]\s*([^\n]{40,800})',
    ]

    for pattern in colon_patterns:
        matches = re.findall(pattern, content)
        prompts.extend(matches)

    # æ¨¡å¼ 3: ç¼–å·åˆ—è¡¨ï¼ˆé€‚åˆæ•™ç¨‹ç±»å†…å®¹ï¼‰
    list_patterns = [
        r'\d+\.\s+([^.!?]{40,800})',
        r'[-*]\s+([^.!?]{40,800})',
    ]

    for pattern in list_patterns:
        matches = re.findall(pattern, content)
        prompts.extend(matches)

    # å»é‡
    unique_prompts = list(dict.fromkeys(prompts))

    # è¿‡æ»¤è´¨é‡
    filtered = []
    for p in unique_prompts:
        p_clean = p.strip()

        # é•¿åº¦è¿‡æ»¤
        if not (40 <= len(p_clean) <= 800):
            continue

        # å†…å®¹è´¨é‡è¿‡æ»¤
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„å­—æ¯æ•°å­—
        alpha_ratio = sum(c.isalnum() or c.isspace() for c in p_clean) / len(p_clean)
        if alpha_ratio < 0.7:
            continue

        # æ£€æŸ¥æ˜¯å¦åŒ…å«åŠ¨ä½œåŠ¨è¯
        action_verbs = ['generate', 'write', 'create', 'design', 'build', 'make', 'act as', 'role', 'task']
        has_action = any(verb.lower() in p_clean.lower() for verb in action_verbs)
        if not has_action:
            continue

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æˆªæ–­æ ‡è®°
        truncation_markers = ['...', '# 1', 'Read more', 'continue reading', 'click to continue']
        if any(marker.lower() in p_clean.lower() for marker in truncation_markers):
            continue

        filtered.append(p_clean)

    return filtered[:max_prompts]


def classify_prompt_type(prompt: str) -> str:
    """
    åˆ†ç±»æç¤ºè¯ç±»å‹

    Args:
        prompt: æç¤ºè¯æ–‡æœ¬

    Returns:
        æç¤ºè¯ç±»å‹
    """
    prompt_lower = prompt.lower()

    image_keywords = [
        'image', 'photo', 'picture', 'portrait', 'painting', 'drawing',
        'illustration', 'midjourney', 'dall-e', 'stable diffusion',
        'render', 'visual', 'art', 'scene', 'landscape'
    ]

    video_keywords = [
        'video', 'animation', 'motion', 'animate', 'runway', 'pika',
        'kling', 'veo', 'clip', 'footage', 'film'
    ]

    text_keywords = [
        'write', 'essay', 'article', 'blog', 'content', 'story',
        'chatgpt', 'gpt', 'llm', 'text generation'
    ]

    image_score = sum(1 for kw in image_keywords if kw in prompt_lower)
    video_score = sum(1 for kw in video_keywords if kw in prompt_lower)
    text_score = sum(1 for kw in text_keywords if kw in prompt_lower)

    if video_score > image_score and video_score > text_score:
        return 'video-generation'
    elif image_score > text_score:
        return 'image-generation'
    elif text_score > 0:
        return 'text-generation'
    else:
        return 'general'


def calculate_quality_score(prompt: str) -> int:
    """
    è®¡ç®—æç¤ºè¯è´¨é‡åˆ†æ•°

    Args:
        prompt: æç¤ºè¯æ–‡æœ¬

    Returns:
        è´¨é‡åˆ†æ•° (0-100)
    """
    score = 0

    # é•¿åº¦è¯„åˆ†
    length = len(prompt)
    if 50 <= length <= 300:
        score += 30
    elif 301 <= length <= 500:
        score += 25
    elif 501 <= length <= 800:
        score += 15
    else:
        score += 5

    # å…³é”®è¯è¯„åˆ†
    quality_keywords = [
        'detailed', 'realistic', 'high quality', 'professional', 'creative',
        'specific', 'precise', 'clear', 'comprehensive', 'well-structured'
    ]
    score += min(20, sum(5 for kw in quality_keywords if kw in prompt.lower()))

    # åŠ¨ä½œåŠ¨è¯è¯„åˆ†
    action_verbs = ['generate', 'create', 'write', 'design', 'build', 'make', 'develop']
    score += min(15, sum(5 for verb in action_verbs if verb in prompt.lower()))

    # ç»“æ„è¯„åˆ†
    if ',' in prompt:
        score += 10
    if ':' in prompt:
        score += 5
    if '\n' in prompt:
        score += 10

    # æè¿°æ€§è¯æ±‡
    descriptive_words = [
        'with', 'featuring', 'including', 'showing', 'displaying', 'depicting'
    ]
    score += min(10, sum(2 for word in descriptive_words if word in prompt.lower()))

    return min(100, score)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("ğŸ” å¼€å§‹ä½¿ç”¨ SearXNG æœç´¢é«˜è´¨é‡ AI æç¤ºè¯")
    logger.info("=" * 80)

    # å­˜å‚¨æ‰€æœ‰æ”¶é›†çš„æç¤ºè¯
    all_prompts = []
    seen_urls = set()

    # éå†æœç´¢æŸ¥è¯¢
    for i, query in enumerate(SEARCH_QUERIES, 1):
        logger.info(f"\n[{i}/{len(SEARCH_QUERIES)}] æœç´¢: {query}")

        # æœç´¢
        results = search_searxng(query, limit=10)

        if not results:
            logger.warning(f"  æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
            continue

        # éå†ç»“æœ
        for j, result in enumerate(results, 1):
            url = result.get('url', '')
            title = result.get('title', '')
            snippet = result.get('content', '')

            # æ£€æŸ¥ URL è´¨é‡
            if not is_high_quality_url(url):
                logger.debug(f"  [{j}] è·³è¿‡ä½è´¨é‡æ¥æº: {url}")
                continue

            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if url in seen_urls:
                logger.debug(f"  [{j}] è·³è¿‡å·²å¤„ç† URL: {url}")
                continue

            seen_urls.add(url)

            logger.info(f"  [{j}] å¤„ç†: {title}")
            logger.debug(f"      URL: {url}")

            # è·å–é¡µé¢å†…å®¹
            page_content = fetch_page_content(url)

            if not page_content:
                logger.warning(f"      è·å–å†…å®¹å¤±è´¥")
                continue

            # ä¿å­˜é¡µé¢å†…å®¹æœ¬èº«ä½œä¸ºæç¤ºè¯
            # ä¼˜å…ˆä¿å­˜å®Œæ•´æ­£æ–‡ï¼Œå¦‚æœæ­£æ–‡å¤ªé•¿åˆ™æˆªå–
            content_to_save = page_content[:15000] if len(page_content) > 15000 else page_content
            prompt_type = classify_prompt_type(content_to_save)
            quality_score = calculate_quality_score(content_to_save)

            prompt_data = {
                'content': content_to_save,
                'title': title,
                'source': 'searxng',
                'url': url,
                'type': prompt_type,
                'quality_score': quality_score,
                'collected_at': datetime.now().isoformat(),
                'search_query': query
            }

            all_prompts.append(prompt_data)
            logger.info(f"      ä¿å­˜é¡µé¢å†…å®¹ ({len(content_to_save)} å­—ç¬¦)")

            # æå–æç¤ºè¯ï¼ˆé¢å¤–æå–ï¼Œä½œä¸ºè¡¥å……ï¼‰
            prompts = extract_prompts_from_content(page_content, max_prompts=15)

            if prompts:
                logger.info(f"      é¢å¤–æå– {len(prompts)} ä¸ªæç¤ºè¯")

                # å¤„ç†æ¯ä¸ªé¢å¤–æç¤ºè¯
                for prompt in prompts[:3]:  # åªä¿å­˜å‰ 3 ä¸ª
                    # é¿å…ä¸ä¸»å†…å®¹é‡å¤
                    if prompt not in page_content:
                        prompt_type = classify_prompt_type(prompt)
                        quality_score = calculate_quality_score(prompt)

                        prompt_data = {
                            'content': prompt,
                            'title': f"{title} (extracted)",
                            'source': 'searxng',
                            'url': url,
                            'type': prompt_type,
                            'quality_score': quality_score,
                            'collected_at': datetime.now().isoformat(),
                            'search_query': query
                        }

                        all_prompts.append(prompt_data)

        # æœç´¢é—´éš”ï¼Œé¿å…è¿‡äºé¢‘ç¹
        time.sleep(2)

    # ä¿å­˜ç»“æœ
    logger.info(f"\n{'=' * 80}")
    logger.info(f"ğŸ“Š æ”¶é›†å®Œæˆï¼")
    logger.info(f"{'=' * 80}")
    logger.info(f"æ€»å…±æ”¶é›†: {len(all_prompts)} ä¸ªæç¤ºè¯")

    # å†™å…¥æ–‡ä»¶
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for prompt in all_prompts:
            f.write(json.dumps(prompt, ensure_ascii=False) + '\n')

    logger.info(f"ä¿å­˜åˆ°: {OUTPUT_FILE}")

    # ç»Ÿè®¡ä¿¡æ¯
    type_counts = {}
    for prompt in all_prompts:
        ptype = prompt['type']
        type_counts[ptype] = type_counts.get(ptype, 0) + 1

    logger.info(f"\nç±»å‹åˆ†å¸ƒ:")
    for ptype, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  {ptype}: {count}")

    # è´¨é‡åˆ†å¸ƒ
    high_quality = sum(1 for p in all_prompts if p['quality_score'] >= 70)
    medium_quality = sum(1 for p in all_prompts if 50 <= p['quality_score'] < 70)
    low_quality = sum(1 for p in all_prompts if p['quality_score'] < 50)

    logger.info(f"\nè´¨é‡åˆ†å¸ƒ:")
    logger.info(f"  é«˜è´¨é‡ (â‰¥70): {high_quality}")
    logger.info(f"  ä¸­ç­‰ (50-69): {medium_quality}")
    logger.info(f"  ä½è´¨é‡ (<50): {low_quality}")

    logger.info(f"\nâœ… å®Œæˆï¼")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
