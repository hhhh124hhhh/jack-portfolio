#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆ AI æç¤ºè¯æ”¶é›†ç³»ç»Ÿ - Phase 1

åŠŸèƒ½ç‰¹æ€§ï¼š
- æ‰©å±•çš„æœç´¢å…³é”®è¯åº“ï¼ˆ50+ æŸ¥è¯¢ï¼‰
- æ™ºèƒ½å…³é”®è¯ç»„åˆç­–ç•¥
- é«˜çº§æœç´¢ç»“æœè¿‡æ»¤
- å¢å¼ºçš„æç¤ºè¯æå–ç®—æ³•
- æ”¯æŒä¸­è‹±æ–‡åŒè¯­æœç´¢
- è‡ªåŠ¨åˆ†ç±»å’Œè´¨é‡è¯„åˆ†
- å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

Phase 1 æ”¹è¿›ï¼š
1. å‡çº§æŸ¥è¯¢ç³»ç»Ÿ
2. æ‰©å±•å…³é”®è¯å’Œæœç´¢ç»„åˆ
3. æ”¹è¿› URL è´¨é‡åˆ¤æ–­
4. ä¼˜åŒ–æç¤ºè¯æå–æ¨¡å¼
"""

import json
import os
import re
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
import logging
import requests
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal

# ============================================================================
# é…ç½®åŒºåŸŸ
# ============================================================================

# ç›®å½•é…ç½®
DATA_DIR = Path("/root/clawd/data/prompts")
OUTPUT_DIR = DATA_DIR / "collected"
OUTPUT_FILE = OUTPUT_DIR / f"prompts-enhanced-{datetime.now().strftime('%Y%m%d-%H%M')}.jsonl"
LOGS_DIR = Path("/root/clawd/logs")

# åˆ›å»ºç›®å½•
DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)

# æ—¥å¿—é…ç½®
logger = logging.getLogger("collect_prompts_enhanced")
logger.setLevel(logging.INFO)

# æ§åˆ¶å°æ—¥å¿—ï¼ˆç®€åŒ–ç‰ˆï¼‰
console_handler = logging.StreamHandler()
console_handler.setFormatter(logging.Formatter('%(message)s'))
logger.addHandler(console_handler)

# æ–‡ä»¶æ—¥å¿—ï¼ˆè¯¦ç»†ç‰ˆï¼‰
file_handler = logging.FileHandler(
    LOGS_DIR / "collect-prompts-enhanced.log",
    encoding='utf-8'
)
file_handler.setFormatter(
    logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
)
logger.addHandler(file_handler)

# SearXNG é…ç½®
SEARXNG_URL = os.getenv("SEARXNG_URL", "http://localhost:8080")
SEARCH_TIMEOUT = 30
MAX_RESULTS_PER_QUERY = 10
MAX_CONTENT_LENGTH = 20000

# å¹¶å‘é…ç½®
MAX_WORKERS = 3
REQUEST_DELAY = 1.5  # è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰

# ============================================================================
# æ‰©å±•çš„æœç´¢å…³é”®è¯åº“ - Phase 1 æ ¸å¿ƒæ”¹è¿›
# ============================================================================

# åŸºç¡€å…³é”®è¯ï¼ˆç”¨äºç»„åˆï¼‰
BASE_KEYWORDS = {
    "prompt": ["prompt", "æç¤ºè¯", "æç¤º", "å‘½ä»¤", "instruction", "template"],
    "ai": ["AI", "artificial intelligence", "äººå·¥æ™ºèƒ½", "LLM", "GPT", "Claude", "Gemini"],
    "type": ["image", "video", "text", "ä»£ç ", "code", "è‰ºæœ¯", "art", "creative"],
    "platform": ["Midjourney", "DALL-E", "Stable Diffusion", "Veo", "Kling", "Runway", "Pika"],
    "action": ["generate", "create", "write", "design", "ç”Ÿæˆ", "åˆ›ä½œ", "è®¾è®¡"],
    "quality": ["best", "top", "high quality", "professional", "é«˜è´¨é‡", "ä¸“ä¸š", "æœ€ä½³"],
}

# ä¸“ä¸šæç¤ºè¯ç½‘ç«™å’Œèµ„æº
PROFESSIONAL_SOURCES = [
    # ç»¼åˆæ€§èµ„æº
    "awesome-chatgpt-prompts github repository",
    "LearnPrompting guide comprehensive tutorial",
    "PromptBase marketplace best prompts",
    "OpenAI prompt engineering guide",

    # å›¾åƒç”Ÿæˆä¸“é¡¹
    "Midjourney prompt tutorial examples",
    "Midjourney å‚æ•°è¯¦è§£ --ar --style --chaos",
    "DALL-E 3 prompt examples guide",
    "Stable Diffusion prompt engineering negative",
    "Stable Diffusion LoRA model prompts",

    # è§†é¢‘ç”Ÿæˆä¸“é¡¹ï¼ˆæ–°å¢ï¼‰
    "Veo 3 prompt examples video generation",
    "Kling AI prompt guide text to video",
    "Runway ML prompt tutorial motion",
    "Pika Labs prompt examples animation",
    "video generation prompt best practices",

    # æŠ€æœ¯å¹³å°æœç´¢
    "site:github.com \"prompt engineering\" tutorial",
    "site:github.com \"AI prompts\" repository",
    "site:medium.com \"prompt engineering\" guide",
    "site:dev.to \"AI prompts\" examples",
    "site:hashnode.com \"prompt guide\" tutorial",
    "site:towardsdatascience.com \"prompt\" techniques",

    # è¡Œä¸šåº”ç”¨ï¼ˆæ–°å¢ï¼‰
    "AI prompts for marketing content",
    "business AI prompt templates",
    "educational AI prompts teaching",
    "product photography AI prompts",
    "character design AI prompts",
    "game asset AI prompts",
    "fashion design AI prompts",
    "architecture AI prompts",

    # é«˜çº§æŠ€å·§ï¼ˆæ–°å¢ï¼‰
    "negative prompt examples Midjourney",
    "prompt chaining techniques LLM",
    "few-shot prompting examples",
    "role-based prompts system instructions",
    "context-aware prompts examples",

    # ç‰¹å®šé¢†åŸŸï¼ˆæ–°å¢ï¼‰
    "legal AI prompts contract",
    "medical AI prompts diagnosis",
    "finance AI prompts analysis",
    "scientific AI prompts research",
    "creative writing AI prompts storytelling",
]

# ä¸­æ–‡æœç´¢æŸ¥è¯¢ï¼ˆæ–°å¢ï¼‰
CHINESE_QUERIES = [
    "Midjourney æç¤ºè¯ æ•™ç¨‹ ç¤ºä¾‹",
    "DALL-E 3 æç¤ºè¯ æŒ‡å—",
    "Stable Diffusion æç¤ºè¯ è´Ÿé¢",
    "AI æç¤ºè¯ å·¥ç¨‹ æœ€ä½³å®è·µ",
    "ChatGPT æç¤ºè¯ æ¨¡æ¿",
    "Claude æç¤ºè¯ è§’è‰²æ‰®æ¼”",
    "AI ç»˜ç”» æç¤ºè¯ é£æ ¼",
    "è§†é¢‘ç”Ÿæˆ æç¤ºè¯ Veo Kling",
    "AI å†™ä½œ æç¤ºè¯ æ–‡æ¡ˆ",
    "å•†ä¸š AI æç¤ºè¯ æ¨¡æ¿",

    "site:github.com \"æç¤ºè¯\" AI",
    "site:zhihu.com \"æç¤ºè¯\" AI",
    "site:csdn.net AI æç¤ºè¯ æ•™ç¨‹",
]

# ç»„åˆæœç´¢ç­–ç•¥ï¼ˆæ–°å¢ï¼‰
# åŸºç¡€å…³é”®è¯ + å¹³å° + è´¨é‡è¯
COMBINATION_QUERIES = []

# è‡ªåŠ¨ç”Ÿæˆç»„åˆæŸ¥è¯¢
platforms = ["Midjourney", "DALL-E", "Stable Diffusion", "Veo", "Kling", "Runway", "Pika"]
actions = ["best prompts", "tutorial", "guide", "examples", "templates"]
qualities = ["professional", "high quality", "advanced"]

for platform in platforms[:4]:  # é™åˆ¶ç»„åˆæ•°é‡
    for action in actions[:2]:
        for quality in qualities[:2]:
            query = f"{platform} {action} {quality}"
            COMBINATION_QUERIES.append(query)

# åˆå¹¶æ‰€æœ‰æœç´¢æŸ¥è¯¢
ALL_QUERIES = list(set(PROFESSIONAL_SOURCES + CHINESE_QUERIES + COMBINATION_QUERIES))

# ============================================================================
# åŸŸåè´¨é‡æ§åˆ¶ - Phase 1 æ”¹è¿›
# ============================================================================

# é«˜è´¨é‡åŸŸåç™½åå•ï¼ˆæ‰©å±•ï¼‰
HIGH_QUALITY_DOMAINS = {
    # ç»¼åˆæ€§èµ„æº
    'github.com', 'gitlab.com', 'bitbucket.org',

    # AI å’ŒæŠ€æœ¯å¹³å°
    'openai.com', 'anthropic.com', 'google.com', 'deepmind.com',
    'midjourney.com', 'stability.ai', 'huggingface.co',

    # æç¤ºè¯ä¸“ä¸šç½‘ç«™
    'promptbase.com', 'learnprompting.org', 'promptengineering.ai',

    # æŠ€æœ¯åšå®¢å’Œç¤¾åŒº
    'medium.com', 'dev.to', 'hashnode.com', 'towardsdatascience.com',
    'analyticsvidhya.com', 'kdnuggets.com', 'machinelearningmastery.com',

    # ä¸­æ–‡æŠ€æœ¯ç¤¾åŒºï¼ˆæ–°å¢ï¼‰
    'zhihu.com', 'csdn.net', 'juejin.cn', 'segmentfault.com',

    # æ•™è‚²èµ„æº
    'coursera.org', 'udacity.com', 'edx.org',

    # æ–‡æ¡£å’ŒæŒ‡å—
    'readthedocs.io', 'docs.python.org',
}

# ä½è´¨é‡åŸŸåé»‘åå•ï¼ˆæ‰©å±•ï¼‰
LOW_QUALITY_DOMAINS = {
    # ç¤¾äº¤åª’ä½“ï¼ˆé€šå¸¸å†…å®¹è´¨é‡ä¸ç¨³å®šï¼‰
    'pinterest.com', 'instagram.com', 'tiktok.com', 'facebook.com',
    'twitter.com', 'x.com',  # ä½¿ç”¨ twitter-search skill

    # æ–°é—»èšåˆå’Œä½è´¨é‡å†…å®¹
    'buzzfeed.com', 'clickhole.com', 'clickbait',

    # å¹¿å‘Šå’Œæ¨å¹¿ç½‘ç«™
    'ad', 'ads', 'promotion', 'affiliate',

    # ä¸‹è½½å’Œç ´è§£ç½‘ç«™
    'crack', 'torrent', 'pirate', 'warez',
}

# ============================================================================
# å…¨å±€çŠ¶æ€
# ============================================================================

# ç”¨äºä¼˜é›…é€€å‡º
shutdown_flag = False

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†"""
    global shutdown_flag
    logger.info("\nâ¸ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
    shutdown_flag = True

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# ============================================================================
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ============================================================================

def is_high_quality_url(url: str) -> Tuple[bool, str]:
    """
    åˆ¤æ–­ URL æ˜¯å¦æ¥è‡ªé«˜è´¨é‡æ¥æº - Phase 1 æ”¹è¿›ç‰ˆ

    Args:
        url: ç›®æ ‡ URL

    Returns:
        (æ˜¯å¦é«˜è´¨é‡, åŸå› )
    """
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        # 1. æ£€æŸ¥é»‘åå•ï¼ˆä¸¥æ ¼ï¼‰
        for blacklisted in LOW_QUALITY_DOMAINS:
            if blacklisted in domain:
                return False, f"é»‘åå•åŸŸå: {blacklisted}"

        # 2. æ£€æŸ¥ç™½åå•ï¼ˆä¿¡ä»»ï¼‰
        for whitelisted in HIGH_QUALITY_DOMAINS:
            if whitelisted in domain:
                return True, f"ç™½åå•åŸŸå: {whitelisted}"

        # 3. æ£€æŸ¥ URL æ¨¡å¼ï¼ˆå¯å‘å¼è§„åˆ™ï¼‰
        # æœ‰ç”¨çš„æ¨¡å¼
        good_patterns = [
            r'/blog/',  # åšå®¢æ–‡ç« 
            r'/tutorial',  # æ•™ç¨‹
            r'/guide',  # æŒ‡å—
            r'/docs/',  # æ–‡æ¡£
            r'/learn/',  # å­¦ä¹ èµ„æº
            r'/wiki/',  # Wiki
            r'github\.com/[^/]+/[^/]+',  # GitHub ä»“åº“
            r'readthedocs\.io',  # ReadTheDocs
        ]

        for pattern in good_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return True, f"åŒ¹é…è‰¯å¥½æ¨¡å¼: {pattern}"

        # 4. æ£€æŸ¥å¯ç–‘æ¨¡å¼
        bad_patterns = [
            r'/ad',  # å¹¿å‘Š
            r'/ads',  # å¹¿å‘Š
            r'/affiliate',  # è”ç›Ÿè¥é”€
            r'/ref=',  # æ¨èé“¾æ¥
            r'\.exe$',  # å¯æ‰§è¡Œæ–‡ä»¶
            r'\.apk$',  # Android åº”ç”¨
        ]

        for pattern in bad_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False, f"åŒ¹é…å¯ç–‘æ¨¡å¼: {pattern}"

        # 5. é»˜è®¤å…è®¸ï¼Œä½†æ ‡è®°ä¸ºæœªéªŒè¯
        return True, "é»˜è®¤å…è®¸ï¼ˆæœªéªŒè¯ï¼‰"

    except Exception as e:
        logger.warning(f"URL è§£æå¤±è´¥ {url}: {e}")
        return True, "è§£æå¤±è´¥ï¼Œé»˜è®¤å…è®¸"


def search_searxng(query: str, limit: int = MAX_RESULTS_PER_QUERY) -> List[Dict]:
    """
    ä½¿ç”¨ SearXNG æœç´¢ - Phase 1 æ”¹è¿›ç‰ˆ

    Args:
        query: æœç´¢æŸ¥è¯¢
        limit: è¿”å›ç»“æœæ•°é‡

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    if shutdown_flag:
        return []

    params = {
        "q": query,
        "format": "json",
        "categories": "general",
        "engines": "",  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨å¼•æ“
    }

    try:
        response = requests.get(
            f"{SEARXNG_URL}/search",
            params=params,
            timeout=SEARCH_TIMEOUT,
            verify=False
        )
        response.raise_for_status()

        data = response.json()
        results = data.get("results", [])[:limit]

        logger.debug(f"æœç´¢ '{query}': {len(results)} ä¸ªç»“æœ")

        return results

    except requests.exceptions.Timeout:
        logger.warning(f"æœç´¢è¶…æ—¶: {query}")
        return []
    except requests.exceptions.RequestException as e:
        logger.warning(f"æœç´¢å¤±è´¥ '{query}': {e}")
        return []
    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯ '{query}': {e}")
        return []


def fetch_page_content(url: str, max_chars: int = MAX_CONTENT_LENGTH) -> Optional[str]:
    """
    è·å–é¡µé¢å†…å®¹ - Phase 1 æ”¹è¿›ç‰ˆ

    Args:
        url: ç›®æ ‡ URL
        max_chars: æœ€å¤§å­—ç¬¦æ•°

    Returns:
        é¡µé¢æ–‡æœ¬å†…å®¹
    """
    if shutdown_flag:
        return None

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }

        response = requests.get(url, headers=headers, timeout=SEARCH_TIMEOUT)
        response.raise_for_status()

        # æ£€æŸ¥å†…å®¹ç±»å‹
        content_type = response.headers.get('content-type', '').lower()
        if 'text/html' not in content_type:
            logger.debug(f"è·³è¿‡é HTML å†…å®¹: {content_type}")
            return None

        text = response.text

        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        text = re.sub(r'<script[^>]*>.*?</script>', ' ', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<style[^>]*>.*?</style>', ' ', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<noscript[^>]*>.*?</noscript>', ' ', text, flags=re.DOTALL | re.IGNORECASE)

        # ç§»é™¤ HTML æ ‡ç­¾
        import html
        text = re.sub(r'<[^>]+>', ' ', text)
        text = html.unescape(text)

        # æ¸…ç†ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()

        # é™åˆ¶é•¿åº¦
        return text[:max_chars]

    except requests.exceptions.Timeout:
        logger.debug(f"è·å–é¡µé¢è¶…æ—¶: {url}")
        return None
    except requests.exceptions.RequestException as e:
        logger.debug(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
        return None
    except Exception as e:
        logger.warning(f"æœªçŸ¥é”™è¯¯ {url}: {e}")
        return None


def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡/æ··åˆï¼‰

    Args:
        text: è¾“å…¥æ–‡æœ¬

    Returns:
        'zh', 'en', 'mixed'
    """
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ•°
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    # è®¡ç®—è‹±æ–‡å­—ç¬¦æ•°
    english_chars = sum(1 for c in text if c.isalpha() and ord(c) < 128)
    # è®¡ç®—æ€»å­—ç¬¦æ•°
    total_chars = chinese_chars + english_chars

    if total_chars == 0:
        return 'unknown'

    # è®¡ç®—æ¯”ä¾‹
    chinese_ratio = chinese_chars / total_chars
    english_ratio = english_chars / total_chars

    # åˆ¤æ–­
    if chinese_ratio > 0.7:
        return 'zh'
    elif english_ratio > 0.7:
        return 'en'
    else:
        return 'mixed'


def is_navigation_or_footer(text: str) -> bool:
    """
    æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æ¥è‡ªå¯¼èˆªæ æˆ–é¡µè„š

    Args:
        text: æ–‡æœ¬å†…å®¹

    Returns:
        True å¦‚æœæ˜¯å¯¼èˆªæ /é¡µè„šå†…å®¹
    """
    text_lower = text.lower()

    # å¯¼èˆªå…³é”®è¯
    nav_keywords = [
        'menu', 'navigation', 'home', 'about', 'contact',
        'login', 'sign in', 'register', 'sign up',
        'pricing', 'pricing plans', 'subscribe', 'subscription',
        'search', 'search...', 'search bar',
        'å¯¼èˆª', 'èœå•', 'é¦–é¡µ', 'å…³äº', 'è”ç³»',
        'ç™»å½•', 'æ³¨å†Œ', 'å®šä»·', 'è®¢é˜…', 'æœç´¢',
    ]

    # é¡µè„šå…³é”®è¯
    footer_keywords = [
        'copyright', 'all rights reserved', 'privacy policy',
        'terms of service', 'cookie policy', 'contact us',
        'follow us', 'social media', 'newsletter',
        'ç‰ˆæƒæ‰€æœ‰', 'éšç§æ”¿ç­–', 'æœåŠ¡æ¡æ¬¾', 'cookieæ”¿ç­–',
        'è”ç³»æˆ‘ä»¬', 'å…³æ³¨æˆ‘ä»¬', 'ç¤¾äº¤åª’ä½“',
    ]

    # å¹¿å‘Šå…³é”®è¯
    ad_keywords = [
        'ad', 'advertisement', 'sponsored', 'affiliate',
        'promo', 'promotion', 'discount', 'sale', 'offer',
        'limited time', 'only $', 'free trial', 'click here',
        'å¹¿å‘Š', 'æ¨å¹¿', 'ä¼˜æƒ ', 'æŠ˜æ‰£', 'ä¿ƒé”€',
    ]

    # æ£€æŸ¥
    for keyword in nav_keywords + footer_keywords + ad_keywords:
        if keyword in text_lower:
            return True

    # æ£€æŸ¥çº¯ URL æˆ–çŸ­æ–‡æœ¬
    if len(text.split()) < 3:
        return True

    return False


def is_truncated(text: str) -> Tuple[bool, str]:
    """
    æ£€æŸ¥æ–‡æœ¬æ˜¯å¦è¢«æˆªæ–­

    Args:
        text: æ–‡æœ¬å†…å®¹

    Returns:
        (æ˜¯å¦æˆªæ–­, æˆªæ–­åŸå› )
    """
    text_lower = text.lower()

    # æˆªæ–­æ ‡è®°
    truncation_markers = [
        '...', 'â€¦', '...', '...',  # çœç•¥å·
        'read more', 'continue reading', 'click to continue',
        'view more', 'see more', 'show more', 'learn more',
        'ç»§ç»­é˜…è¯»', 'ç‚¹å‡»ç»§ç»­', 'æŸ¥çœ‹æ›´å¤š', 'äº†è§£æ›´å¤š',
        '[...]', '(...)', '{...}', '<...>',
    ]

    for marker in truncation_markers:
        if marker in text_lower:
            return True, f"æˆªæ–­æ ‡è®°: {marker}"

    # æ£€æŸ¥ç»“å°¾ - ä¸å®Œæ•´çš„å¥å­
    if text[-3:] in ['...', '...', '...', 'â€¦']:
        return True, "ä»¥çœç•¥å·ç»“å°¾"

    # æ£€æŸ¥å¼€å¤´ - ç¼ºå°‘ä¸»è¯­æˆ–åŠ¨è¯
    if text[0].islower() and not text.startswith(('a ', 'an ', 'the ')):
        return True, "ä»¥å°å†™å­—æ¯å¼€å¤´ï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰"

    # æ£€æŸ¥æ ‡ç‚¹ - æ­£å¸¸æ–‡æœ¬åº”è¯¥æœ‰æ ‡ç‚¹ç¬¦å·ç»“å°¾
    if not text[-1] in ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '"', "'", '"', "'", '`']:
        # ä½†å¦‚æœå¾ˆçŸ­ï¼ˆ< 200 å­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯å…³é”®è¯åˆ—è¡¨
        if len(text) > 200:
            return True, "ç¼ºå°‘ç»“å°¾æ ‡ç‚¹"

    return False, ""


def extract_prompts_from_content(content: str, max_prompts: int = 20) -> List[str]:
    """
    ä»å†…å®¹ä¸­æå–æç¤ºè¯ - Phase 2 æ”¹è¿›ç‰ˆ

    æ”¹è¿›ç‚¹ï¼š
    - æ›´ç²¾ç¡®çš„ä¸Šä¸‹æ–‡æ ‡è®°
    - å¯¼èˆªæ /é¡µè„šè¿‡æ»¤
    - å®Œæ•´æ€§æ£€æŸ¥
    - è¯­è¨€æ„ŸçŸ¥

    Args:
        content: é¡µé¢å†…å®¹
        max_prompts: æœ€å¤§æå–æ•°é‡

    Returns:
        æç¤ºè¯åˆ—è¡¨
    """
    prompts = []

    # æ¨¡å¼ 1: ä»£ç å—ä¸­çš„æç¤ºè¯ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    code_block_patterns = [
        r'```(?:prompt|text|æç¤ºè¯)?\s*\n+([\s\S]{50,1000}?)\n+```',
    ]

    for pattern in code_block_patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        prompts.extend(matches)

    # æ¨¡å¼ 2: æ˜ç¡®æ ‡è®°åçš„å†…å®¹
    explicit_patterns = [
        r'(?:prompt[:ï¼š]\s*)["\']([^"\']{50,1000})["\']',  # Prompt: "text"
        r'(?:æç¤ºè¯[:ï¼š]\s*)["\']([^"\']{50,1000})["\']',  # æç¤ºè¯: "text"
        r'(?:example[:ï¼š]\s*)["\']([^"\']{50,1000})["\']',  # Example: "text"
        r'(?:ä¾‹å­[:ï¼š]\s*)["\']([^"\']{50,1000})["\']',  # ä¾‹å­: "text"
    ]

    for pattern in explicit_patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        prompts.extend(matches)

    # æ¨¡å¼ 3: è§’è‰²æ‰®æ¼”æ¨¡å¼
    roleplay_patterns = [
        r'(?:act as|act as a|æ‰®æ¼”|è§’è‰²[:ï¼š]\s*)([^.\n]{50,1000})',
        r'(?:you are|you are a|ä½ æ˜¯)([\s]+[^.\n]{50,1000})',
    ]

    for pattern in roleplay_patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        prompts.extend(matches)

    # æ¨¡å¼ 4: åŠ¨ä½œæŒ‡ä»¤æ¨¡å¼
    action_patterns = [
        r'(?:generate|create|design|write|ç”Ÿæˆ|åˆ›ä½œ|è®¾è®¡)[\s,]+([^.\n]{50,1000})',
        r'(?:create|make|generate)([\s]+a[\s]+[^.\n]{50,1000})',
    ]

    for pattern in action_patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        prompts.extend(matches)

    # æ¨¡å¼ 5: åˆ—è¡¨ä¸­çš„æç¤ºè¯ï¼ˆæœ€åï¼Œå› ä¸ºå¯èƒ½åŒ…å«å™ªéŸ³ï¼‰
    list_patterns = [
        r'(?i)(?:^\d+[\.\)]|[-*â€¢])\s+["\']([^"\']{50,1000})["\']',  # å¸¦å¼•å·çš„åˆ—è¡¨é¡¹
        r'(?i)(?:^\d+[\.\)]|[-*â€¢])\s+([^.\n]{80,500})',  # è¾ƒé•¿çš„åˆ—è¡¨é¡¹
    ]

    for pattern in list_patterns:
        matches = re.findall(pattern, content, re.MULTILINE)
        prompts.extend(matches)

    # å»é‡ï¼ˆä¿ç•™é¡ºåºï¼‰
    seen = set()
    unique_prompts = []
    for p in prompts:
        p_clean = p.strip()
        if p_clean and p_clean not in seen:
            seen.add(p_clean)
            unique_prompts.append(p_clean)

    # è´¨é‡è¿‡æ»¤ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    filtered = []
    for p in unique_prompts:
        p_clean = p.strip()

        # 1. é•¿åº¦è¿‡æ»¤ï¼ˆè°ƒæ•´èŒƒå›´ï¼‰
        if not (50 <= len(p_clean) <= 1000):
            continue

        # 2. å¯¼èˆªæ /é¡µè„šæ£€æŸ¥ï¼ˆæ–°å¢ï¼‰
        if is_navigation_or_footer(p_clean):
            continue

        # 3. å®Œæ•´æ€§æ£€æŸ¥ï¼ˆæ–°å¢ï¼‰
        is_trunc, trunc_reason = is_truncated(p_clean)
        if is_trunc and len(p_clean) < 300:
            # çŸ­æ–‡æœ¬ä¸”è¢«æˆªæ–­ï¼Œè·³è¿‡
            continue

        # 4. å†…å®¹è´¨é‡è¿‡æ»¤
        # æ£€æŸ¥å­—æ¯æ•°å­—/ä¸­æ–‡æ¯”ä¾‹
        alpha_ratio = sum(
            c.isalnum() or c.isspace() or ord(c) > 127  # æ”¯æŒä¸­æ–‡
            for c in p_clean
        ) / len(p_clean)
        if alpha_ratio < 0.6:
            continue

        # 5. æ£€æŸ¥æ˜¯å¦æœ‰æ„ä¹‰çš„å†…å®¹ï¼ˆå…³é”®è¯æ£€æŸ¥ï¼‰
        meaningful_keywords = [
            # è‹±æ–‡å…³é”®è¯
            'image', 'photo', 'picture', 'portrait', 'art', 'design', 'create',
            'generate', 'write', 'style', 'quality', 'detailed', 'realistic',
            'video', 'animation', 'text', 'story', 'code', 'function',
            # ä¸­æ–‡å…³é”®è¯
            'å›¾åƒ', 'ç…§ç‰‡', 'è‰ºæœ¯', 'è®¾è®¡', 'åˆ›å»º', 'ç”Ÿæˆ', 'å†™ä½œ',
            'é£æ ¼', 'è´¨é‡', 'è¯¦ç»†', 'é€¼çœŸ', 'è§†é¢‘', 'åŠ¨ç”»', 'æ–‡æœ¬',
            'æ•…äº‹', 'ä»£ç ', 'å‡½æ•°',
        ]
        has_meaningful = any(
            kw in p_clean.lower()
            for kw in meaningful_keywords
        )
        if not has_meaningful:
            continue

        # 6. æ£€æŸ¥åƒåœ¾å†…å®¹
        junk_patterns = [
            r'^\s*https?://',  # çº¯ URL
            r'^\s*[a-z]{10,}\s*$',  # çº¯éšæœºå­—ç¬¦
            r'^\s*\d+\s*$',  # çº¯æ•°å­—
            r'^\s*[^\w\s]{10,}\s*$',  # çº¯ç‰¹æ®Šå­—ç¬¦
        ]
        for pattern in junk_patterns:
            if re.match(pattern, p_clean, re.IGNORECASE):
                break
        else:
            # æ·»åŠ å®Œæ•´æ€§ä¿¡æ¯åˆ°å…ƒæ•°æ®
            filtered.append({
                'content': p_clean,
                'is_truncated': is_trunc,
                'truncation_reason': trunc_reason,
            })

    return filtered[:max_prompts]


def classify_prompt_type(prompt: str, language: str = 'unknown') -> str:
    """
    åˆ†ç±»æç¤ºè¯ç±»å‹ - Phase 2 æ”¹è¿›ç‰ˆï¼ˆè¯­è¨€æ„ŸçŸ¥ï¼‰

    Args:
        prompt: æç¤ºè¯æ–‡æœ¬
        language: è¯­è¨€ ('zh', 'en', 'mixed', 'unknown')

    Returns:
        æç¤ºè¯ç±»å‹
    """
    prompt_lower = prompt.lower()

    # å›¾åƒç”Ÿæˆå…³é”®è¯ï¼ˆæ‰©å±•ï¼‰
    image_keywords = {
        'en': [
            'image', 'photo', 'picture', 'portrait', 'painting', 'drawing',
            'illustration', 'midjourney', 'dall-e', 'stable diffusion', 'diffusion',
            'render', 'visual', 'art', 'scene', 'landscape', 'portrait',
            'sketch', 'watercolor', 'oil painting', 'digital art',
        ],
        'zh': [
            'å›¾ç‰‡', 'å›¾åƒ', 'ç»˜ç”»', 'æ’ç”»', 'ç…§ç‰‡', 'æ¸²æŸ“',
        ]
    }

    # è§†é¢‘ç”Ÿæˆå…³é”®è¯ï¼ˆæ‰©å±•ï¼‰
    video_keywords = {
        'en': [
            'video', 'animation', 'motion', 'animate', 'runway', 'pika',
            'kling', 'veo', 'clip', 'footage', 'film', 'movie',
            'transition', 'camera movement', 'zoom', 'pan',
        ],
        'zh': [
            'è§†é¢‘', 'åŠ¨ç”»', 'å½±ç‰‡', 'è½¬åœº',
        ]
    }

    # æ–‡æœ¬ç”Ÿæˆå…³é”®è¯ï¼ˆæ‰©å±•ï¼‰
    text_keywords = {
        'en': [
            'write', 'essay', 'article', 'blog', 'content', 'story',
            'chatgpt', 'gpt', 'llm', 'text generation', 'summarize',
            'translate', 'analyze', 'explain', 'code', 'programming',
        ],
        'zh': [
            'å†™ä½œ', 'æ–‡ç« ', 'åšå®¢', 'æ•…äº‹', 'ä»£ç ', 'ç¼–ç¨‹',
        ]
    }

    # ä»£ç ç”Ÿæˆå…³é”®è¯ï¼ˆæ–°å¢ï¼‰
    code_keywords = {
        'en': [
            'code', 'function', 'class', 'algorithm', 'debug', 'refactor',
        ],
        'zh': [
            'ä»£ç ', 'å‡½æ•°', 'ç±»', 'ç®—æ³•', 'è°ƒè¯•', 'é‡æ„',
        ]
    }

    # è§’è‰²æ‰®æ¼”å…³é”®è¯ï¼ˆæ–°å¢ï¼‰
    roleplay_keywords = {
        'en': [
            'act as', 'you are', 'role', 'character', 'persona',
        ],
        'zh': [
            'æ‰®æ¼”', 'è§’è‰²', 'ä½ æ˜¯',
        ]
    }

    # è®¡ç®—åˆ†æ•°ï¼ˆè¯­è¨€æ„ŸçŸ¥ï¼‰
    def calculate_score(keywords_dict: Dict, lang: str) -> int:
        """æ ¹æ®è¯­è¨€è®¡ç®—å…³é”®è¯å¾—åˆ†"""
        score = 0
        # ä¼˜å…ˆä½¿ç”¨åŒ¹é…çš„è¯­è¨€å…³é”®è¯
        if lang in keywords_dict:
            score += sum(1 for kw in keywords_dict[lang] if kw in prompt_lower)
        # ä¹Ÿæ£€æŸ¥å…¶ä»–è¯­è¨€çš„å…³é”®è¯ï¼ˆæ··åˆè¯­è¨€çš„æƒ…å†µï¼‰
        for other_lang, kw_list in keywords_dict.items():
            if other_lang != lang:
                score += sum(1 for kw in kw_list if kw in prompt_lower) // 2  # é™ä½æƒé‡
        return score

    image_score = calculate_score(image_keywords, language)
    video_score = calculate_score(video_keywords, language)
    text_score = calculate_score(text_keywords, language)
    code_score = calculate_score(code_keywords, language)
    roleplay_score = calculate_score(roleplay_keywords, language)

    # åˆ¤æ–­ç±»å‹
    if code_score >= 2 and code_score > text_score:
        return 'code-generation'
    elif roleplay_score >= 2:
        return 'roleplay'
    elif video_score > image_score and video_score > text_score:
        return 'video-generation'
    elif image_score > text_score:
        return 'image-generation'
    elif text_score > 0:
        return 'text-generation'
    else:
        return 'general'


def calculate_quality_score(prompt: str, is_truncated: bool = False) -> int:
    """
    è®¡ç®—æç¤ºè¯è´¨é‡åˆ†æ•° - Phase 2 æ”¹è¿›ç‰ˆï¼ˆè€ƒè™‘æˆªæ–­ï¼‰

    Args:
        prompt: æç¤ºè¯æ–‡æœ¬
        is_truncated: æ˜¯å¦è¢«æˆªæ–­

    Returns:
        è´¨é‡åˆ†æ•° (0-100)
    """
    score = 0
    prompt_lower = prompt.lower()

    # 1. é•¿åº¦è¯„åˆ†ï¼ˆæ”¹è¿› - ç»™æ›´é•¿æç¤ºè¯æ›´é«˜åˆ†ï¼‰
    length = len(prompt)
    if 80 <= length <= 200:
        score += 20
    elif 201 <= length <= 400:
        score += 30
    elif 401 <= length <= 600:
        score += 35
    elif 601 <= length <= 800:
        score += 30
    elif 801 <= length <= 1000:
        score += 25
    else:
        score += 15

    # 2. å…³é”®è¯è¯„åˆ†ï¼ˆæ‰©å±•ï¼‰
    quality_keywords = [
        'detailed', 'realistic', 'high quality', 'professional', 'creative',
        'specific', 'precise', 'clear', 'comprehensive', 'well-structured',
        'vibrant', 'stunning', 'beautiful', 'elegant', 'sophisticated',
        'è¯¦ç»†', 'é€¼çœŸ', 'é«˜è´¨é‡', 'ä¸“ä¸š', 'åˆ›æ„', 'å…·ä½“', 'ç²¾ç¡®', 'æ¸…æ™°',
        'ç”ŸåŠ¨', 'æƒŠè‰³', 'ç¾ä¸½', 'ä¼˜é›…', 'ç²¾è‡´',
    ]
    score += min(25, sum(3 for kw in quality_keywords if kw in prompt_lower))

    # 3. åŠ¨ä½œåŠ¨è¯è¯„åˆ†ï¼ˆæ‰©å±•ï¼‰
    action_verbs = [
        'generate', 'create', 'write', 'design', 'build', 'make', 'develop',
        'analyze', 'explain', 'summarize', 'translate', 'optimize',
        'render', 'depict', 'illustrate', 'portray', 'capture',
        'ç”Ÿæˆ', 'åˆ›ä½œ', 'ç¼–å†™', 'è®¾è®¡', 'æ„å»º', 'åˆ¶ä½œ', 'å¼€å‘',
        'åˆ†æ', 'è§£é‡Š', 'æ€»ç»“', 'ç¿»è¯‘', 'ä¼˜åŒ–', 'æ¸²æŸ“', 'æç»˜',
    ]
    score += min(20, sum(3 for verb in action_verbs if verb in prompt_lower))

    # 4. ç»“æ„è¯„åˆ†ï¼ˆæ”¹è¿›ï¼‰
    if ',' in prompt:
        score += 6
    if ':' in prompt:
        score += 4
    if '\n' in prompt or 'ï¼Œ' in prompt:
        score += 6
    if '--' in prompt:  # Midjourney å‚æ•°
        score += 5

    # 5. æè¿°æ€§è¯æ±‡ï¼ˆæ”¹è¿›ï¼‰
    descriptive_words = [
        'with', 'featuring', 'including', 'showing', 'displaying', 'depicting',
        'containing', 'using', 'inspired by', 'style of', 'in the style',
        'åŒ…å«', 'å±•ç¤º', 'æç»˜', 'ä½¿ç”¨', 'çµæ„Ÿæ¥è‡ª', 'é£æ ¼',
    ]
    score += min(15, sum(3 for word in descriptive_words if word in prompt_lower))

    # 6. ç»†èŠ‚ç¨‹åº¦ï¼ˆæ”¹è¿›ï¼‰
    detail_markers = [
        'in the style of', 'similar to', 'resembling', 'like',
        'inspired by', 'based on', 'reminiscent of',
        'é£æ ¼', 'ç±»ä¼¼äº', 'åƒ', 'çµæ„Ÿæ¥è‡ª', 'åŸºäº',
    ]
    if any(marker in prompt_lower for marker in detail_markers):
        score += 8

    # 7. é£æ ¼å‚è€ƒï¼ˆæ–°å¢ï¼‰
    style_keywords = [
        'photorealistic', 'hyperrealistic', 'cinematic', 'dramatic',
        'minimalist', 'vintage', 'modern', 'contemporary',
        'è¶…å†™å®', 'ç”µå½±æ„Ÿ', 'æˆå‰§æ€§', 'æç®€', 'å¤å¤', 'ç°ä»£', 'å½“ä»£',
    ]
    if any(kw in prompt_lower for kw in style_keywords):
        score += 5

    # 8. æˆªæ–­æƒ©ç½šï¼ˆæ–°å¢ - å¦‚æœæˆªæ–­ï¼Œé™ä½åˆ†æ•°ä½†ä¸è¦å®Œå…¨æ‹’ç»ï¼‰
    if is_truncated:
        score = max(score - 15, score * 0.7)  # è‡³å°‘é™ä½ 15 åˆ†æˆ– 30%

    return min(100, int(score))


def process_search_result(result: Dict, seen_urls: Set[str]) -> List[Dict]:
    """
    å¤„ç†å•ä¸ªæœç´¢ç»“æœ

    Args:
        result: æœç´¢ç»“æœå­—å…¸
        seen_urls: å·²å¤„ç†çš„ URL é›†åˆ

    Returns:
        æå–çš„æç¤ºè¯åˆ—è¡¨
    """
    url = result.get('url', '')
    title = result.get('title', '')
    snippet = result.get('content', '')

    # æ£€æŸ¥ URL è´¨é‡
    is_high_quality, reason = is_high_quality_url(url)
    if not is_high_quality:
        logger.debug(f"  è·³è¿‡ä½è´¨é‡ URL: {reason}")
        return []

    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    if url in seen_urls:
        logger.debug(f"  è·³è¿‡å·²å¤„ç† URL")
        return []

    seen_urls.add(url)

    logger.info(f"  å¤„ç†: {title[:60]}...")

    # è·å–é¡µé¢å†…å®¹
    content = fetch_page_content(url)
    if not content:
        return []

    # æå–æç¤ºè¯ï¼ˆç°åœ¨è¿”å›å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«å…ƒæ•°æ®ï¼‰
    prompt_data_list = extract_prompts_from_content(content, max_prompts=20)

    if not prompt_data_list:
        return []

    logger.info(f"    æ‰¾åˆ° {len(prompt_data_list)} ä¸ªæç¤ºè¯")

    # å¤„ç†æ¯ä¸ªæç¤ºè¯
    prompt_list = []
    for prompt_dict in prompt_data_list:
        prompt = prompt_dict['content']
        is_truncated = prompt_dict['is_truncated']

        # æ£€æµ‹è¯­è¨€
        language = detect_language(prompt)

        # åˆ†ç±»æç¤ºè¯ç±»å‹ï¼ˆä¼ å…¥è¯­è¨€ï¼‰
        prompt_type = classify_prompt_type(prompt, language)

        # è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆä¼ å…¥æˆªæ–­ä¿¡æ¯ï¼‰
        quality_score = calculate_quality_score(prompt, is_truncated)

        prompt_data = {
            'content': prompt,
            'title': title,
            'source': 'searxng-enhanced',
            'url': url,
            'type': prompt_type,
            'quality_score': quality_score,
            'language': language,  # æ–°å¢ï¼šè¯­è¨€å­—æ®µ
            'is_truncated': is_truncated,  # æ–°å¢ï¼šæˆªæ–­çŠ¶æ€
            'truncation_reason': prompt_dict['truncation_reason'],  # æ–°å¢ï¼šæˆªæ–­åŸå› 
            'collected_at': datetime.now().isoformat(),
        }

        prompt_list.append(prompt_data)

    return prompt_list


def run_collection(queries: List[str], max_workers: int = MAX_WORKERS) -> List[Dict]:
    """
    æ‰§è¡Œæ”¶é›†ä»»åŠ¡ - Phase 1 å¹¶å‘ç‰ˆ

    Args:
        queries: æœç´¢æŸ¥è¯¢åˆ—è¡¨
        max_workers: æœ€å¤§å¹¶å‘æ•°

    Returns:
        æ‰€æœ‰æ”¶é›†çš„æç¤ºè¯
    """
    all_prompts = []
    seen_urls = set()

    logger.info(f"å¼€å§‹æ‰§è¡Œ {len(queries)} ä¸ªæœç´¢æŸ¥è¯¢...")

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰æœç´¢ä»»åŠ¡
        future_to_query = {
            executor.submit(search_searxng, query): query
            for query in queries
        }

        # å¤„ç†å®Œæˆçš„æœç´¢
        for future in as_completed(future_to_query):
            if shutdown_flag:
                break

            query = future_to_query[future]

            try:
                results = future.result()

                if not results:
                    continue

                logger.info(f"\næœç´¢: {query}")
                logger.info(f"  æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")

                # å¤„ç†æ¯ä¸ªç»“æœ
                for result in results:
                    if shutdown_flag:
                        break

                    prompts = process_search_result(result, seen_urls)
                    all_prompts.extend(prompts)

                    # å»¶è¿Ÿï¼Œé¿å…è¿‡è½½
                    time.sleep(REQUEST_DELAY)

            except Exception as e:
                logger.error(f"å¤„ç†æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")

    return all_prompts


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("ğŸš€ å¢å¼ºç‰ˆ AI æç¤ºè¯æ”¶é›†ç³»ç»Ÿ - Phase 1")
    logger.info("=" * 80)
    logger.info(f"æœç´¢æŸ¥è¯¢æ€»æ•°: {len(ALL_QUERIES)}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    logger.info("=" * 80)

    # æ‰§è¡Œæ”¶é›†
    all_prompts = run_collection(ALL_QUERIES)

    if shutdown_flag:
        logger.info("\nâ¸ï¸  æ”¶é›†ä¸­æ–­ï¼Œä¿å­˜å·²æ”¶é›†çš„æ•°æ®...")
    else:
        logger.info(f"\n{'=' * 80}")
        logger.info("ğŸ“Š æ”¶é›†å®Œæˆï¼")
        logger.info(f"{'=' * 80}")

    # ä¿å­˜ç»“æœ
    logger.info(f"æ€»å…±æ”¶é›†: {len(all_prompts)} ä¸ªæç¤ºè¯")

    # å†™å…¥æ–‡ä»¶
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for prompt in all_prompts:
            f.write(json.dumps(prompt, ensure_ascii=False) + '\n')

    logger.info(f"âœ… ä¿å­˜åˆ°: {OUTPUT_FILE}")

    # ç»Ÿè®¡ä¿¡æ¯
    if all_prompts:
        # ç±»å‹åˆ†å¸ƒ
        type_counts = {}
        for prompt in all_prompts:
            ptype = prompt['type']
            type_counts[ptype] = type_counts.get(ptype, 0) + 1

        logger.info(f"\nç±»å‹åˆ†å¸ƒ:")
        for ptype, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  {ptype}: {count}")

        # è´¨é‡åˆ†å¸ƒ
        high_quality = sum(1 for p in all_prompts if p['quality_score'] >= 70)
        medium_quality = sum(1 for p in all_prompts if 50 <= p['quality_score'] < 70)
        low_quality = sum(1 for p in all_prompts if p['quality_score'] < 50)

        logger.info(f"\nè´¨é‡åˆ†å¸ƒ:")
        logger.info(f"  é«˜è´¨é‡ (â‰¥70): {high_quality} ({high_quality*100//len(all_prompts) if all_prompts else 0}%)")
        logger.info(f"  ä¸­ç­‰ (50-69): {medium_quality} ({medium_quality*100//len(all_prompts) if all_prompts else 0}%)")
        logger.info(f"  ä½è´¨é‡ (<50): {low_quality} ({low_quality*100//len(all_prompts) if all_prompts else 0}%)")

        # å¹³å‡è´¨é‡åˆ†æ•°
        avg_score = sum(p['quality_score'] for p in all_prompts) / len(all_prompts)
        logger.info(f"\nå¹³å‡è´¨é‡åˆ†æ•°: {avg_score:.1f}/100")

        # é«˜è´¨é‡æç¤ºè¯ç¤ºä¾‹
        high_quality_prompts = sorted(
            [p for p in all_prompts if p['quality_score'] >= 80],
            key=lambda x: x['quality_score'],
            reverse=True
        )[:5]

        if high_quality_prompts:
            logger.info(f"\nğŸ† å‰ 5 ä¸ªé«˜è´¨é‡æç¤ºè¯ç¤ºä¾‹:")
            for i, p in enumerate(high_quality_prompts, 1):
                logger.info(f"\n  {i}. [{p['type']}] è´¨é‡åˆ†æ•°: {p['quality_score']}")
                logger.info(f"     {p['content'][:100]}...")

    logger.info(f"\n{'=' * 80}")
    logger.info("âœ… å®Œæˆï¼")
    logger.info(f"{'=' * 80}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
